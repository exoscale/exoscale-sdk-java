/*
 * Exoscale Public API
 *  Infrastructure automation API, allowing programmatic access to all Exoscale products and services.  The [OpenAPI Specification](http://spec.openapis.org/oas/v3.0.3.html) source of this documentation can be obtained here:  * [JSON format](https://openapi-v2.exoscale.com/source.json) * [YAML format](https://openapi-v2.exoscale.com/source.yaml)
 *
 * OpenAPI spec version: 2.0.0
 * Contact: api@exoscale.com
 *
 * NOTE: This class is auto generated by the swagger code generator program.
 * https://github.com/swagger-api/swagger-codegen.git
 * Do not edit the class manually.
 */

package io.swagger.client.model;

import java.util.Objects;
import java.util.Arrays;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonValue;
import io.swagger.client.model.ConfigureLogCleanerForTopicCompaction;
import io.swagger.v3.oas.annotations.media.Schema;
/**
 * JsonSchemaKafka
 */

@javax.annotation.Generated(value = "io.swagger.codegen.v3.generators.java.JavaClientCodegen", date = "2024-03-08T17:23:40.261901+01:00[Europe/Vienna]")

public class JsonSchemaKafka {
  @JsonProperty("sasl_oauthbearer_expected_audience")
  private String saslOauthbearerExpectedAudience = null;

  @JsonProperty("group_max_session_timeout_ms")
  private Integer groupMaxSessionTimeoutMs = null;

  @JsonProperty("log_flush_interval_messages")
  private Integer logFlushIntervalMessages = null;

  @JsonProperty("sasl_oauthbearer_jwks_endpoint_url")
  private String saslOauthbearerJwksEndpointUrl = null;

  @JsonProperty("max_connections_per_ip")
  private Integer maxConnectionsPerIp = null;

  @JsonProperty("sasl_oauthbearer_expected_issuer")
  private String saslOauthbearerExpectedIssuer = null;

  @JsonProperty("log_index_size_max_bytes")
  private Integer logIndexSizeMaxBytes = null;

  @JsonProperty("auto_create_topics_enable")
  private Boolean autoCreateTopicsEnable = null;

  @JsonProperty("log_index_interval_bytes")
  private Integer logIndexIntervalBytes = null;

  @JsonProperty("replica_fetch_max_bytes")
  private Integer replicaFetchMaxBytes = null;

  @JsonProperty("num_partitions")
  private Integer numPartitions = null;

  @JsonProperty("transaction_state_log_segment_bytes")
  private Integer transactionStateLogSegmentBytes = null;

  @JsonProperty("replica_fetch_response_max_bytes")
  private Integer replicaFetchResponseMaxBytes = null;

  /**
   * Define whether the timestamp in the message is message create time or log append time.
   */
  public enum LogMessageTimestampTypeEnum {
    CREATETIME("CreateTime"),
    LOGAPPENDTIME("LogAppendTime");

    private String value;

    LogMessageTimestampTypeEnum(String value) {
      this.value = value;
    }
    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }
    @JsonCreator
    public static LogMessageTimestampTypeEnum fromValue(String input) {
      for (LogMessageTimestampTypeEnum b : LogMessageTimestampTypeEnum.values()) {
        if (b.value.equals(input)) {
          return b;
        }
      }
      return null;
    }

  }  @JsonProperty("log_message_timestamp_type")
  private LogMessageTimestampTypeEnum logMessageTimestampType = null;

  @JsonProperty("connections_max_idle_ms")
  private Integer connectionsMaxIdleMs = null;

  @JsonProperty("log_flush_interval_ms")
  private Integer logFlushIntervalMs = null;

  @JsonProperty("log_preallocate")
  private Boolean logPreallocate = null;

  @JsonProperty("log_segment_delete_delay_ms")
  private Integer logSegmentDeleteDelayMs = null;

  @JsonProperty("message_max_bytes")
  private Integer messageMaxBytes = null;

  @JsonProperty("group_initial_rebalance_delay_ms")
  private Integer groupInitialRebalanceDelayMs = null;

  @JsonProperty("log_local_retention_bytes")
  private Integer logLocalRetentionBytes = null;

  @JsonProperty("log_roll_jitter_ms")
  private Integer logRollJitterMs = null;

  @JsonProperty("transaction_remove_expired_transaction_cleanup_interval_ms")
  private Integer transactionRemoveExpiredTransactionCleanupIntervalMs = null;

  @JsonProperty("default_replication_factor")
  private Integer defaultReplicationFactor = null;

  @JsonProperty("log_roll_ms")
  private Integer logRollMs = null;

  @JsonProperty("producer_purgatory_purge_interval_requests")
  private Integer producerPurgatoryPurgeIntervalRequests = null;

  @JsonProperty("log_retention_bytes")
  private Integer logRetentionBytes = null;

  @JsonProperty("min_insync_replicas")
  private Integer minInsyncReplicas = null;

  /**
   * Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (&#x27;gzip&#x27;, &#x27;snappy&#x27;, &#x27;lz4&#x27;, &#x27;zstd&#x27;). It additionally accepts &#x27;uncompressed&#x27; which is equivalent to no compression; and &#x27;producer&#x27; which means retain the original compression codec set by the producer.
   */
  public enum CompressionTypeEnum {
    GZIP("gzip"),
    SNAPPY("snappy"),
    LZ4("lz4"),
    ZSTD("zstd"),
    UNCOMPRESSED("uncompressed"),
    PRODUCER("producer");

    private String value;

    CompressionTypeEnum(String value) {
      this.value = value;
    }
    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }
    @JsonCreator
    public static CompressionTypeEnum fromValue(String input) {
      for (CompressionTypeEnum b : CompressionTypeEnum.values()) {
        if (b.value.equals(input)) {
          return b;
        }
      }
      return null;
    }

  }  @JsonProperty("compression_type")
  private CompressionTypeEnum compressionType = null;

  @JsonProperty("log_message_timestamp_difference_max_ms")
  private Integer logMessageTimestampDifferenceMaxMs = null;

  @JsonProperty("log_local_retention_ms")
  private Integer logLocalRetentionMs = null;

  @JsonProperty("log_message_downconversion_enable")
  private Boolean logMessageDownconversionEnable = null;

  @JsonProperty("sasl_oauthbearer_sub_claim_name")
  private String saslOauthbearerSubClaimName = null;

  @JsonProperty("max_incremental_fetch_session_cache_slots")
  private Integer maxIncrementalFetchSessionCacheSlots = null;

  @JsonProperty("log_retention_hours")
  private Integer logRetentionHours = null;

  @JsonProperty("group_min_session_timeout_ms")
  private Integer groupMinSessionTimeoutMs = null;

  @JsonProperty("socket_request_max_bytes")
  private Integer socketRequestMaxBytes = null;

  @JsonProperty("log_segment_bytes")
  private Integer logSegmentBytes = null;

  @JsonProperty("log-cleanup-and-compaction")
  private ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction = null;

  @JsonProperty("offsets_retention_minutes")
  private Integer offsetsRetentionMinutes = null;

  @JsonProperty("log_retention_ms")
  private Integer logRetentionMs = null;

  public JsonSchemaKafka saslOauthbearerExpectedAudience(String saslOauthbearerExpectedAudience) {
    this.saslOauthbearerExpectedAudience = saslOauthbearerExpectedAudience;
    return this;
  }

   /**
   * The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences.
   * @return saslOauthbearerExpectedAudience
  **/
  @Schema(description = "The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences.")
  public String getSaslOauthbearerExpectedAudience() {
    return saslOauthbearerExpectedAudience;
  }

  public void setSaslOauthbearerExpectedAudience(String saslOauthbearerExpectedAudience) {
    this.saslOauthbearerExpectedAudience = saslOauthbearerExpectedAudience;
  }

  public JsonSchemaKafka groupMaxSessionTimeoutMs(Integer groupMaxSessionTimeoutMs) {
    this.groupMaxSessionTimeoutMs = groupMaxSessionTimeoutMs;
    return this;
  }

   /**
   * The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
   * minimum: 0
   * maximum: 1800000
   * @return groupMaxSessionTimeoutMs
  **/
  @Schema(example = "1800000", description = "The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.")
  public Integer getGroupMaxSessionTimeoutMs() {
    return groupMaxSessionTimeoutMs;
  }

  public void setGroupMaxSessionTimeoutMs(Integer groupMaxSessionTimeoutMs) {
    this.groupMaxSessionTimeoutMs = groupMaxSessionTimeoutMs;
  }

  public JsonSchemaKafka logFlushIntervalMessages(Integer logFlushIntervalMessages) {
    this.logFlushIntervalMessages = logFlushIntervalMessages;
    return this;
  }

   /**
   * The number of messages accumulated on a log partition before messages are flushed to disk
   * minimum: 1
   * maximum: 9223372036854775807
   * @return logFlushIntervalMessages
  **/
  @Schema(example = "9223372036854775807", description = "The number of messages accumulated on a log partition before messages are flushed to disk")
  public Integer getLogFlushIntervalMessages() {
    return logFlushIntervalMessages;
  }

  public void setLogFlushIntervalMessages(Integer logFlushIntervalMessages) {
    this.logFlushIntervalMessages = logFlushIntervalMessages;
  }

  public JsonSchemaKafka saslOauthbearerJwksEndpointUrl(String saslOauthbearerJwksEndpointUrl) {
    this.saslOauthbearerJwksEndpointUrl = saslOauthbearerJwksEndpointUrl;
    return this;
  }

   /**
   * OIDC JWKS endpoint URL. By setting this the SASL SSL OAuth2/OIDC authentication is enabled. See also other options for SASL OAuth2/OIDC. 
   * @return saslOauthbearerJwksEndpointUrl
  **/
  @Schema(description = "OIDC JWKS endpoint URL. By setting this the SASL SSL OAuth2/OIDC authentication is enabled. See also other options for SASL OAuth2/OIDC. ")
  public String getSaslOauthbearerJwksEndpointUrl() {
    return saslOauthbearerJwksEndpointUrl;
  }

  public void setSaslOauthbearerJwksEndpointUrl(String saslOauthbearerJwksEndpointUrl) {
    this.saslOauthbearerJwksEndpointUrl = saslOauthbearerJwksEndpointUrl;
  }

  public JsonSchemaKafka maxConnectionsPerIp(Integer maxConnectionsPerIp) {
    this.maxConnectionsPerIp = maxConnectionsPerIp;
    return this;
  }

   /**
   * The maximum number of connections allowed from each ip address (defaults to 2147483647).
   * minimum: 256
   * maximum: 2147483647
   * @return maxConnectionsPerIp
  **/
  @Schema(description = "The maximum number of connections allowed from each ip address (defaults to 2147483647).")
  public Integer getMaxConnectionsPerIp() {
    return maxConnectionsPerIp;
  }

  public void setMaxConnectionsPerIp(Integer maxConnectionsPerIp) {
    this.maxConnectionsPerIp = maxConnectionsPerIp;
  }

  public JsonSchemaKafka saslOauthbearerExpectedIssuer(String saslOauthbearerExpectedIssuer) {
    this.saslOauthbearerExpectedIssuer = saslOauthbearerExpectedIssuer;
    return this;
  }

   /**
   * Optional setting for the broker to use to verify that the JWT was created by the expected issuer.
   * @return saslOauthbearerExpectedIssuer
  **/
  @Schema(description = "Optional setting for the broker to use to verify that the JWT was created by the expected issuer.")
  public String getSaslOauthbearerExpectedIssuer() {
    return saslOauthbearerExpectedIssuer;
  }

  public void setSaslOauthbearerExpectedIssuer(String saslOauthbearerExpectedIssuer) {
    this.saslOauthbearerExpectedIssuer = saslOauthbearerExpectedIssuer;
  }

  public JsonSchemaKafka logIndexSizeMaxBytes(Integer logIndexSizeMaxBytes) {
    this.logIndexSizeMaxBytes = logIndexSizeMaxBytes;
    return this;
  }

   /**
   * The maximum size in bytes of the offset index
   * minimum: 1048576
   * maximum: 104857600
   * @return logIndexSizeMaxBytes
  **/
  @Schema(example = "10485760", description = "The maximum size in bytes of the offset index")
  public Integer getLogIndexSizeMaxBytes() {
    return logIndexSizeMaxBytes;
  }

  public void setLogIndexSizeMaxBytes(Integer logIndexSizeMaxBytes) {
    this.logIndexSizeMaxBytes = logIndexSizeMaxBytes;
  }

  public JsonSchemaKafka autoCreateTopicsEnable(Boolean autoCreateTopicsEnable) {
    this.autoCreateTopicsEnable = autoCreateTopicsEnable;
    return this;
  }

   /**
   * Enable auto creation of topics
   * @return autoCreateTopicsEnable
  **/
  @Schema(example = "true", description = "Enable auto creation of topics")
  public Boolean isAutoCreateTopicsEnable() {
    return autoCreateTopicsEnable;
  }

  public void setAutoCreateTopicsEnable(Boolean autoCreateTopicsEnable) {
    this.autoCreateTopicsEnable = autoCreateTopicsEnable;
  }

  public JsonSchemaKafka logIndexIntervalBytes(Integer logIndexIntervalBytes) {
    this.logIndexIntervalBytes = logIndexIntervalBytes;
    return this;
  }

   /**
   * The interval with which Kafka adds an entry to the offset index
   * minimum: 0
   * maximum: 104857600
   * @return logIndexIntervalBytes
  **/
  @Schema(example = "4096", description = "The interval with which Kafka adds an entry to the offset index")
  public Integer getLogIndexIntervalBytes() {
    return logIndexIntervalBytes;
  }

  public void setLogIndexIntervalBytes(Integer logIndexIntervalBytes) {
    this.logIndexIntervalBytes = logIndexIntervalBytes;
  }

  public JsonSchemaKafka replicaFetchMaxBytes(Integer replicaFetchMaxBytes) {
    this.replicaFetchMaxBytes = replicaFetchMaxBytes;
    return this;
  }

   /**
   * The number of bytes of messages to attempt to fetch for each partition (defaults to 1048576). This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made.
   * minimum: 1048576
   * maximum: 104857600
   * @return replicaFetchMaxBytes
  **/
  @Schema(description = "The number of bytes of messages to attempt to fetch for each partition (defaults to 1048576). This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made.")
  public Integer getReplicaFetchMaxBytes() {
    return replicaFetchMaxBytes;
  }

  public void setReplicaFetchMaxBytes(Integer replicaFetchMaxBytes) {
    this.replicaFetchMaxBytes = replicaFetchMaxBytes;
  }

  public JsonSchemaKafka numPartitions(Integer numPartitions) {
    this.numPartitions = numPartitions;
    return this;
  }

   /**
   * Number of partitions for autocreated topics
   * minimum: 1
   * maximum: 1000
   * @return numPartitions
  **/
  @Schema(description = "Number of partitions for autocreated topics")
  public Integer getNumPartitions() {
    return numPartitions;
  }

  public void setNumPartitions(Integer numPartitions) {
    this.numPartitions = numPartitions;
  }

  public JsonSchemaKafka transactionStateLogSegmentBytes(Integer transactionStateLogSegmentBytes) {
    this.transactionStateLogSegmentBytes = transactionStateLogSegmentBytes;
    return this;
  }

   /**
   * The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (defaults to 104857600 (100 mebibytes)).
   * minimum: 1048576
   * maximum: 2147483647
   * @return transactionStateLogSegmentBytes
  **/
  @Schema(example = "104857600", description = "The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (defaults to 104857600 (100 mebibytes)).")
  public Integer getTransactionStateLogSegmentBytes() {
    return transactionStateLogSegmentBytes;
  }

  public void setTransactionStateLogSegmentBytes(Integer transactionStateLogSegmentBytes) {
    this.transactionStateLogSegmentBytes = transactionStateLogSegmentBytes;
  }

  public JsonSchemaKafka replicaFetchResponseMaxBytes(Integer replicaFetchResponseMaxBytes) {
    this.replicaFetchResponseMaxBytes = replicaFetchResponseMaxBytes;
    return this;
  }

   /**
   * Maximum bytes expected for the entire fetch response (defaults to 10485760). Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum.
   * minimum: 10485760
   * maximum: 1048576000
   * @return replicaFetchResponseMaxBytes
  **/
  @Schema(description = "Maximum bytes expected for the entire fetch response (defaults to 10485760). Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum.")
  public Integer getReplicaFetchResponseMaxBytes() {
    return replicaFetchResponseMaxBytes;
  }

  public void setReplicaFetchResponseMaxBytes(Integer replicaFetchResponseMaxBytes) {
    this.replicaFetchResponseMaxBytes = replicaFetchResponseMaxBytes;
  }

  public JsonSchemaKafka logMessageTimestampType(LogMessageTimestampTypeEnum logMessageTimestampType) {
    this.logMessageTimestampType = logMessageTimestampType;
    return this;
  }

   /**
   * Define whether the timestamp in the message is message create time or log append time.
   * @return logMessageTimestampType
  **/
  @Schema(description = "Define whether the timestamp in the message is message create time or log append time.")
  public LogMessageTimestampTypeEnum getLogMessageTimestampType() {
    return logMessageTimestampType;
  }

  public void setLogMessageTimestampType(LogMessageTimestampTypeEnum logMessageTimestampType) {
    this.logMessageTimestampType = logMessageTimestampType;
  }

  public JsonSchemaKafka connectionsMaxIdleMs(Integer connectionsMaxIdleMs) {
    this.connectionsMaxIdleMs = connectionsMaxIdleMs;
    return this;
  }

   /**
   * Idle connections timeout: the server socket processor threads close the connections that idle for longer than this.
   * minimum: 1000
   * maximum: 3600000
   * @return connectionsMaxIdleMs
  **/
  @Schema(example = "540000", description = "Idle connections timeout: the server socket processor threads close the connections that idle for longer than this.")
  public Integer getConnectionsMaxIdleMs() {
    return connectionsMaxIdleMs;
  }

  public void setConnectionsMaxIdleMs(Integer connectionsMaxIdleMs) {
    this.connectionsMaxIdleMs = connectionsMaxIdleMs;
  }

  public JsonSchemaKafka logFlushIntervalMs(Integer logFlushIntervalMs) {
    this.logFlushIntervalMs = logFlushIntervalMs;
    return this;
  }

   /**
   * The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logFlushIntervalMs
  **/
  @Schema(description = "The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used")
  public Integer getLogFlushIntervalMs() {
    return logFlushIntervalMs;
  }

  public void setLogFlushIntervalMs(Integer logFlushIntervalMs) {
    this.logFlushIntervalMs = logFlushIntervalMs;
  }

  public JsonSchemaKafka logPreallocate(Boolean logPreallocate) {
    this.logPreallocate = logPreallocate;
    return this;
  }

   /**
   * Should pre allocate file when create new segment?
   * @return logPreallocate
  **/
  @Schema(example = "false", description = "Should pre allocate file when create new segment?")
  public Boolean isLogPreallocate() {
    return logPreallocate;
  }

  public void setLogPreallocate(Boolean logPreallocate) {
    this.logPreallocate = logPreallocate;
  }

  public JsonSchemaKafka logSegmentDeleteDelayMs(Integer logSegmentDeleteDelayMs) {
    this.logSegmentDeleteDelayMs = logSegmentDeleteDelayMs;
    return this;
  }

   /**
   * The amount of time to wait before deleting a file from the filesystem
   * minimum: 0
   * maximum: 3600000
   * @return logSegmentDeleteDelayMs
  **/
  @Schema(example = "60000", description = "The amount of time to wait before deleting a file from the filesystem")
  public Integer getLogSegmentDeleteDelayMs() {
    return logSegmentDeleteDelayMs;
  }

  public void setLogSegmentDeleteDelayMs(Integer logSegmentDeleteDelayMs) {
    this.logSegmentDeleteDelayMs = logSegmentDeleteDelayMs;
  }

  public JsonSchemaKafka messageMaxBytes(Integer messageMaxBytes) {
    this.messageMaxBytes = messageMaxBytes;
    return this;
  }

   /**
   * The maximum size of message that the server can receive.
   * minimum: 0
   * maximum: 100001200
   * @return messageMaxBytes
  **/
  @Schema(example = "1048588", description = "The maximum size of message that the server can receive.")
  public Integer getMessageMaxBytes() {
    return messageMaxBytes;
  }

  public void setMessageMaxBytes(Integer messageMaxBytes) {
    this.messageMaxBytes = messageMaxBytes;
  }

  public JsonSchemaKafka groupInitialRebalanceDelayMs(Integer groupInitialRebalanceDelayMs) {
    this.groupInitialRebalanceDelayMs = groupInitialRebalanceDelayMs;
    return this;
  }

   /**
   * The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.
   * minimum: 0
   * maximum: 300000
   * @return groupInitialRebalanceDelayMs
  **/
  @Schema(example = "3000", description = "The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.")
  public Integer getGroupInitialRebalanceDelayMs() {
    return groupInitialRebalanceDelayMs;
  }

  public void setGroupInitialRebalanceDelayMs(Integer groupInitialRebalanceDelayMs) {
    this.groupInitialRebalanceDelayMs = groupInitialRebalanceDelayMs;
  }

  public JsonSchemaKafka logLocalRetentionBytes(Integer logLocalRetentionBytes) {
    this.logLocalRetentionBytes = logLocalRetentionBytes;
    return this;
  }

   /**
   * The maximum size of local log segments that can grow for a partition before it gets eligible for deletion. If set to -2, the value of log.retention.bytes is used. The effective value should always be less than or equal to log.retention.bytes value.
   * minimum: -2
   * maximum: 9223372036854775807
   * @return logLocalRetentionBytes
  **/
  @Schema(description = "The maximum size of local log segments that can grow for a partition before it gets eligible for deletion. If set to -2, the value of log.retention.bytes is used. The effective value should always be less than or equal to log.retention.bytes value.")
  public Integer getLogLocalRetentionBytes() {
    return logLocalRetentionBytes;
  }

  public void setLogLocalRetentionBytes(Integer logLocalRetentionBytes) {
    this.logLocalRetentionBytes = logLocalRetentionBytes;
  }

  public JsonSchemaKafka logRollJitterMs(Integer logRollJitterMs) {
    this.logRollJitterMs = logRollJitterMs;
    return this;
  }

   /**
   * The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logRollJitterMs
  **/
  @Schema(description = "The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used")
  public Integer getLogRollJitterMs() {
    return logRollJitterMs;
  }

  public void setLogRollJitterMs(Integer logRollJitterMs) {
    this.logRollJitterMs = logRollJitterMs;
  }

  public JsonSchemaKafka transactionRemoveExpiredTransactionCleanupIntervalMs(Integer transactionRemoveExpiredTransactionCleanupIntervalMs) {
    this.transactionRemoveExpiredTransactionCleanupIntervalMs = transactionRemoveExpiredTransactionCleanupIntervalMs;
    return this;
  }

   /**
   * The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults to 3600000 (1 hour)).
   * minimum: 600000
   * maximum: 3600000
   * @return transactionRemoveExpiredTransactionCleanupIntervalMs
  **/
  @Schema(example = "3600000", description = "The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults to 3600000 (1 hour)).")
  public Integer getTransactionRemoveExpiredTransactionCleanupIntervalMs() {
    return transactionRemoveExpiredTransactionCleanupIntervalMs;
  }

  public void setTransactionRemoveExpiredTransactionCleanupIntervalMs(Integer transactionRemoveExpiredTransactionCleanupIntervalMs) {
    this.transactionRemoveExpiredTransactionCleanupIntervalMs = transactionRemoveExpiredTransactionCleanupIntervalMs;
  }

  public JsonSchemaKafka defaultReplicationFactor(Integer defaultReplicationFactor) {
    this.defaultReplicationFactor = defaultReplicationFactor;
    return this;
  }

   /**
   * Replication factor for autocreated topics
   * minimum: 1
   * maximum: 10
   * @return defaultReplicationFactor
  **/
  @Schema(description = "Replication factor for autocreated topics")
  public Integer getDefaultReplicationFactor() {
    return defaultReplicationFactor;
  }

  public void setDefaultReplicationFactor(Integer defaultReplicationFactor) {
    this.defaultReplicationFactor = defaultReplicationFactor;
  }

  public JsonSchemaKafka logRollMs(Integer logRollMs) {
    this.logRollMs = logRollMs;
    return this;
  }

   /**
   * The maximum time before a new log segment is rolled out (in milliseconds).
   * minimum: 1
   * maximum: 9223372036854775807
   * @return logRollMs
  **/
  @Schema(description = "The maximum time before a new log segment is rolled out (in milliseconds).")
  public Integer getLogRollMs() {
    return logRollMs;
  }

  public void setLogRollMs(Integer logRollMs) {
    this.logRollMs = logRollMs;
  }

  public JsonSchemaKafka producerPurgatoryPurgeIntervalRequests(Integer producerPurgatoryPurgeIntervalRequests) {
    this.producerPurgatoryPurgeIntervalRequests = producerPurgatoryPurgeIntervalRequests;
    return this;
  }

   /**
   * The purge interval (in number of requests) of the producer request purgatory(defaults to 1000).
   * minimum: 10
   * maximum: 10000
   * @return producerPurgatoryPurgeIntervalRequests
  **/
  @Schema(description = "The purge interval (in number of requests) of the producer request purgatory(defaults to 1000).")
  public Integer getProducerPurgatoryPurgeIntervalRequests() {
    return producerPurgatoryPurgeIntervalRequests;
  }

  public void setProducerPurgatoryPurgeIntervalRequests(Integer producerPurgatoryPurgeIntervalRequests) {
    this.producerPurgatoryPurgeIntervalRequests = producerPurgatoryPurgeIntervalRequests;
  }

  public JsonSchemaKafka logRetentionBytes(Integer logRetentionBytes) {
    this.logRetentionBytes = logRetentionBytes;
    return this;
  }

   /**
   * The maximum size of the log before deleting messages
   * minimum: -1
   * maximum: 9223372036854775807
   * @return logRetentionBytes
  **/
  @Schema(description = "The maximum size of the log before deleting messages")
  public Integer getLogRetentionBytes() {
    return logRetentionBytes;
  }

  public void setLogRetentionBytes(Integer logRetentionBytes) {
    this.logRetentionBytes = logRetentionBytes;
  }

  public JsonSchemaKafka minInsyncReplicas(Integer minInsyncReplicas) {
    this.minInsyncReplicas = minInsyncReplicas;
    return this;
  }

   /**
   * When a producer sets acks to &#x27;all&#x27; (or &#x27;-1&#x27;), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful.
   * minimum: 1
   * maximum: 7
   * @return minInsyncReplicas
  **/
  @Schema(example = "1", description = "When a producer sets acks to 'all' (or '-1'), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful.")
  public Integer getMinInsyncReplicas() {
    return minInsyncReplicas;
  }

  public void setMinInsyncReplicas(Integer minInsyncReplicas) {
    this.minInsyncReplicas = minInsyncReplicas;
  }

  public JsonSchemaKafka compressionType(CompressionTypeEnum compressionType) {
    this.compressionType = compressionType;
    return this;
  }

   /**
   * Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (&#x27;gzip&#x27;, &#x27;snappy&#x27;, &#x27;lz4&#x27;, &#x27;zstd&#x27;). It additionally accepts &#x27;uncompressed&#x27; which is equivalent to no compression; and &#x27;producer&#x27; which means retain the original compression codec set by the producer.
   * @return compressionType
  **/
  @Schema(description = "Specify the final compression type for a given topic. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer' which means retain the original compression codec set by the producer.")
  public CompressionTypeEnum getCompressionType() {
    return compressionType;
  }

  public void setCompressionType(CompressionTypeEnum compressionType) {
    this.compressionType = compressionType;
  }

  public JsonSchemaKafka logMessageTimestampDifferenceMaxMs(Integer logMessageTimestampDifferenceMaxMs) {
    this.logMessageTimestampDifferenceMaxMs = logMessageTimestampDifferenceMaxMs;
    return this;
  }

   /**
   * The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logMessageTimestampDifferenceMaxMs
  **/
  @Schema(description = "The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message")
  public Integer getLogMessageTimestampDifferenceMaxMs() {
    return logMessageTimestampDifferenceMaxMs;
  }

  public void setLogMessageTimestampDifferenceMaxMs(Integer logMessageTimestampDifferenceMaxMs) {
    this.logMessageTimestampDifferenceMaxMs = logMessageTimestampDifferenceMaxMs;
  }

  public JsonSchemaKafka logLocalRetentionMs(Integer logLocalRetentionMs) {
    this.logLocalRetentionMs = logLocalRetentionMs;
    return this;
  }

   /**
   * The number of milliseconds to keep the local log segments before it gets eligible for deletion. If set to -2, the value of log.retention.ms is used. The effective value should always be less than or equal to log.retention.ms value.
   * minimum: -2
   * maximum: 9223372036854775807
   * @return logLocalRetentionMs
  **/
  @Schema(description = "The number of milliseconds to keep the local log segments before it gets eligible for deletion. If set to -2, the value of log.retention.ms is used. The effective value should always be less than or equal to log.retention.ms value.")
  public Integer getLogLocalRetentionMs() {
    return logLocalRetentionMs;
  }

  public void setLogLocalRetentionMs(Integer logLocalRetentionMs) {
    this.logLocalRetentionMs = logLocalRetentionMs;
  }

  public JsonSchemaKafka logMessageDownconversionEnable(Boolean logMessageDownconversionEnable) {
    this.logMessageDownconversionEnable = logMessageDownconversionEnable;
    return this;
  }

   /**
   * This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests. 
   * @return logMessageDownconversionEnable
  **/
  @Schema(example = "true", description = "This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests. ")
  public Boolean isLogMessageDownconversionEnable() {
    return logMessageDownconversionEnable;
  }

  public void setLogMessageDownconversionEnable(Boolean logMessageDownconversionEnable) {
    this.logMessageDownconversionEnable = logMessageDownconversionEnable;
  }

  public JsonSchemaKafka saslOauthbearerSubClaimName(String saslOauthbearerSubClaimName) {
    this.saslOauthbearerSubClaimName = saslOauthbearerSubClaimName;
    return this;
  }

   /**
   * Name of the scope from which to extract the subject claim from the JWT. Defaults to sub.
   * @return saslOauthbearerSubClaimName
  **/
  @Schema(description = "Name of the scope from which to extract the subject claim from the JWT. Defaults to sub.")
  public String getSaslOauthbearerSubClaimName() {
    return saslOauthbearerSubClaimName;
  }

  public void setSaslOauthbearerSubClaimName(String saslOauthbearerSubClaimName) {
    this.saslOauthbearerSubClaimName = saslOauthbearerSubClaimName;
  }

  public JsonSchemaKafka maxIncrementalFetchSessionCacheSlots(Integer maxIncrementalFetchSessionCacheSlots) {
    this.maxIncrementalFetchSessionCacheSlots = maxIncrementalFetchSessionCacheSlots;
    return this;
  }

   /**
   * The maximum number of incremental fetch sessions that the broker will maintain.
   * minimum: 1000
   * maximum: 10000
   * @return maxIncrementalFetchSessionCacheSlots
  **/
  @Schema(example = "1000", description = "The maximum number of incremental fetch sessions that the broker will maintain.")
  public Integer getMaxIncrementalFetchSessionCacheSlots() {
    return maxIncrementalFetchSessionCacheSlots;
  }

  public void setMaxIncrementalFetchSessionCacheSlots(Integer maxIncrementalFetchSessionCacheSlots) {
    this.maxIncrementalFetchSessionCacheSlots = maxIncrementalFetchSessionCacheSlots;
  }

  public JsonSchemaKafka logRetentionHours(Integer logRetentionHours) {
    this.logRetentionHours = logRetentionHours;
    return this;
  }

   /**
   * The number of hours to keep a log file before deleting it
   * minimum: -1
   * maximum: 2147483647
   * @return logRetentionHours
  **/
  @Schema(description = "The number of hours to keep a log file before deleting it")
  public Integer getLogRetentionHours() {
    return logRetentionHours;
  }

  public void setLogRetentionHours(Integer logRetentionHours) {
    this.logRetentionHours = logRetentionHours;
  }

  public JsonSchemaKafka groupMinSessionTimeoutMs(Integer groupMinSessionTimeoutMs) {
    this.groupMinSessionTimeoutMs = groupMinSessionTimeoutMs;
    return this;
  }

   /**
   * The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
   * minimum: 0
   * maximum: 60000
   * @return groupMinSessionTimeoutMs
  **/
  @Schema(example = "6000", description = "The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.")
  public Integer getGroupMinSessionTimeoutMs() {
    return groupMinSessionTimeoutMs;
  }

  public void setGroupMinSessionTimeoutMs(Integer groupMinSessionTimeoutMs) {
    this.groupMinSessionTimeoutMs = groupMinSessionTimeoutMs;
  }

  public JsonSchemaKafka socketRequestMaxBytes(Integer socketRequestMaxBytes) {
    this.socketRequestMaxBytes = socketRequestMaxBytes;
    return this;
  }

   /**
   * The maximum number of bytes in a socket request (defaults to 104857600).
   * minimum: 10485760
   * maximum: 209715200
   * @return socketRequestMaxBytes
  **/
  @Schema(description = "The maximum number of bytes in a socket request (defaults to 104857600).")
  public Integer getSocketRequestMaxBytes() {
    return socketRequestMaxBytes;
  }

  public void setSocketRequestMaxBytes(Integer socketRequestMaxBytes) {
    this.socketRequestMaxBytes = socketRequestMaxBytes;
  }

  public JsonSchemaKafka logSegmentBytes(Integer logSegmentBytes) {
    this.logSegmentBytes = logSegmentBytes;
    return this;
  }

   /**
   * The maximum size of a single log file
   * minimum: 10485760
   * maximum: 1073741824
   * @return logSegmentBytes
  **/
  @Schema(description = "The maximum size of a single log file")
  public Integer getLogSegmentBytes() {
    return logSegmentBytes;
  }

  public void setLogSegmentBytes(Integer logSegmentBytes) {
    this.logSegmentBytes = logSegmentBytes;
  }

  public JsonSchemaKafka logCleanupAndCompaction(ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction) {
    this.logCleanupAndCompaction = logCleanupAndCompaction;
    return this;
  }

   /**
   * Get logCleanupAndCompaction
   * @return logCleanupAndCompaction
  **/
  @Schema(description = "")
  public ConfigureLogCleanerForTopicCompaction getLogCleanupAndCompaction() {
    return logCleanupAndCompaction;
  }

  public void setLogCleanupAndCompaction(ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction) {
    this.logCleanupAndCompaction = logCleanupAndCompaction;
  }

  public JsonSchemaKafka offsetsRetentionMinutes(Integer offsetsRetentionMinutes) {
    this.offsetsRetentionMinutes = offsetsRetentionMinutes;
    return this;
  }

   /**
   * Log retention window in minutes for offsets topic
   * minimum: 1
   * maximum: 2147483647
   * @return offsetsRetentionMinutes
  **/
  @Schema(example = "10080", description = "Log retention window in minutes for offsets topic")
  public Integer getOffsetsRetentionMinutes() {
    return offsetsRetentionMinutes;
  }

  public void setOffsetsRetentionMinutes(Integer offsetsRetentionMinutes) {
    this.offsetsRetentionMinutes = offsetsRetentionMinutes;
  }

  public JsonSchemaKafka logRetentionMs(Integer logRetentionMs) {
    this.logRetentionMs = logRetentionMs;
    return this;
  }

   /**
   * The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.
   * minimum: -1
   * maximum: 9223372036854775807
   * @return logRetentionMs
  **/
  @Schema(description = "The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.")
  public Integer getLogRetentionMs() {
    return logRetentionMs;
  }

  public void setLogRetentionMs(Integer logRetentionMs) {
    this.logRetentionMs = logRetentionMs;
  }


  @Override
  public boolean equals(java.lang.Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    JsonSchemaKafka jsonSchemaKafka = (JsonSchemaKafka) o;
    return Objects.equals(this.saslOauthbearerExpectedAudience, jsonSchemaKafka.saslOauthbearerExpectedAudience) &&
        Objects.equals(this.groupMaxSessionTimeoutMs, jsonSchemaKafka.groupMaxSessionTimeoutMs) &&
        Objects.equals(this.logFlushIntervalMessages, jsonSchemaKafka.logFlushIntervalMessages) &&
        Objects.equals(this.saslOauthbearerJwksEndpointUrl, jsonSchemaKafka.saslOauthbearerJwksEndpointUrl) &&
        Objects.equals(this.maxConnectionsPerIp, jsonSchemaKafka.maxConnectionsPerIp) &&
        Objects.equals(this.saslOauthbearerExpectedIssuer, jsonSchemaKafka.saslOauthbearerExpectedIssuer) &&
        Objects.equals(this.logIndexSizeMaxBytes, jsonSchemaKafka.logIndexSizeMaxBytes) &&
        Objects.equals(this.autoCreateTopicsEnable, jsonSchemaKafka.autoCreateTopicsEnable) &&
        Objects.equals(this.logIndexIntervalBytes, jsonSchemaKafka.logIndexIntervalBytes) &&
        Objects.equals(this.replicaFetchMaxBytes, jsonSchemaKafka.replicaFetchMaxBytes) &&
        Objects.equals(this.numPartitions, jsonSchemaKafka.numPartitions) &&
        Objects.equals(this.transactionStateLogSegmentBytes, jsonSchemaKafka.transactionStateLogSegmentBytes) &&
        Objects.equals(this.replicaFetchResponseMaxBytes, jsonSchemaKafka.replicaFetchResponseMaxBytes) &&
        Objects.equals(this.logMessageTimestampType, jsonSchemaKafka.logMessageTimestampType) &&
        Objects.equals(this.connectionsMaxIdleMs, jsonSchemaKafka.connectionsMaxIdleMs) &&
        Objects.equals(this.logFlushIntervalMs, jsonSchemaKafka.logFlushIntervalMs) &&
        Objects.equals(this.logPreallocate, jsonSchemaKafka.logPreallocate) &&
        Objects.equals(this.logSegmentDeleteDelayMs, jsonSchemaKafka.logSegmentDeleteDelayMs) &&
        Objects.equals(this.messageMaxBytes, jsonSchemaKafka.messageMaxBytes) &&
        Objects.equals(this.groupInitialRebalanceDelayMs, jsonSchemaKafka.groupInitialRebalanceDelayMs) &&
        Objects.equals(this.logLocalRetentionBytes, jsonSchemaKafka.logLocalRetentionBytes) &&
        Objects.equals(this.logRollJitterMs, jsonSchemaKafka.logRollJitterMs) &&
        Objects.equals(this.transactionRemoveExpiredTransactionCleanupIntervalMs, jsonSchemaKafka.transactionRemoveExpiredTransactionCleanupIntervalMs) &&
        Objects.equals(this.defaultReplicationFactor, jsonSchemaKafka.defaultReplicationFactor) &&
        Objects.equals(this.logRollMs, jsonSchemaKafka.logRollMs) &&
        Objects.equals(this.producerPurgatoryPurgeIntervalRequests, jsonSchemaKafka.producerPurgatoryPurgeIntervalRequests) &&
        Objects.equals(this.logRetentionBytes, jsonSchemaKafka.logRetentionBytes) &&
        Objects.equals(this.minInsyncReplicas, jsonSchemaKafka.minInsyncReplicas) &&
        Objects.equals(this.compressionType, jsonSchemaKafka.compressionType) &&
        Objects.equals(this.logMessageTimestampDifferenceMaxMs, jsonSchemaKafka.logMessageTimestampDifferenceMaxMs) &&
        Objects.equals(this.logLocalRetentionMs, jsonSchemaKafka.logLocalRetentionMs) &&
        Objects.equals(this.logMessageDownconversionEnable, jsonSchemaKafka.logMessageDownconversionEnable) &&
        Objects.equals(this.saslOauthbearerSubClaimName, jsonSchemaKafka.saslOauthbearerSubClaimName) &&
        Objects.equals(this.maxIncrementalFetchSessionCacheSlots, jsonSchemaKafka.maxIncrementalFetchSessionCacheSlots) &&
        Objects.equals(this.logRetentionHours, jsonSchemaKafka.logRetentionHours) &&
        Objects.equals(this.groupMinSessionTimeoutMs, jsonSchemaKafka.groupMinSessionTimeoutMs) &&
        Objects.equals(this.socketRequestMaxBytes, jsonSchemaKafka.socketRequestMaxBytes) &&
        Objects.equals(this.logSegmentBytes, jsonSchemaKafka.logSegmentBytes) &&
        Objects.equals(this.logCleanupAndCompaction, jsonSchemaKafka.logCleanupAndCompaction) &&
        Objects.equals(this.offsetsRetentionMinutes, jsonSchemaKafka.offsetsRetentionMinutes) &&
        Objects.equals(this.logRetentionMs, jsonSchemaKafka.logRetentionMs);
  }

  @Override
  public int hashCode() {
    return Objects.hash(saslOauthbearerExpectedAudience, groupMaxSessionTimeoutMs, logFlushIntervalMessages, saslOauthbearerJwksEndpointUrl, maxConnectionsPerIp, saslOauthbearerExpectedIssuer, logIndexSizeMaxBytes, autoCreateTopicsEnable, logIndexIntervalBytes, replicaFetchMaxBytes, numPartitions, transactionStateLogSegmentBytes, replicaFetchResponseMaxBytes, logMessageTimestampType, connectionsMaxIdleMs, logFlushIntervalMs, logPreallocate, logSegmentDeleteDelayMs, messageMaxBytes, groupInitialRebalanceDelayMs, logLocalRetentionBytes, logRollJitterMs, transactionRemoveExpiredTransactionCleanupIntervalMs, defaultReplicationFactor, logRollMs, producerPurgatoryPurgeIntervalRequests, logRetentionBytes, minInsyncReplicas, compressionType, logMessageTimestampDifferenceMaxMs, logLocalRetentionMs, logMessageDownconversionEnable, saslOauthbearerSubClaimName, maxIncrementalFetchSessionCacheSlots, logRetentionHours, groupMinSessionTimeoutMs, socketRequestMaxBytes, logSegmentBytes, logCleanupAndCompaction, offsetsRetentionMinutes, logRetentionMs);
  }


  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class JsonSchemaKafka {\n");
    
    sb.append("    saslOauthbearerExpectedAudience: ").append(toIndentedString(saslOauthbearerExpectedAudience)).append("\n");
    sb.append("    groupMaxSessionTimeoutMs: ").append(toIndentedString(groupMaxSessionTimeoutMs)).append("\n");
    sb.append("    logFlushIntervalMessages: ").append(toIndentedString(logFlushIntervalMessages)).append("\n");
    sb.append("    saslOauthbearerJwksEndpointUrl: ").append(toIndentedString(saslOauthbearerJwksEndpointUrl)).append("\n");
    sb.append("    maxConnectionsPerIp: ").append(toIndentedString(maxConnectionsPerIp)).append("\n");
    sb.append("    saslOauthbearerExpectedIssuer: ").append(toIndentedString(saslOauthbearerExpectedIssuer)).append("\n");
    sb.append("    logIndexSizeMaxBytes: ").append(toIndentedString(logIndexSizeMaxBytes)).append("\n");
    sb.append("    autoCreateTopicsEnable: ").append(toIndentedString(autoCreateTopicsEnable)).append("\n");
    sb.append("    logIndexIntervalBytes: ").append(toIndentedString(logIndexIntervalBytes)).append("\n");
    sb.append("    replicaFetchMaxBytes: ").append(toIndentedString(replicaFetchMaxBytes)).append("\n");
    sb.append("    numPartitions: ").append(toIndentedString(numPartitions)).append("\n");
    sb.append("    transactionStateLogSegmentBytes: ").append(toIndentedString(transactionStateLogSegmentBytes)).append("\n");
    sb.append("    replicaFetchResponseMaxBytes: ").append(toIndentedString(replicaFetchResponseMaxBytes)).append("\n");
    sb.append("    logMessageTimestampType: ").append(toIndentedString(logMessageTimestampType)).append("\n");
    sb.append("    connectionsMaxIdleMs: ").append(toIndentedString(connectionsMaxIdleMs)).append("\n");
    sb.append("    logFlushIntervalMs: ").append(toIndentedString(logFlushIntervalMs)).append("\n");
    sb.append("    logPreallocate: ").append(toIndentedString(logPreallocate)).append("\n");
    sb.append("    logSegmentDeleteDelayMs: ").append(toIndentedString(logSegmentDeleteDelayMs)).append("\n");
    sb.append("    messageMaxBytes: ").append(toIndentedString(messageMaxBytes)).append("\n");
    sb.append("    groupInitialRebalanceDelayMs: ").append(toIndentedString(groupInitialRebalanceDelayMs)).append("\n");
    sb.append("    logLocalRetentionBytes: ").append(toIndentedString(logLocalRetentionBytes)).append("\n");
    sb.append("    logRollJitterMs: ").append(toIndentedString(logRollJitterMs)).append("\n");
    sb.append("    transactionRemoveExpiredTransactionCleanupIntervalMs: ").append(toIndentedString(transactionRemoveExpiredTransactionCleanupIntervalMs)).append("\n");
    sb.append("    defaultReplicationFactor: ").append(toIndentedString(defaultReplicationFactor)).append("\n");
    sb.append("    logRollMs: ").append(toIndentedString(logRollMs)).append("\n");
    sb.append("    producerPurgatoryPurgeIntervalRequests: ").append(toIndentedString(producerPurgatoryPurgeIntervalRequests)).append("\n");
    sb.append("    logRetentionBytes: ").append(toIndentedString(logRetentionBytes)).append("\n");
    sb.append("    minInsyncReplicas: ").append(toIndentedString(minInsyncReplicas)).append("\n");
    sb.append("    compressionType: ").append(toIndentedString(compressionType)).append("\n");
    sb.append("    logMessageTimestampDifferenceMaxMs: ").append(toIndentedString(logMessageTimestampDifferenceMaxMs)).append("\n");
    sb.append("    logLocalRetentionMs: ").append(toIndentedString(logLocalRetentionMs)).append("\n");
    sb.append("    logMessageDownconversionEnable: ").append(toIndentedString(logMessageDownconversionEnable)).append("\n");
    sb.append("    saslOauthbearerSubClaimName: ").append(toIndentedString(saslOauthbearerSubClaimName)).append("\n");
    sb.append("    maxIncrementalFetchSessionCacheSlots: ").append(toIndentedString(maxIncrementalFetchSessionCacheSlots)).append("\n");
    sb.append("    logRetentionHours: ").append(toIndentedString(logRetentionHours)).append("\n");
    sb.append("    groupMinSessionTimeoutMs: ").append(toIndentedString(groupMinSessionTimeoutMs)).append("\n");
    sb.append("    socketRequestMaxBytes: ").append(toIndentedString(socketRequestMaxBytes)).append("\n");
    sb.append("    logSegmentBytes: ").append(toIndentedString(logSegmentBytes)).append("\n");
    sb.append("    logCleanupAndCompaction: ").append(toIndentedString(logCleanupAndCompaction)).append("\n");
    sb.append("    offsetsRetentionMinutes: ").append(toIndentedString(offsetsRetentionMinutes)).append("\n");
    sb.append("    logRetentionMs: ").append(toIndentedString(logRetentionMs)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(java.lang.Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }

}

/*
 * Exoscale Public API
 *  Infrastructure automation API, allowing programmatic access to all Exoscale products and services.  The [OpenAPI Specification](http://spec.openapis.org/oas/v3.0.3.html) source of this documentation can be obtained here:  * [JSON format](https://openapi-v2.exoscale.com/source.json) * [YAML format](https://openapi-v2.exoscale.com/source.yaml)
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: api@exoscale.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.Arrays;
import org.openapitools.client.model.ConfigureLogCleanerForTopicCompaction;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.lang.reflect.Type;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * JsonSchemaKafka
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-03-27T17:14:44.205710495Z[Etc/UTC]", comments = "Generator version: 7.5.0-SNAPSHOT")
public class JsonSchemaKafka {
  public static final String SERIALIZED_NAME_SASL_OAUTHBEARER_EXPECTED_AUDIENCE = "sasl_oauthbearer_expected_audience";
  @SerializedName(SERIALIZED_NAME_SASL_OAUTHBEARER_EXPECTED_AUDIENCE)
  private String saslOauthbearerExpectedAudience;

  public static final String SERIALIZED_NAME_GROUP_MAX_SESSION_TIMEOUT_MS = "group_max_session_timeout_ms";
  @SerializedName(SERIALIZED_NAME_GROUP_MAX_SESSION_TIMEOUT_MS)
  private Integer groupMaxSessionTimeoutMs;

  public static final String SERIALIZED_NAME_LOG_FLUSH_INTERVAL_MESSAGES = "log_flush_interval_messages";
  @SerializedName(SERIALIZED_NAME_LOG_FLUSH_INTERVAL_MESSAGES)
  private Integer logFlushIntervalMessages;

  public static final String SERIALIZED_NAME_SASL_OAUTHBEARER_JWKS_ENDPOINT_URL = "sasl_oauthbearer_jwks_endpoint_url";
  @SerializedName(SERIALIZED_NAME_SASL_OAUTHBEARER_JWKS_ENDPOINT_URL)
  private String saslOauthbearerJwksEndpointUrl;

  public static final String SERIALIZED_NAME_MAX_CONNECTIONS_PER_IP = "max_connections_per_ip";
  @SerializedName(SERIALIZED_NAME_MAX_CONNECTIONS_PER_IP)
  private Integer maxConnectionsPerIp;

  public static final String SERIALIZED_NAME_SASL_OAUTHBEARER_EXPECTED_ISSUER = "sasl_oauthbearer_expected_issuer";
  @SerializedName(SERIALIZED_NAME_SASL_OAUTHBEARER_EXPECTED_ISSUER)
  private String saslOauthbearerExpectedIssuer;

  public static final String SERIALIZED_NAME_LOG_INDEX_SIZE_MAX_BYTES = "log_index_size_max_bytes";
  @SerializedName(SERIALIZED_NAME_LOG_INDEX_SIZE_MAX_BYTES)
  private Integer logIndexSizeMaxBytes;

  public static final String SERIALIZED_NAME_AUTO_CREATE_TOPICS_ENABLE = "auto_create_topics_enable";
  @SerializedName(SERIALIZED_NAME_AUTO_CREATE_TOPICS_ENABLE)
  private Boolean autoCreateTopicsEnable;

  public static final String SERIALIZED_NAME_LOG_INDEX_INTERVAL_BYTES = "log_index_interval_bytes";
  @SerializedName(SERIALIZED_NAME_LOG_INDEX_INTERVAL_BYTES)
  private Integer logIndexIntervalBytes;

  public static final String SERIALIZED_NAME_REPLICA_FETCH_MAX_BYTES = "replica_fetch_max_bytes";
  @SerializedName(SERIALIZED_NAME_REPLICA_FETCH_MAX_BYTES)
  private Integer replicaFetchMaxBytes;

  public static final String SERIALIZED_NAME_NUM_PARTITIONS = "num_partitions";
  @SerializedName(SERIALIZED_NAME_NUM_PARTITIONS)
  private Integer numPartitions;

  public static final String SERIALIZED_NAME_TRANSACTION_STATE_LOG_SEGMENT_BYTES = "transaction_state_log_segment_bytes";
  @SerializedName(SERIALIZED_NAME_TRANSACTION_STATE_LOG_SEGMENT_BYTES)
  private Integer transactionStateLogSegmentBytes;

  public static final String SERIALIZED_NAME_REPLICA_FETCH_RESPONSE_MAX_BYTES = "replica_fetch_response_max_bytes";
  @SerializedName(SERIALIZED_NAME_REPLICA_FETCH_RESPONSE_MAX_BYTES)
  private Integer replicaFetchResponseMaxBytes;

  /**
   * Define whether the timestamp in the message is message create time or log append time.
   */
  @JsonAdapter(LogMessageTimestampTypeEnum.Adapter.class)
  public enum LogMessageTimestampTypeEnum {
    CREATETIME("CreateTime"),
    
    LOGAPPENDTIME("LogAppendTime");

    private String value;

    LogMessageTimestampTypeEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static LogMessageTimestampTypeEnum fromValue(String value) {
      for (LogMessageTimestampTypeEnum b : LogMessageTimestampTypeEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<LogMessageTimestampTypeEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final LogMessageTimestampTypeEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public LogMessageTimestampTypeEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return LogMessageTimestampTypeEnum.fromValue(value);
      }
    }

    public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      String value = jsonElement.getAsString();
      LogMessageTimestampTypeEnum.fromValue(value);
    }
  }

  public static final String SERIALIZED_NAME_LOG_MESSAGE_TIMESTAMP_TYPE = "log_message_timestamp_type";
  @SerializedName(SERIALIZED_NAME_LOG_MESSAGE_TIMESTAMP_TYPE)
  private LogMessageTimestampTypeEnum logMessageTimestampType;

  public static final String SERIALIZED_NAME_CONNECTIONS_MAX_IDLE_MS = "connections_max_idle_ms";
  @SerializedName(SERIALIZED_NAME_CONNECTIONS_MAX_IDLE_MS)
  private Integer connectionsMaxIdleMs;

  public static final String SERIALIZED_NAME_LOG_FLUSH_INTERVAL_MS = "log_flush_interval_ms";
  @SerializedName(SERIALIZED_NAME_LOG_FLUSH_INTERVAL_MS)
  private Integer logFlushIntervalMs;

  public static final String SERIALIZED_NAME_LOG_PREALLOCATE = "log_preallocate";
  @SerializedName(SERIALIZED_NAME_LOG_PREALLOCATE)
  private Boolean logPreallocate;

  public static final String SERIALIZED_NAME_LOG_SEGMENT_DELETE_DELAY_MS = "log_segment_delete_delay_ms";
  @SerializedName(SERIALIZED_NAME_LOG_SEGMENT_DELETE_DELAY_MS)
  private Integer logSegmentDeleteDelayMs;

  public static final String SERIALIZED_NAME_MESSAGE_MAX_BYTES = "message_max_bytes";
  @SerializedName(SERIALIZED_NAME_MESSAGE_MAX_BYTES)
  private Integer messageMaxBytes;

  public static final String SERIALIZED_NAME_GROUP_INITIAL_REBALANCE_DELAY_MS = "group_initial_rebalance_delay_ms";
  @SerializedName(SERIALIZED_NAME_GROUP_INITIAL_REBALANCE_DELAY_MS)
  private Integer groupInitialRebalanceDelayMs;

  public static final String SERIALIZED_NAME_LOG_LOCAL_RETENTION_BYTES = "log_local_retention_bytes";
  @SerializedName(SERIALIZED_NAME_LOG_LOCAL_RETENTION_BYTES)
  private Integer logLocalRetentionBytes;

  public static final String SERIALIZED_NAME_LOG_ROLL_JITTER_MS = "log_roll_jitter_ms";
  @SerializedName(SERIALIZED_NAME_LOG_ROLL_JITTER_MS)
  private Integer logRollJitterMs;

  public static final String SERIALIZED_NAME_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS = "transaction_remove_expired_transaction_cleanup_interval_ms";
  @SerializedName(SERIALIZED_NAME_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS)
  private Integer transactionRemoveExpiredTransactionCleanupIntervalMs;

  public static final String SERIALIZED_NAME_DEFAULT_REPLICATION_FACTOR = "default_replication_factor";
  @SerializedName(SERIALIZED_NAME_DEFAULT_REPLICATION_FACTOR)
  private Integer defaultReplicationFactor;

  public static final String SERIALIZED_NAME_LOG_ROLL_MS = "log_roll_ms";
  @SerializedName(SERIALIZED_NAME_LOG_ROLL_MS)
  private Integer logRollMs;

  public static final String SERIALIZED_NAME_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS = "producer_purgatory_purge_interval_requests";
  @SerializedName(SERIALIZED_NAME_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS)
  private Integer producerPurgatoryPurgeIntervalRequests;

  public static final String SERIALIZED_NAME_LOG_RETENTION_BYTES = "log_retention_bytes";
  @SerializedName(SERIALIZED_NAME_LOG_RETENTION_BYTES)
  private Integer logRetentionBytes;

  public static final String SERIALIZED_NAME_MIN_INSYNC_REPLICAS = "min_insync_replicas";
  @SerializedName(SERIALIZED_NAME_MIN_INSYNC_REPLICAS)
  private Integer minInsyncReplicas;

  /**
   * Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (&#39;gzip&#39;, &#39;snappy&#39;, &#39;lz4&#39;, &#39;zstd&#39;). It additionally accepts &#39;uncompressed&#39; which is equivalent to no compression; and &#39;producer&#39; which means retain the original compression codec set by the producer.
   */
  @JsonAdapter(CompressionTypeEnum.Adapter.class)
  public enum CompressionTypeEnum {
    GZIP("gzip"),
    
    SNAPPY("snappy"),
    
    LZ4("lz4"),
    
    ZSTD("zstd"),
    
    UNCOMPRESSED("uncompressed"),
    
    PRODUCER("producer");

    private String value;

    CompressionTypeEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static CompressionTypeEnum fromValue(String value) {
      for (CompressionTypeEnum b : CompressionTypeEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<CompressionTypeEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final CompressionTypeEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public CompressionTypeEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return CompressionTypeEnum.fromValue(value);
      }
    }

    public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      String value = jsonElement.getAsString();
      CompressionTypeEnum.fromValue(value);
    }
  }

  public static final String SERIALIZED_NAME_COMPRESSION_TYPE = "compression_type";
  @SerializedName(SERIALIZED_NAME_COMPRESSION_TYPE)
  private CompressionTypeEnum compressionType;

  public static final String SERIALIZED_NAME_LOG_MESSAGE_TIMESTAMP_DIFFERENCE_MAX_MS = "log_message_timestamp_difference_max_ms";
  @SerializedName(SERIALIZED_NAME_LOG_MESSAGE_TIMESTAMP_DIFFERENCE_MAX_MS)
  private Integer logMessageTimestampDifferenceMaxMs;

  public static final String SERIALIZED_NAME_LOG_LOCAL_RETENTION_MS = "log_local_retention_ms";
  @SerializedName(SERIALIZED_NAME_LOG_LOCAL_RETENTION_MS)
  private Integer logLocalRetentionMs;

  public static final String SERIALIZED_NAME_LOG_MESSAGE_DOWNCONVERSION_ENABLE = "log_message_downconversion_enable";
  @SerializedName(SERIALIZED_NAME_LOG_MESSAGE_DOWNCONVERSION_ENABLE)
  private Boolean logMessageDownconversionEnable;

  public static final String SERIALIZED_NAME_SASL_OAUTHBEARER_SUB_CLAIM_NAME = "sasl_oauthbearer_sub_claim_name";
  @SerializedName(SERIALIZED_NAME_SASL_OAUTHBEARER_SUB_CLAIM_NAME)
  private String saslOauthbearerSubClaimName;

  public static final String SERIALIZED_NAME_MAX_INCREMENTAL_FETCH_SESSION_CACHE_SLOTS = "max_incremental_fetch_session_cache_slots";
  @SerializedName(SERIALIZED_NAME_MAX_INCREMENTAL_FETCH_SESSION_CACHE_SLOTS)
  private Integer maxIncrementalFetchSessionCacheSlots;

  public static final String SERIALIZED_NAME_LOG_RETENTION_HOURS = "log_retention_hours";
  @SerializedName(SERIALIZED_NAME_LOG_RETENTION_HOURS)
  private Integer logRetentionHours;

  public static final String SERIALIZED_NAME_GROUP_MIN_SESSION_TIMEOUT_MS = "group_min_session_timeout_ms";
  @SerializedName(SERIALIZED_NAME_GROUP_MIN_SESSION_TIMEOUT_MS)
  private Integer groupMinSessionTimeoutMs;

  public static final String SERIALIZED_NAME_SOCKET_REQUEST_MAX_BYTES = "socket_request_max_bytes";
  @SerializedName(SERIALIZED_NAME_SOCKET_REQUEST_MAX_BYTES)
  private Integer socketRequestMaxBytes;

  public static final String SERIALIZED_NAME_LOG_SEGMENT_BYTES = "log_segment_bytes";
  @SerializedName(SERIALIZED_NAME_LOG_SEGMENT_BYTES)
  private Integer logSegmentBytes;

  public static final String SERIALIZED_NAME_LOG_CLEANUP_AND_COMPACTION = "log-cleanup-and-compaction";
  @SerializedName(SERIALIZED_NAME_LOG_CLEANUP_AND_COMPACTION)
  private ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction;

  public static final String SERIALIZED_NAME_OFFSETS_RETENTION_MINUTES = "offsets_retention_minutes";
  @SerializedName(SERIALIZED_NAME_OFFSETS_RETENTION_MINUTES)
  private Integer offsetsRetentionMinutes;

  public static final String SERIALIZED_NAME_LOG_RETENTION_MS = "log_retention_ms";
  @SerializedName(SERIALIZED_NAME_LOG_RETENTION_MS)
  private Integer logRetentionMs;

  public JsonSchemaKafka() {
  }

  public JsonSchemaKafka saslOauthbearerExpectedAudience(String saslOauthbearerExpectedAudience) {
    this.saslOauthbearerExpectedAudience = saslOauthbearerExpectedAudience;
    return this;
  }

   /**
   * The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences.
   * @return saslOauthbearerExpectedAudience
  **/
  @javax.annotation.Nullable
  public String getSaslOauthbearerExpectedAudience() {
    return saslOauthbearerExpectedAudience;
  }

  public void setSaslOauthbearerExpectedAudience(String saslOauthbearerExpectedAudience) {
    this.saslOauthbearerExpectedAudience = saslOauthbearerExpectedAudience;
  }


  public JsonSchemaKafka groupMaxSessionTimeoutMs(Integer groupMaxSessionTimeoutMs) {
    this.groupMaxSessionTimeoutMs = groupMaxSessionTimeoutMs;
    return this;
  }

   /**
   * The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
   * minimum: 0
   * maximum: 1800000
   * @return groupMaxSessionTimeoutMs
  **/
  @javax.annotation.Nullable
  public Integer getGroupMaxSessionTimeoutMs() {
    return groupMaxSessionTimeoutMs;
  }

  public void setGroupMaxSessionTimeoutMs(Integer groupMaxSessionTimeoutMs) {
    this.groupMaxSessionTimeoutMs = groupMaxSessionTimeoutMs;
  }


  public JsonSchemaKafka logFlushIntervalMessages(Integer logFlushIntervalMessages) {
    this.logFlushIntervalMessages = logFlushIntervalMessages;
    return this;
  }

   /**
   * The number of messages accumulated on a log partition before messages are flushed to disk
   * minimum: 1
   * maximum: 9223372036854775807
   * @return logFlushIntervalMessages
  **/
  @javax.annotation.Nullable
  public Integer getLogFlushIntervalMessages() {
    return logFlushIntervalMessages;
  }

  public void setLogFlushIntervalMessages(Integer logFlushIntervalMessages) {
    this.logFlushIntervalMessages = logFlushIntervalMessages;
  }


  public JsonSchemaKafka saslOauthbearerJwksEndpointUrl(String saslOauthbearerJwksEndpointUrl) {
    this.saslOauthbearerJwksEndpointUrl = saslOauthbearerJwksEndpointUrl;
    return this;
  }

   /**
   * OIDC JWKS endpoint URL. By setting this the SASL SSL OAuth2/OIDC authentication is enabled. See also other options for SASL OAuth2/OIDC. 
   * @return saslOauthbearerJwksEndpointUrl
  **/
  @javax.annotation.Nullable
  public String getSaslOauthbearerJwksEndpointUrl() {
    return saslOauthbearerJwksEndpointUrl;
  }

  public void setSaslOauthbearerJwksEndpointUrl(String saslOauthbearerJwksEndpointUrl) {
    this.saslOauthbearerJwksEndpointUrl = saslOauthbearerJwksEndpointUrl;
  }


  public JsonSchemaKafka maxConnectionsPerIp(Integer maxConnectionsPerIp) {
    this.maxConnectionsPerIp = maxConnectionsPerIp;
    return this;
  }

   /**
   * The maximum number of connections allowed from each ip address (defaults to 2147483647).
   * minimum: 256
   * maximum: 2147483647
   * @return maxConnectionsPerIp
  **/
  @javax.annotation.Nullable
  public Integer getMaxConnectionsPerIp() {
    return maxConnectionsPerIp;
  }

  public void setMaxConnectionsPerIp(Integer maxConnectionsPerIp) {
    this.maxConnectionsPerIp = maxConnectionsPerIp;
  }


  public JsonSchemaKafka saslOauthbearerExpectedIssuer(String saslOauthbearerExpectedIssuer) {
    this.saslOauthbearerExpectedIssuer = saslOauthbearerExpectedIssuer;
    return this;
  }

   /**
   * Optional setting for the broker to use to verify that the JWT was created by the expected issuer.
   * @return saslOauthbearerExpectedIssuer
  **/
  @javax.annotation.Nullable
  public String getSaslOauthbearerExpectedIssuer() {
    return saslOauthbearerExpectedIssuer;
  }

  public void setSaslOauthbearerExpectedIssuer(String saslOauthbearerExpectedIssuer) {
    this.saslOauthbearerExpectedIssuer = saslOauthbearerExpectedIssuer;
  }


  public JsonSchemaKafka logIndexSizeMaxBytes(Integer logIndexSizeMaxBytes) {
    this.logIndexSizeMaxBytes = logIndexSizeMaxBytes;
    return this;
  }

   /**
   * The maximum size in bytes of the offset index
   * minimum: 1048576
   * maximum: 104857600
   * @return logIndexSizeMaxBytes
  **/
  @javax.annotation.Nullable
  public Integer getLogIndexSizeMaxBytes() {
    return logIndexSizeMaxBytes;
  }

  public void setLogIndexSizeMaxBytes(Integer logIndexSizeMaxBytes) {
    this.logIndexSizeMaxBytes = logIndexSizeMaxBytes;
  }


  public JsonSchemaKafka autoCreateTopicsEnable(Boolean autoCreateTopicsEnable) {
    this.autoCreateTopicsEnable = autoCreateTopicsEnable;
    return this;
  }

   /**
   * Enable auto creation of topics
   * @return autoCreateTopicsEnable
  **/
  @javax.annotation.Nullable
  public Boolean getAutoCreateTopicsEnable() {
    return autoCreateTopicsEnable;
  }

  public void setAutoCreateTopicsEnable(Boolean autoCreateTopicsEnable) {
    this.autoCreateTopicsEnable = autoCreateTopicsEnable;
  }


  public JsonSchemaKafka logIndexIntervalBytes(Integer logIndexIntervalBytes) {
    this.logIndexIntervalBytes = logIndexIntervalBytes;
    return this;
  }

   /**
   * The interval with which Kafka adds an entry to the offset index
   * minimum: 0
   * maximum: 104857600
   * @return logIndexIntervalBytes
  **/
  @javax.annotation.Nullable
  public Integer getLogIndexIntervalBytes() {
    return logIndexIntervalBytes;
  }

  public void setLogIndexIntervalBytes(Integer logIndexIntervalBytes) {
    this.logIndexIntervalBytes = logIndexIntervalBytes;
  }


  public JsonSchemaKafka replicaFetchMaxBytes(Integer replicaFetchMaxBytes) {
    this.replicaFetchMaxBytes = replicaFetchMaxBytes;
    return this;
  }

   /**
   * The number of bytes of messages to attempt to fetch for each partition (defaults to 1048576). This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made.
   * minimum: 1048576
   * maximum: 104857600
   * @return replicaFetchMaxBytes
  **/
  @javax.annotation.Nullable
  public Integer getReplicaFetchMaxBytes() {
    return replicaFetchMaxBytes;
  }

  public void setReplicaFetchMaxBytes(Integer replicaFetchMaxBytes) {
    this.replicaFetchMaxBytes = replicaFetchMaxBytes;
  }


  public JsonSchemaKafka numPartitions(Integer numPartitions) {
    this.numPartitions = numPartitions;
    return this;
  }

   /**
   * Number of partitions for autocreated topics
   * minimum: 1
   * maximum: 1000
   * @return numPartitions
  **/
  @javax.annotation.Nullable
  public Integer getNumPartitions() {
    return numPartitions;
  }

  public void setNumPartitions(Integer numPartitions) {
    this.numPartitions = numPartitions;
  }


  public JsonSchemaKafka transactionStateLogSegmentBytes(Integer transactionStateLogSegmentBytes) {
    this.transactionStateLogSegmentBytes = transactionStateLogSegmentBytes;
    return this;
  }

   /**
   * The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (defaults to 104857600 (100 mebibytes)).
   * minimum: 1048576
   * maximum: 2147483647
   * @return transactionStateLogSegmentBytes
  **/
  @javax.annotation.Nullable
  public Integer getTransactionStateLogSegmentBytes() {
    return transactionStateLogSegmentBytes;
  }

  public void setTransactionStateLogSegmentBytes(Integer transactionStateLogSegmentBytes) {
    this.transactionStateLogSegmentBytes = transactionStateLogSegmentBytes;
  }


  public JsonSchemaKafka replicaFetchResponseMaxBytes(Integer replicaFetchResponseMaxBytes) {
    this.replicaFetchResponseMaxBytes = replicaFetchResponseMaxBytes;
    return this;
  }

   /**
   * Maximum bytes expected for the entire fetch response (defaults to 10485760). Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum.
   * minimum: 10485760
   * maximum: 1048576000
   * @return replicaFetchResponseMaxBytes
  **/
  @javax.annotation.Nullable
  public Integer getReplicaFetchResponseMaxBytes() {
    return replicaFetchResponseMaxBytes;
  }

  public void setReplicaFetchResponseMaxBytes(Integer replicaFetchResponseMaxBytes) {
    this.replicaFetchResponseMaxBytes = replicaFetchResponseMaxBytes;
  }


  public JsonSchemaKafka logMessageTimestampType(LogMessageTimestampTypeEnum logMessageTimestampType) {
    this.logMessageTimestampType = logMessageTimestampType;
    return this;
  }

   /**
   * Define whether the timestamp in the message is message create time or log append time.
   * @return logMessageTimestampType
  **/
  @javax.annotation.Nullable
  public LogMessageTimestampTypeEnum getLogMessageTimestampType() {
    return logMessageTimestampType;
  }

  public void setLogMessageTimestampType(LogMessageTimestampTypeEnum logMessageTimestampType) {
    this.logMessageTimestampType = logMessageTimestampType;
  }


  public JsonSchemaKafka connectionsMaxIdleMs(Integer connectionsMaxIdleMs) {
    this.connectionsMaxIdleMs = connectionsMaxIdleMs;
    return this;
  }

   /**
   * Idle connections timeout: the server socket processor threads close the connections that idle for longer than this.
   * minimum: 1000
   * maximum: 3600000
   * @return connectionsMaxIdleMs
  **/
  @javax.annotation.Nullable
  public Integer getConnectionsMaxIdleMs() {
    return connectionsMaxIdleMs;
  }

  public void setConnectionsMaxIdleMs(Integer connectionsMaxIdleMs) {
    this.connectionsMaxIdleMs = connectionsMaxIdleMs;
  }


  public JsonSchemaKafka logFlushIntervalMs(Integer logFlushIntervalMs) {
    this.logFlushIntervalMs = logFlushIntervalMs;
    return this;
  }

   /**
   * The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logFlushIntervalMs
  **/
  @javax.annotation.Nullable
  public Integer getLogFlushIntervalMs() {
    return logFlushIntervalMs;
  }

  public void setLogFlushIntervalMs(Integer logFlushIntervalMs) {
    this.logFlushIntervalMs = logFlushIntervalMs;
  }


  public JsonSchemaKafka logPreallocate(Boolean logPreallocate) {
    this.logPreallocate = logPreallocate;
    return this;
  }

   /**
   * Should pre allocate file when create new segment?
   * @return logPreallocate
  **/
  @javax.annotation.Nullable
  public Boolean getLogPreallocate() {
    return logPreallocate;
  }

  public void setLogPreallocate(Boolean logPreallocate) {
    this.logPreallocate = logPreallocate;
  }


  public JsonSchemaKafka logSegmentDeleteDelayMs(Integer logSegmentDeleteDelayMs) {
    this.logSegmentDeleteDelayMs = logSegmentDeleteDelayMs;
    return this;
  }

   /**
   * The amount of time to wait before deleting a file from the filesystem
   * minimum: 0
   * maximum: 3600000
   * @return logSegmentDeleteDelayMs
  **/
  @javax.annotation.Nullable
  public Integer getLogSegmentDeleteDelayMs() {
    return logSegmentDeleteDelayMs;
  }

  public void setLogSegmentDeleteDelayMs(Integer logSegmentDeleteDelayMs) {
    this.logSegmentDeleteDelayMs = logSegmentDeleteDelayMs;
  }


  public JsonSchemaKafka messageMaxBytes(Integer messageMaxBytes) {
    this.messageMaxBytes = messageMaxBytes;
    return this;
  }

   /**
   * The maximum size of message that the server can receive.
   * minimum: 0
   * maximum: 100001200
   * @return messageMaxBytes
  **/
  @javax.annotation.Nullable
  public Integer getMessageMaxBytes() {
    return messageMaxBytes;
  }

  public void setMessageMaxBytes(Integer messageMaxBytes) {
    this.messageMaxBytes = messageMaxBytes;
  }


  public JsonSchemaKafka groupInitialRebalanceDelayMs(Integer groupInitialRebalanceDelayMs) {
    this.groupInitialRebalanceDelayMs = groupInitialRebalanceDelayMs;
    return this;
  }

   /**
   * The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.
   * minimum: 0
   * maximum: 300000
   * @return groupInitialRebalanceDelayMs
  **/
  @javax.annotation.Nullable
  public Integer getGroupInitialRebalanceDelayMs() {
    return groupInitialRebalanceDelayMs;
  }

  public void setGroupInitialRebalanceDelayMs(Integer groupInitialRebalanceDelayMs) {
    this.groupInitialRebalanceDelayMs = groupInitialRebalanceDelayMs;
  }


  public JsonSchemaKafka logLocalRetentionBytes(Integer logLocalRetentionBytes) {
    this.logLocalRetentionBytes = logLocalRetentionBytes;
    return this;
  }

   /**
   * The maximum size of local log segments that can grow for a partition before it gets eligible for deletion. If set to -2, the value of log.retention.bytes is used. The effective value should always be less than or equal to log.retention.bytes value.
   * minimum: -2
   * maximum: 9223372036854775807
   * @return logLocalRetentionBytes
  **/
  @javax.annotation.Nullable
  public Integer getLogLocalRetentionBytes() {
    return logLocalRetentionBytes;
  }

  public void setLogLocalRetentionBytes(Integer logLocalRetentionBytes) {
    this.logLocalRetentionBytes = logLocalRetentionBytes;
  }


  public JsonSchemaKafka logRollJitterMs(Integer logRollJitterMs) {
    this.logRollJitterMs = logRollJitterMs;
    return this;
  }

   /**
   * The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logRollJitterMs
  **/
  @javax.annotation.Nullable
  public Integer getLogRollJitterMs() {
    return logRollJitterMs;
  }

  public void setLogRollJitterMs(Integer logRollJitterMs) {
    this.logRollJitterMs = logRollJitterMs;
  }


  public JsonSchemaKafka transactionRemoveExpiredTransactionCleanupIntervalMs(Integer transactionRemoveExpiredTransactionCleanupIntervalMs) {
    this.transactionRemoveExpiredTransactionCleanupIntervalMs = transactionRemoveExpiredTransactionCleanupIntervalMs;
    return this;
  }

   /**
   * The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults to 3600000 (1 hour)).
   * minimum: 600000
   * maximum: 3600000
   * @return transactionRemoveExpiredTransactionCleanupIntervalMs
  **/
  @javax.annotation.Nullable
  public Integer getTransactionRemoveExpiredTransactionCleanupIntervalMs() {
    return transactionRemoveExpiredTransactionCleanupIntervalMs;
  }

  public void setTransactionRemoveExpiredTransactionCleanupIntervalMs(Integer transactionRemoveExpiredTransactionCleanupIntervalMs) {
    this.transactionRemoveExpiredTransactionCleanupIntervalMs = transactionRemoveExpiredTransactionCleanupIntervalMs;
  }


  public JsonSchemaKafka defaultReplicationFactor(Integer defaultReplicationFactor) {
    this.defaultReplicationFactor = defaultReplicationFactor;
    return this;
  }

   /**
   * Replication factor for autocreated topics
   * minimum: 1
   * maximum: 10
   * @return defaultReplicationFactor
  **/
  @javax.annotation.Nullable
  public Integer getDefaultReplicationFactor() {
    return defaultReplicationFactor;
  }

  public void setDefaultReplicationFactor(Integer defaultReplicationFactor) {
    this.defaultReplicationFactor = defaultReplicationFactor;
  }


  public JsonSchemaKafka logRollMs(Integer logRollMs) {
    this.logRollMs = logRollMs;
    return this;
  }

   /**
   * The maximum time before a new log segment is rolled out (in milliseconds).
   * minimum: 1
   * maximum: 9223372036854775807
   * @return logRollMs
  **/
  @javax.annotation.Nullable
  public Integer getLogRollMs() {
    return logRollMs;
  }

  public void setLogRollMs(Integer logRollMs) {
    this.logRollMs = logRollMs;
  }


  public JsonSchemaKafka producerPurgatoryPurgeIntervalRequests(Integer producerPurgatoryPurgeIntervalRequests) {
    this.producerPurgatoryPurgeIntervalRequests = producerPurgatoryPurgeIntervalRequests;
    return this;
  }

   /**
   * The purge interval (in number of requests) of the producer request purgatory(defaults to 1000).
   * minimum: 10
   * maximum: 10000
   * @return producerPurgatoryPurgeIntervalRequests
  **/
  @javax.annotation.Nullable
  public Integer getProducerPurgatoryPurgeIntervalRequests() {
    return producerPurgatoryPurgeIntervalRequests;
  }

  public void setProducerPurgatoryPurgeIntervalRequests(Integer producerPurgatoryPurgeIntervalRequests) {
    this.producerPurgatoryPurgeIntervalRequests = producerPurgatoryPurgeIntervalRequests;
  }


  public JsonSchemaKafka logRetentionBytes(Integer logRetentionBytes) {
    this.logRetentionBytes = logRetentionBytes;
    return this;
  }

   /**
   * The maximum size of the log before deleting messages
   * minimum: -1
   * maximum: 9223372036854775807
   * @return logRetentionBytes
  **/
  @javax.annotation.Nullable
  public Integer getLogRetentionBytes() {
    return logRetentionBytes;
  }

  public void setLogRetentionBytes(Integer logRetentionBytes) {
    this.logRetentionBytes = logRetentionBytes;
  }


  public JsonSchemaKafka minInsyncReplicas(Integer minInsyncReplicas) {
    this.minInsyncReplicas = minInsyncReplicas;
    return this;
  }

   /**
   * When a producer sets acks to &#39;all&#39; (or &#39;-1&#39;), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful.
   * minimum: 1
   * maximum: 7
   * @return minInsyncReplicas
  **/
  @javax.annotation.Nullable
  public Integer getMinInsyncReplicas() {
    return minInsyncReplicas;
  }

  public void setMinInsyncReplicas(Integer minInsyncReplicas) {
    this.minInsyncReplicas = minInsyncReplicas;
  }


  public JsonSchemaKafka compressionType(CompressionTypeEnum compressionType) {
    this.compressionType = compressionType;
    return this;
  }

   /**
   * Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (&#39;gzip&#39;, &#39;snappy&#39;, &#39;lz4&#39;, &#39;zstd&#39;). It additionally accepts &#39;uncompressed&#39; which is equivalent to no compression; and &#39;producer&#39; which means retain the original compression codec set by the producer.
   * @return compressionType
  **/
  @javax.annotation.Nullable
  public CompressionTypeEnum getCompressionType() {
    return compressionType;
  }

  public void setCompressionType(CompressionTypeEnum compressionType) {
    this.compressionType = compressionType;
  }


  public JsonSchemaKafka logMessageTimestampDifferenceMaxMs(Integer logMessageTimestampDifferenceMaxMs) {
    this.logMessageTimestampDifferenceMaxMs = logMessageTimestampDifferenceMaxMs;
    return this;
  }

   /**
   * The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logMessageTimestampDifferenceMaxMs
  **/
  @javax.annotation.Nullable
  public Integer getLogMessageTimestampDifferenceMaxMs() {
    return logMessageTimestampDifferenceMaxMs;
  }

  public void setLogMessageTimestampDifferenceMaxMs(Integer logMessageTimestampDifferenceMaxMs) {
    this.logMessageTimestampDifferenceMaxMs = logMessageTimestampDifferenceMaxMs;
  }


  public JsonSchemaKafka logLocalRetentionMs(Integer logLocalRetentionMs) {
    this.logLocalRetentionMs = logLocalRetentionMs;
    return this;
  }

   /**
   * The number of milliseconds to keep the local log segments before it gets eligible for deletion. If set to -2, the value of log.retention.ms is used. The effective value should always be less than or equal to log.retention.ms value.
   * minimum: -2
   * maximum: 9223372036854775807
   * @return logLocalRetentionMs
  **/
  @javax.annotation.Nullable
  public Integer getLogLocalRetentionMs() {
    return logLocalRetentionMs;
  }

  public void setLogLocalRetentionMs(Integer logLocalRetentionMs) {
    this.logLocalRetentionMs = logLocalRetentionMs;
  }


  public JsonSchemaKafka logMessageDownconversionEnable(Boolean logMessageDownconversionEnable) {
    this.logMessageDownconversionEnable = logMessageDownconversionEnable;
    return this;
  }

   /**
   * This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests. 
   * @return logMessageDownconversionEnable
  **/
  @javax.annotation.Nullable
  public Boolean getLogMessageDownconversionEnable() {
    return logMessageDownconversionEnable;
  }

  public void setLogMessageDownconversionEnable(Boolean logMessageDownconversionEnable) {
    this.logMessageDownconversionEnable = logMessageDownconversionEnable;
  }


  public JsonSchemaKafka saslOauthbearerSubClaimName(String saslOauthbearerSubClaimName) {
    this.saslOauthbearerSubClaimName = saslOauthbearerSubClaimName;
    return this;
  }

   /**
   * Name of the scope from which to extract the subject claim from the JWT. Defaults to sub.
   * @return saslOauthbearerSubClaimName
  **/
  @javax.annotation.Nullable
  public String getSaslOauthbearerSubClaimName() {
    return saslOauthbearerSubClaimName;
  }

  public void setSaslOauthbearerSubClaimName(String saslOauthbearerSubClaimName) {
    this.saslOauthbearerSubClaimName = saslOauthbearerSubClaimName;
  }


  public JsonSchemaKafka maxIncrementalFetchSessionCacheSlots(Integer maxIncrementalFetchSessionCacheSlots) {
    this.maxIncrementalFetchSessionCacheSlots = maxIncrementalFetchSessionCacheSlots;
    return this;
  }

   /**
   * The maximum number of incremental fetch sessions that the broker will maintain.
   * minimum: 1000
   * maximum: 10000
   * @return maxIncrementalFetchSessionCacheSlots
  **/
  @javax.annotation.Nullable
  public Integer getMaxIncrementalFetchSessionCacheSlots() {
    return maxIncrementalFetchSessionCacheSlots;
  }

  public void setMaxIncrementalFetchSessionCacheSlots(Integer maxIncrementalFetchSessionCacheSlots) {
    this.maxIncrementalFetchSessionCacheSlots = maxIncrementalFetchSessionCacheSlots;
  }


  public JsonSchemaKafka logRetentionHours(Integer logRetentionHours) {
    this.logRetentionHours = logRetentionHours;
    return this;
  }

   /**
   * The number of hours to keep a log file before deleting it
   * minimum: -1
   * maximum: 2147483647
   * @return logRetentionHours
  **/
  @javax.annotation.Nullable
  public Integer getLogRetentionHours() {
    return logRetentionHours;
  }

  public void setLogRetentionHours(Integer logRetentionHours) {
    this.logRetentionHours = logRetentionHours;
  }


  public JsonSchemaKafka groupMinSessionTimeoutMs(Integer groupMinSessionTimeoutMs) {
    this.groupMinSessionTimeoutMs = groupMinSessionTimeoutMs;
    return this;
  }

   /**
   * The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
   * minimum: 0
   * maximum: 60000
   * @return groupMinSessionTimeoutMs
  **/
  @javax.annotation.Nullable
  public Integer getGroupMinSessionTimeoutMs() {
    return groupMinSessionTimeoutMs;
  }

  public void setGroupMinSessionTimeoutMs(Integer groupMinSessionTimeoutMs) {
    this.groupMinSessionTimeoutMs = groupMinSessionTimeoutMs;
  }


  public JsonSchemaKafka socketRequestMaxBytes(Integer socketRequestMaxBytes) {
    this.socketRequestMaxBytes = socketRequestMaxBytes;
    return this;
  }

   /**
   * The maximum number of bytes in a socket request (defaults to 104857600).
   * minimum: 10485760
   * maximum: 209715200
   * @return socketRequestMaxBytes
  **/
  @javax.annotation.Nullable
  public Integer getSocketRequestMaxBytes() {
    return socketRequestMaxBytes;
  }

  public void setSocketRequestMaxBytes(Integer socketRequestMaxBytes) {
    this.socketRequestMaxBytes = socketRequestMaxBytes;
  }


  public JsonSchemaKafka logSegmentBytes(Integer logSegmentBytes) {
    this.logSegmentBytes = logSegmentBytes;
    return this;
  }

   /**
   * The maximum size of a single log file
   * minimum: 10485760
   * maximum: 1073741824
   * @return logSegmentBytes
  **/
  @javax.annotation.Nullable
  public Integer getLogSegmentBytes() {
    return logSegmentBytes;
  }

  public void setLogSegmentBytes(Integer logSegmentBytes) {
    this.logSegmentBytes = logSegmentBytes;
  }


  public JsonSchemaKafka logCleanupAndCompaction(ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction) {
    this.logCleanupAndCompaction = logCleanupAndCompaction;
    return this;
  }

   /**
   * Get logCleanupAndCompaction
   * @return logCleanupAndCompaction
  **/
  @javax.annotation.Nullable
  public ConfigureLogCleanerForTopicCompaction getLogCleanupAndCompaction() {
    return logCleanupAndCompaction;
  }

  public void setLogCleanupAndCompaction(ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction) {
    this.logCleanupAndCompaction = logCleanupAndCompaction;
  }


  public JsonSchemaKafka offsetsRetentionMinutes(Integer offsetsRetentionMinutes) {
    this.offsetsRetentionMinutes = offsetsRetentionMinutes;
    return this;
  }

   /**
   * Log retention window in minutes for offsets topic
   * minimum: 1
   * maximum: 2147483647
   * @return offsetsRetentionMinutes
  **/
  @javax.annotation.Nullable
  public Integer getOffsetsRetentionMinutes() {
    return offsetsRetentionMinutes;
  }

  public void setOffsetsRetentionMinutes(Integer offsetsRetentionMinutes) {
    this.offsetsRetentionMinutes = offsetsRetentionMinutes;
  }


  public JsonSchemaKafka logRetentionMs(Integer logRetentionMs) {
    this.logRetentionMs = logRetentionMs;
    return this;
  }

   /**
   * The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.
   * minimum: -1
   * maximum: 9223372036854775807
   * @return logRetentionMs
  **/
  @javax.annotation.Nullable
  public Integer getLogRetentionMs() {
    return logRetentionMs;
  }

  public void setLogRetentionMs(Integer logRetentionMs) {
    this.logRetentionMs = logRetentionMs;
  }



  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    JsonSchemaKafka jsonSchemaKafka = (JsonSchemaKafka) o;
    return Objects.equals(this.saslOauthbearerExpectedAudience, jsonSchemaKafka.saslOauthbearerExpectedAudience) &&
        Objects.equals(this.groupMaxSessionTimeoutMs, jsonSchemaKafka.groupMaxSessionTimeoutMs) &&
        Objects.equals(this.logFlushIntervalMessages, jsonSchemaKafka.logFlushIntervalMessages) &&
        Objects.equals(this.saslOauthbearerJwksEndpointUrl, jsonSchemaKafka.saslOauthbearerJwksEndpointUrl) &&
        Objects.equals(this.maxConnectionsPerIp, jsonSchemaKafka.maxConnectionsPerIp) &&
        Objects.equals(this.saslOauthbearerExpectedIssuer, jsonSchemaKafka.saslOauthbearerExpectedIssuer) &&
        Objects.equals(this.logIndexSizeMaxBytes, jsonSchemaKafka.logIndexSizeMaxBytes) &&
        Objects.equals(this.autoCreateTopicsEnable, jsonSchemaKafka.autoCreateTopicsEnable) &&
        Objects.equals(this.logIndexIntervalBytes, jsonSchemaKafka.logIndexIntervalBytes) &&
        Objects.equals(this.replicaFetchMaxBytes, jsonSchemaKafka.replicaFetchMaxBytes) &&
        Objects.equals(this.numPartitions, jsonSchemaKafka.numPartitions) &&
        Objects.equals(this.transactionStateLogSegmentBytes, jsonSchemaKafka.transactionStateLogSegmentBytes) &&
        Objects.equals(this.replicaFetchResponseMaxBytes, jsonSchemaKafka.replicaFetchResponseMaxBytes) &&
        Objects.equals(this.logMessageTimestampType, jsonSchemaKafka.logMessageTimestampType) &&
        Objects.equals(this.connectionsMaxIdleMs, jsonSchemaKafka.connectionsMaxIdleMs) &&
        Objects.equals(this.logFlushIntervalMs, jsonSchemaKafka.logFlushIntervalMs) &&
        Objects.equals(this.logPreallocate, jsonSchemaKafka.logPreallocate) &&
        Objects.equals(this.logSegmentDeleteDelayMs, jsonSchemaKafka.logSegmentDeleteDelayMs) &&
        Objects.equals(this.messageMaxBytes, jsonSchemaKafka.messageMaxBytes) &&
        Objects.equals(this.groupInitialRebalanceDelayMs, jsonSchemaKafka.groupInitialRebalanceDelayMs) &&
        Objects.equals(this.logLocalRetentionBytes, jsonSchemaKafka.logLocalRetentionBytes) &&
        Objects.equals(this.logRollJitterMs, jsonSchemaKafka.logRollJitterMs) &&
        Objects.equals(this.transactionRemoveExpiredTransactionCleanupIntervalMs, jsonSchemaKafka.transactionRemoveExpiredTransactionCleanupIntervalMs) &&
        Objects.equals(this.defaultReplicationFactor, jsonSchemaKafka.defaultReplicationFactor) &&
        Objects.equals(this.logRollMs, jsonSchemaKafka.logRollMs) &&
        Objects.equals(this.producerPurgatoryPurgeIntervalRequests, jsonSchemaKafka.producerPurgatoryPurgeIntervalRequests) &&
        Objects.equals(this.logRetentionBytes, jsonSchemaKafka.logRetentionBytes) &&
        Objects.equals(this.minInsyncReplicas, jsonSchemaKafka.minInsyncReplicas) &&
        Objects.equals(this.compressionType, jsonSchemaKafka.compressionType) &&
        Objects.equals(this.logMessageTimestampDifferenceMaxMs, jsonSchemaKafka.logMessageTimestampDifferenceMaxMs) &&
        Objects.equals(this.logLocalRetentionMs, jsonSchemaKafka.logLocalRetentionMs) &&
        Objects.equals(this.logMessageDownconversionEnable, jsonSchemaKafka.logMessageDownconversionEnable) &&
        Objects.equals(this.saslOauthbearerSubClaimName, jsonSchemaKafka.saslOauthbearerSubClaimName) &&
        Objects.equals(this.maxIncrementalFetchSessionCacheSlots, jsonSchemaKafka.maxIncrementalFetchSessionCacheSlots) &&
        Objects.equals(this.logRetentionHours, jsonSchemaKafka.logRetentionHours) &&
        Objects.equals(this.groupMinSessionTimeoutMs, jsonSchemaKafka.groupMinSessionTimeoutMs) &&
        Objects.equals(this.socketRequestMaxBytes, jsonSchemaKafka.socketRequestMaxBytes) &&
        Objects.equals(this.logSegmentBytes, jsonSchemaKafka.logSegmentBytes) &&
        Objects.equals(this.logCleanupAndCompaction, jsonSchemaKafka.logCleanupAndCompaction) &&
        Objects.equals(this.offsetsRetentionMinutes, jsonSchemaKafka.offsetsRetentionMinutes) &&
        Objects.equals(this.logRetentionMs, jsonSchemaKafka.logRetentionMs);
  }

  @Override
  public int hashCode() {
    return Objects.hash(saslOauthbearerExpectedAudience, groupMaxSessionTimeoutMs, logFlushIntervalMessages, saslOauthbearerJwksEndpointUrl, maxConnectionsPerIp, saslOauthbearerExpectedIssuer, logIndexSizeMaxBytes, autoCreateTopicsEnable, logIndexIntervalBytes, replicaFetchMaxBytes, numPartitions, transactionStateLogSegmentBytes, replicaFetchResponseMaxBytes, logMessageTimestampType, connectionsMaxIdleMs, logFlushIntervalMs, logPreallocate, logSegmentDeleteDelayMs, messageMaxBytes, groupInitialRebalanceDelayMs, logLocalRetentionBytes, logRollJitterMs, transactionRemoveExpiredTransactionCleanupIntervalMs, defaultReplicationFactor, logRollMs, producerPurgatoryPurgeIntervalRequests, logRetentionBytes, minInsyncReplicas, compressionType, logMessageTimestampDifferenceMaxMs, logLocalRetentionMs, logMessageDownconversionEnable, saslOauthbearerSubClaimName, maxIncrementalFetchSessionCacheSlots, logRetentionHours, groupMinSessionTimeoutMs, socketRequestMaxBytes, logSegmentBytes, logCleanupAndCompaction, offsetsRetentionMinutes, logRetentionMs);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class JsonSchemaKafka {\n");
    sb.append("    saslOauthbearerExpectedAudience: ").append(toIndentedString(saslOauthbearerExpectedAudience)).append("\n");
    sb.append("    groupMaxSessionTimeoutMs: ").append(toIndentedString(groupMaxSessionTimeoutMs)).append("\n");
    sb.append("    logFlushIntervalMessages: ").append(toIndentedString(logFlushIntervalMessages)).append("\n");
    sb.append("    saslOauthbearerJwksEndpointUrl: ").append(toIndentedString(saslOauthbearerJwksEndpointUrl)).append("\n");
    sb.append("    maxConnectionsPerIp: ").append(toIndentedString(maxConnectionsPerIp)).append("\n");
    sb.append("    saslOauthbearerExpectedIssuer: ").append(toIndentedString(saslOauthbearerExpectedIssuer)).append("\n");
    sb.append("    logIndexSizeMaxBytes: ").append(toIndentedString(logIndexSizeMaxBytes)).append("\n");
    sb.append("    autoCreateTopicsEnable: ").append(toIndentedString(autoCreateTopicsEnable)).append("\n");
    sb.append("    logIndexIntervalBytes: ").append(toIndentedString(logIndexIntervalBytes)).append("\n");
    sb.append("    replicaFetchMaxBytes: ").append(toIndentedString(replicaFetchMaxBytes)).append("\n");
    sb.append("    numPartitions: ").append(toIndentedString(numPartitions)).append("\n");
    sb.append("    transactionStateLogSegmentBytes: ").append(toIndentedString(transactionStateLogSegmentBytes)).append("\n");
    sb.append("    replicaFetchResponseMaxBytes: ").append(toIndentedString(replicaFetchResponseMaxBytes)).append("\n");
    sb.append("    logMessageTimestampType: ").append(toIndentedString(logMessageTimestampType)).append("\n");
    sb.append("    connectionsMaxIdleMs: ").append(toIndentedString(connectionsMaxIdleMs)).append("\n");
    sb.append("    logFlushIntervalMs: ").append(toIndentedString(logFlushIntervalMs)).append("\n");
    sb.append("    logPreallocate: ").append(toIndentedString(logPreallocate)).append("\n");
    sb.append("    logSegmentDeleteDelayMs: ").append(toIndentedString(logSegmentDeleteDelayMs)).append("\n");
    sb.append("    messageMaxBytes: ").append(toIndentedString(messageMaxBytes)).append("\n");
    sb.append("    groupInitialRebalanceDelayMs: ").append(toIndentedString(groupInitialRebalanceDelayMs)).append("\n");
    sb.append("    logLocalRetentionBytes: ").append(toIndentedString(logLocalRetentionBytes)).append("\n");
    sb.append("    logRollJitterMs: ").append(toIndentedString(logRollJitterMs)).append("\n");
    sb.append("    transactionRemoveExpiredTransactionCleanupIntervalMs: ").append(toIndentedString(transactionRemoveExpiredTransactionCleanupIntervalMs)).append("\n");
    sb.append("    defaultReplicationFactor: ").append(toIndentedString(defaultReplicationFactor)).append("\n");
    sb.append("    logRollMs: ").append(toIndentedString(logRollMs)).append("\n");
    sb.append("    producerPurgatoryPurgeIntervalRequests: ").append(toIndentedString(producerPurgatoryPurgeIntervalRequests)).append("\n");
    sb.append("    logRetentionBytes: ").append(toIndentedString(logRetentionBytes)).append("\n");
    sb.append("    minInsyncReplicas: ").append(toIndentedString(minInsyncReplicas)).append("\n");
    sb.append("    compressionType: ").append(toIndentedString(compressionType)).append("\n");
    sb.append("    logMessageTimestampDifferenceMaxMs: ").append(toIndentedString(logMessageTimestampDifferenceMaxMs)).append("\n");
    sb.append("    logLocalRetentionMs: ").append(toIndentedString(logLocalRetentionMs)).append("\n");
    sb.append("    logMessageDownconversionEnable: ").append(toIndentedString(logMessageDownconversionEnable)).append("\n");
    sb.append("    saslOauthbearerSubClaimName: ").append(toIndentedString(saslOauthbearerSubClaimName)).append("\n");
    sb.append("    maxIncrementalFetchSessionCacheSlots: ").append(toIndentedString(maxIncrementalFetchSessionCacheSlots)).append("\n");
    sb.append("    logRetentionHours: ").append(toIndentedString(logRetentionHours)).append("\n");
    sb.append("    groupMinSessionTimeoutMs: ").append(toIndentedString(groupMinSessionTimeoutMs)).append("\n");
    sb.append("    socketRequestMaxBytes: ").append(toIndentedString(socketRequestMaxBytes)).append("\n");
    sb.append("    logSegmentBytes: ").append(toIndentedString(logSegmentBytes)).append("\n");
    sb.append("    logCleanupAndCompaction: ").append(toIndentedString(logCleanupAndCompaction)).append("\n");
    sb.append("    offsetsRetentionMinutes: ").append(toIndentedString(offsetsRetentionMinutes)).append("\n");
    sb.append("    logRetentionMs: ").append(toIndentedString(logRetentionMs)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("sasl_oauthbearer_expected_audience");
    openapiFields.add("group_max_session_timeout_ms");
    openapiFields.add("log_flush_interval_messages");
    openapiFields.add("sasl_oauthbearer_jwks_endpoint_url");
    openapiFields.add("max_connections_per_ip");
    openapiFields.add("sasl_oauthbearer_expected_issuer");
    openapiFields.add("log_index_size_max_bytes");
    openapiFields.add("auto_create_topics_enable");
    openapiFields.add("log_index_interval_bytes");
    openapiFields.add("replica_fetch_max_bytes");
    openapiFields.add("num_partitions");
    openapiFields.add("transaction_state_log_segment_bytes");
    openapiFields.add("replica_fetch_response_max_bytes");
    openapiFields.add("log_message_timestamp_type");
    openapiFields.add("connections_max_idle_ms");
    openapiFields.add("log_flush_interval_ms");
    openapiFields.add("log_preallocate");
    openapiFields.add("log_segment_delete_delay_ms");
    openapiFields.add("message_max_bytes");
    openapiFields.add("group_initial_rebalance_delay_ms");
    openapiFields.add("log_local_retention_bytes");
    openapiFields.add("log_roll_jitter_ms");
    openapiFields.add("transaction_remove_expired_transaction_cleanup_interval_ms");
    openapiFields.add("default_replication_factor");
    openapiFields.add("log_roll_ms");
    openapiFields.add("producer_purgatory_purge_interval_requests");
    openapiFields.add("log_retention_bytes");
    openapiFields.add("min_insync_replicas");
    openapiFields.add("compression_type");
    openapiFields.add("log_message_timestamp_difference_max_ms");
    openapiFields.add("log_local_retention_ms");
    openapiFields.add("log_message_downconversion_enable");
    openapiFields.add("sasl_oauthbearer_sub_claim_name");
    openapiFields.add("max_incremental_fetch_session_cache_slots");
    openapiFields.add("log_retention_hours");
    openapiFields.add("group_min_session_timeout_ms");
    openapiFields.add("socket_request_max_bytes");
    openapiFields.add("log_segment_bytes");
    openapiFields.add("log-cleanup-and-compaction");
    openapiFields.add("offsets_retention_minutes");
    openapiFields.add("log_retention_ms");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

 /**
  * Validates the JSON Element and throws an exception if issues found
  *
  * @param jsonElement JSON Element
  * @throws IOException if the JSON Element is invalid with respect to JsonSchemaKafka
  */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!JsonSchemaKafka.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in JsonSchemaKafka is not found in the empty JSON string", JsonSchemaKafka.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!JsonSchemaKafka.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `JsonSchemaKafka` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      if ((jsonObj.get("sasl_oauthbearer_expected_audience") != null && !jsonObj.get("sasl_oauthbearer_expected_audience").isJsonNull()) && !jsonObj.get("sasl_oauthbearer_expected_audience").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `sasl_oauthbearer_expected_audience` to be a primitive type in the JSON string but got `%s`", jsonObj.get("sasl_oauthbearer_expected_audience").toString()));
      }
      if ((jsonObj.get("sasl_oauthbearer_jwks_endpoint_url") != null && !jsonObj.get("sasl_oauthbearer_jwks_endpoint_url").isJsonNull()) && !jsonObj.get("sasl_oauthbearer_jwks_endpoint_url").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `sasl_oauthbearer_jwks_endpoint_url` to be a primitive type in the JSON string but got `%s`", jsonObj.get("sasl_oauthbearer_jwks_endpoint_url").toString()));
      }
      if ((jsonObj.get("sasl_oauthbearer_expected_issuer") != null && !jsonObj.get("sasl_oauthbearer_expected_issuer").isJsonNull()) && !jsonObj.get("sasl_oauthbearer_expected_issuer").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `sasl_oauthbearer_expected_issuer` to be a primitive type in the JSON string but got `%s`", jsonObj.get("sasl_oauthbearer_expected_issuer").toString()));
      }
      if ((jsonObj.get("log_message_timestamp_type") != null && !jsonObj.get("log_message_timestamp_type").isJsonNull()) && !jsonObj.get("log_message_timestamp_type").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `log_message_timestamp_type` to be a primitive type in the JSON string but got `%s`", jsonObj.get("log_message_timestamp_type").toString()));
      }
      // validate the optional field `log_message_timestamp_type`
      if (jsonObj.get("log_message_timestamp_type") != null && !jsonObj.get("log_message_timestamp_type").isJsonNull()) {
        LogMessageTimestampTypeEnum.validateJsonElement(jsonObj.get("log_message_timestamp_type"));
      }
      if ((jsonObj.get("compression_type") != null && !jsonObj.get("compression_type").isJsonNull()) && !jsonObj.get("compression_type").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `compression_type` to be a primitive type in the JSON string but got `%s`", jsonObj.get("compression_type").toString()));
      }
      // validate the optional field `compression_type`
      if (jsonObj.get("compression_type") != null && !jsonObj.get("compression_type").isJsonNull()) {
        CompressionTypeEnum.validateJsonElement(jsonObj.get("compression_type"));
      }
      if ((jsonObj.get("sasl_oauthbearer_sub_claim_name") != null && !jsonObj.get("sasl_oauthbearer_sub_claim_name").isJsonNull()) && !jsonObj.get("sasl_oauthbearer_sub_claim_name").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `sasl_oauthbearer_sub_claim_name` to be a primitive type in the JSON string but got `%s`", jsonObj.get("sasl_oauthbearer_sub_claim_name").toString()));
      }
      // validate the optional field `log-cleanup-and-compaction`
      if (jsonObj.get("log-cleanup-and-compaction") != null && !jsonObj.get("log-cleanup-and-compaction").isJsonNull()) {
        ConfigureLogCleanerForTopicCompaction.validateJsonElement(jsonObj.get("log-cleanup-and-compaction"));
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!JsonSchemaKafka.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'JsonSchemaKafka' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<JsonSchemaKafka> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(JsonSchemaKafka.class));

       return (TypeAdapter<T>) new TypeAdapter<JsonSchemaKafka>() {
           @Override
           public void write(JsonWriter out, JsonSchemaKafka value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public JsonSchemaKafka read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

 /**
  * Create an instance of JsonSchemaKafka given an JSON string
  *
  * @param jsonString JSON string
  * @return An instance of JsonSchemaKafka
  * @throws IOException if the JSON string is invalid with respect to JsonSchemaKafka
  */
  public static JsonSchemaKafka fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, JsonSchemaKafka.class);
  }

 /**
  * Convert an instance of JsonSchemaKafka to an JSON string
  *
  * @return JSON string
  */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}


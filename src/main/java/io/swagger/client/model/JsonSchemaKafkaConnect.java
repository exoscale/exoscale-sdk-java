/*
 * Exoscale Public API
 *  Infrastructure automation API, allowing programmatic access to all Exoscale products and services.  The [OpenAPI Specification](http://spec.openapis.org/oas/v3.0.3.html) source of this documentation can be obtained here:  * [JSON format](https://openapi-v2.exoscale.com/source.json) * [YAML format](https://openapi-v2.exoscale.com/source.yaml)
 *
 * OpenAPI spec version: 2.0.0
 * Contact: api@exoscale.com
 *
 * NOTE: This class is auto generated by the swagger code generator program.
 * https://github.com/swagger-api/swagger-codegen.git
 * Do not edit the class manually.
 */

package io.swagger.client.model;

import java.util.Objects;
import java.util.Arrays;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonValue;
import io.swagger.v3.oas.annotations.media.Schema;
/**
 * JsonSchemaKafkaConnect
 */

@javax.annotation.Generated(value = "io.swagger.codegen.v3.generators.java.JavaClientCodegen", date = "2024-03-08T17:23:40.261901+01:00[Europe/Vienna]")

public class JsonSchemaKafkaConnect {
  @JsonProperty("producer_buffer_memory")
  private Integer producerBufferMemory = null;

  @JsonProperty("consumer_max_poll_interval_ms")
  private Integer consumerMaxPollIntervalMs = null;

  /**
   * Specify the default compression type for producers. This configuration accepts the standard compression codecs (&#x27;gzip&#x27;, &#x27;snappy&#x27;, &#x27;lz4&#x27;, &#x27;zstd&#x27;). It additionally accepts &#x27;none&#x27; which is the default and equivalent to no compression.
   */
  public enum ProducerCompressionTypeEnum {
    GZIP("gzip"),
    SNAPPY("snappy"),
    LZ4("lz4"),
    ZSTD("zstd"),
    NONE("none");

    private String value;

    ProducerCompressionTypeEnum(String value) {
      this.value = value;
    }
    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }
    @JsonCreator
    public static ProducerCompressionTypeEnum fromValue(String input) {
      for (ProducerCompressionTypeEnum b : ProducerCompressionTypeEnum.values()) {
        if (b.value.equals(input)) {
          return b;
        }
      }
      return null;
    }

  }  @JsonProperty("producer_compression_type")
  private ProducerCompressionTypeEnum producerCompressionType = null;

  /**
   * Defines what client configurations can be overridden by the connector. Default is None
   */
  public enum ConnectorClientConfigOverridePolicyEnum {
    NONE("None"),
    ALL("All");

    private String value;

    ConnectorClientConfigOverridePolicyEnum(String value) {
      this.value = value;
    }
    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }
    @JsonCreator
    public static ConnectorClientConfigOverridePolicyEnum fromValue(String input) {
      for (ConnectorClientConfigOverridePolicyEnum b : ConnectorClientConfigOverridePolicyEnum.values()) {
        if (b.value.equals(input)) {
          return b;
        }
      }
      return null;
    }

  }  @JsonProperty("connector_client_config_override_policy")
  private ConnectorClientConfigOverridePolicyEnum connectorClientConfigOverridePolicy = null;

  @JsonProperty("offset_flush_interval_ms")
  private Integer offsetFlushIntervalMs = null;

  @JsonProperty("scheduled_rebalance_max_delay_ms")
  private Integer scheduledRebalanceMaxDelayMs = null;

  @JsonProperty("consumer_fetch_max_bytes")
  private Integer consumerFetchMaxBytes = null;

  @JsonProperty("consumer_max_partition_fetch_bytes")
  private Integer consumerMaxPartitionFetchBytes = null;

  @JsonProperty("offset_flush_timeout_ms")
  private Integer offsetFlushTimeoutMs = null;

  /**
   * What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest
   */
  public enum ConsumerAutoOffsetResetEnum {
    EARLIEST("earliest"),
    LATEST("latest");

    private String value;

    ConsumerAutoOffsetResetEnum(String value) {
      this.value = value;
    }
    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }
    @JsonCreator
    public static ConsumerAutoOffsetResetEnum fromValue(String input) {
      for (ConsumerAutoOffsetResetEnum b : ConsumerAutoOffsetResetEnum.values()) {
        if (b.value.equals(input)) {
          return b;
        }
      }
      return null;
    }

  }  @JsonProperty("consumer_auto_offset_reset")
  private ConsumerAutoOffsetResetEnum consumerAutoOffsetReset = null;

  @JsonProperty("producer_max_request_size")
  private Integer producerMaxRequestSize = null;

  @JsonProperty("producer_batch_size")
  private Integer producerBatchSize = null;

  @JsonProperty("session_timeout_ms")
  private Integer sessionTimeoutMs = null;

  @JsonProperty("producer_linger_ms")
  private Integer producerLingerMs = null;

  /**
   * Transaction read isolation level. read_uncommitted is the default, but read_committed can be used if consume-exactly-once behavior is desired.
   */
  public enum ConsumerIsolationLevelEnum {
    UNCOMMITTED("read_uncommitted"),
    COMMITTED("read_committed");

    private String value;

    ConsumerIsolationLevelEnum(String value) {
      this.value = value;
    }
    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }
    @JsonCreator
    public static ConsumerIsolationLevelEnum fromValue(String input) {
      for (ConsumerIsolationLevelEnum b : ConsumerIsolationLevelEnum.values()) {
        if (b.value.equals(input)) {
          return b;
        }
      }
      return null;
    }

  }  @JsonProperty("consumer_isolation_level")
  private ConsumerIsolationLevelEnum consumerIsolationLevel = null;

  @JsonProperty("consumer_max_poll_records")
  private Integer consumerMaxPollRecords = null;

  public JsonSchemaKafkaConnect producerBufferMemory(Integer producerBufferMemory) {
    this.producerBufferMemory = producerBufferMemory;
    return this;
  }

   /**
   * The total bytes of memory the producer can use to buffer records waiting to be sent to the broker (defaults to 33554432).
   * minimum: 5242880
   * maximum: 134217728
   * @return producerBufferMemory
  **/
  @Schema(example = "8388608", description = "The total bytes of memory the producer can use to buffer records waiting to be sent to the broker (defaults to 33554432).")
  public Integer getProducerBufferMemory() {
    return producerBufferMemory;
  }

  public void setProducerBufferMemory(Integer producerBufferMemory) {
    this.producerBufferMemory = producerBufferMemory;
  }

  public JsonSchemaKafkaConnect consumerMaxPollIntervalMs(Integer consumerMaxPollIntervalMs) {
    this.consumerMaxPollIntervalMs = consumerMaxPollIntervalMs;
    return this;
  }

   /**
   * The maximum delay in milliseconds between invocations of poll() when using consumer group management (defaults to 300000).
   * minimum: 1
   * maximum: 2147483647
   * @return consumerMaxPollIntervalMs
  **/
  @Schema(example = "300000", description = "The maximum delay in milliseconds between invocations of poll() when using consumer group management (defaults to 300000).")
  public Integer getConsumerMaxPollIntervalMs() {
    return consumerMaxPollIntervalMs;
  }

  public void setConsumerMaxPollIntervalMs(Integer consumerMaxPollIntervalMs) {
    this.consumerMaxPollIntervalMs = consumerMaxPollIntervalMs;
  }

  public JsonSchemaKafkaConnect producerCompressionType(ProducerCompressionTypeEnum producerCompressionType) {
    this.producerCompressionType = producerCompressionType;
    return this;
  }

   /**
   * Specify the default compression type for producers. This configuration accepts the standard compression codecs (&#x27;gzip&#x27;, &#x27;snappy&#x27;, &#x27;lz4&#x27;, &#x27;zstd&#x27;). It additionally accepts &#x27;none&#x27; which is the default and equivalent to no compression.
   * @return producerCompressionType
  **/
  @Schema(description = "Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.")
  public ProducerCompressionTypeEnum getProducerCompressionType() {
    return producerCompressionType;
  }

  public void setProducerCompressionType(ProducerCompressionTypeEnum producerCompressionType) {
    this.producerCompressionType = producerCompressionType;
  }

  public JsonSchemaKafkaConnect connectorClientConfigOverridePolicy(ConnectorClientConfigOverridePolicyEnum connectorClientConfigOverridePolicy) {
    this.connectorClientConfigOverridePolicy = connectorClientConfigOverridePolicy;
    return this;
  }

   /**
   * Defines what client configurations can be overridden by the connector. Default is None
   * @return connectorClientConfigOverridePolicy
  **/
  @Schema(description = "Defines what client configurations can be overridden by the connector. Default is None")
  public ConnectorClientConfigOverridePolicyEnum getConnectorClientConfigOverridePolicy() {
    return connectorClientConfigOverridePolicy;
  }

  public void setConnectorClientConfigOverridePolicy(ConnectorClientConfigOverridePolicyEnum connectorClientConfigOverridePolicy) {
    this.connectorClientConfigOverridePolicy = connectorClientConfigOverridePolicy;
  }

  public JsonSchemaKafkaConnect offsetFlushIntervalMs(Integer offsetFlushIntervalMs) {
    this.offsetFlushIntervalMs = offsetFlushIntervalMs;
    return this;
  }

   /**
   * The interval at which to try committing offsets for tasks (defaults to 60000).
   * minimum: 1
   * maximum: 100000000
   * @return offsetFlushIntervalMs
  **/
  @Schema(example = "60000", description = "The interval at which to try committing offsets for tasks (defaults to 60000).")
  public Integer getOffsetFlushIntervalMs() {
    return offsetFlushIntervalMs;
  }

  public void setOffsetFlushIntervalMs(Integer offsetFlushIntervalMs) {
    this.offsetFlushIntervalMs = offsetFlushIntervalMs;
  }

  public JsonSchemaKafkaConnect scheduledRebalanceMaxDelayMs(Integer scheduledRebalanceMaxDelayMs) {
    this.scheduledRebalanceMaxDelayMs = scheduledRebalanceMaxDelayMs;
    return this;
  }

   /**
   * The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned.  Defaults to 5 minutes.
   * minimum: 0
   * maximum: 600000
   * @return scheduledRebalanceMaxDelayMs
  **/
  @Schema(example = "300000", description = "The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned.  Defaults to 5 minutes.")
  public Integer getScheduledRebalanceMaxDelayMs() {
    return scheduledRebalanceMaxDelayMs;
  }

  public void setScheduledRebalanceMaxDelayMs(Integer scheduledRebalanceMaxDelayMs) {
    this.scheduledRebalanceMaxDelayMs = scheduledRebalanceMaxDelayMs;
  }

  public JsonSchemaKafkaConnect consumerFetchMaxBytes(Integer consumerFetchMaxBytes) {
    this.consumerFetchMaxBytes = consumerFetchMaxBytes;
    return this;
  }

   /**
   * Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum.
   * minimum: 1048576
   * maximum: 104857600
   * @return consumerFetchMaxBytes
  **/
  @Schema(example = "52428800", description = "Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum.")
  public Integer getConsumerFetchMaxBytes() {
    return consumerFetchMaxBytes;
  }

  public void setConsumerFetchMaxBytes(Integer consumerFetchMaxBytes) {
    this.consumerFetchMaxBytes = consumerFetchMaxBytes;
  }

  public JsonSchemaKafkaConnect consumerMaxPartitionFetchBytes(Integer consumerMaxPartitionFetchBytes) {
    this.consumerMaxPartitionFetchBytes = consumerMaxPartitionFetchBytes;
    return this;
  }

   /**
   * Records are fetched in batches by the consumer.If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. 
   * minimum: 1048576
   * maximum: 104857600
   * @return consumerMaxPartitionFetchBytes
  **/
  @Schema(example = "1048576", description = "Records are fetched in batches by the consumer.If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. ")
  public Integer getConsumerMaxPartitionFetchBytes() {
    return consumerMaxPartitionFetchBytes;
  }

  public void setConsumerMaxPartitionFetchBytes(Integer consumerMaxPartitionFetchBytes) {
    this.consumerMaxPartitionFetchBytes = consumerMaxPartitionFetchBytes;
  }

  public JsonSchemaKafkaConnect offsetFlushTimeoutMs(Integer offsetFlushTimeoutMs) {
    this.offsetFlushTimeoutMs = offsetFlushTimeoutMs;
    return this;
  }

   /**
   * Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt (defaults to 5000).
   * minimum: 1
   * maximum: 2147483647
   * @return offsetFlushTimeoutMs
  **/
  @Schema(example = "5000", description = "Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt (defaults to 5000).")
  public Integer getOffsetFlushTimeoutMs() {
    return offsetFlushTimeoutMs;
  }

  public void setOffsetFlushTimeoutMs(Integer offsetFlushTimeoutMs) {
    this.offsetFlushTimeoutMs = offsetFlushTimeoutMs;
  }

  public JsonSchemaKafkaConnect consumerAutoOffsetReset(ConsumerAutoOffsetResetEnum consumerAutoOffsetReset) {
    this.consumerAutoOffsetReset = consumerAutoOffsetReset;
    return this;
  }

   /**
   * What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest
   * @return consumerAutoOffsetReset
  **/
  @Schema(description = "What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest")
  public ConsumerAutoOffsetResetEnum getConsumerAutoOffsetReset() {
    return consumerAutoOffsetReset;
  }

  public void setConsumerAutoOffsetReset(ConsumerAutoOffsetResetEnum consumerAutoOffsetReset) {
    this.consumerAutoOffsetReset = consumerAutoOffsetReset;
  }

  public JsonSchemaKafkaConnect producerMaxRequestSize(Integer producerMaxRequestSize) {
    this.producerMaxRequestSize = producerMaxRequestSize;
    return this;
  }

   /**
   * This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.
   * minimum: 131072
   * maximum: 67108864
   * @return producerMaxRequestSize
  **/
  @Schema(example = "1048576", description = "This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.")
  public Integer getProducerMaxRequestSize() {
    return producerMaxRequestSize;
  }

  public void setProducerMaxRequestSize(Integer producerMaxRequestSize) {
    this.producerMaxRequestSize = producerMaxRequestSize;
  }

  public JsonSchemaKafkaConnect producerBatchSize(Integer producerBatchSize) {
    this.producerBatchSize = producerBatchSize;
    return this;
  }

   /**
   * This setting gives the upper bound of the batch size to be sent. If there are fewer than this many bytes accumulated for this partition, the producer will &#x27;linger&#x27; for the linger.ms time waiting for more records to show up. A batch size of zero will disable batching entirely (defaults to 16384).
   * minimum: 0
   * maximum: 5242880
   * @return producerBatchSize
  **/
  @Schema(example = "1024", description = "This setting gives the upper bound of the batch size to be sent. If there are fewer than this many bytes accumulated for this partition, the producer will 'linger' for the linger.ms time waiting for more records to show up. A batch size of zero will disable batching entirely (defaults to 16384).")
  public Integer getProducerBatchSize() {
    return producerBatchSize;
  }

  public void setProducerBatchSize(Integer producerBatchSize) {
    this.producerBatchSize = producerBatchSize;
  }

  public JsonSchemaKafkaConnect sessionTimeoutMs(Integer sessionTimeoutMs) {
    this.sessionTimeoutMs = sessionTimeoutMs;
    return this;
  }

   /**
   * The timeout in milliseconds used to detect failures when using Kafka’s group management facilities (defaults to 10000).
   * minimum: 1
   * maximum: 2147483647
   * @return sessionTimeoutMs
  **/
  @Schema(example = "10000", description = "The timeout in milliseconds used to detect failures when using Kafka’s group management facilities (defaults to 10000).")
  public Integer getSessionTimeoutMs() {
    return sessionTimeoutMs;
  }

  public void setSessionTimeoutMs(Integer sessionTimeoutMs) {
    this.sessionTimeoutMs = sessionTimeoutMs;
  }

  public JsonSchemaKafkaConnect producerLingerMs(Integer producerLingerMs) {
    this.producerLingerMs = producerLingerMs;
    return this;
  }

   /**
   * This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will &#x27;linger&#x27; for the specified time waiting for more records to show up. Defaults to 0.
   * minimum: 0
   * maximum: 5000
   * @return producerLingerMs
  **/
  @Schema(example = "100", description = "This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will 'linger' for the specified time waiting for more records to show up. Defaults to 0.")
  public Integer getProducerLingerMs() {
    return producerLingerMs;
  }

  public void setProducerLingerMs(Integer producerLingerMs) {
    this.producerLingerMs = producerLingerMs;
  }

  public JsonSchemaKafkaConnect consumerIsolationLevel(ConsumerIsolationLevelEnum consumerIsolationLevel) {
    this.consumerIsolationLevel = consumerIsolationLevel;
    return this;
  }

   /**
   * Transaction read isolation level. read_uncommitted is the default, but read_committed can be used if consume-exactly-once behavior is desired.
   * @return consumerIsolationLevel
  **/
  @Schema(description = "Transaction read isolation level. read_uncommitted is the default, but read_committed can be used if consume-exactly-once behavior is desired.")
  public ConsumerIsolationLevelEnum getConsumerIsolationLevel() {
    return consumerIsolationLevel;
  }

  public void setConsumerIsolationLevel(ConsumerIsolationLevelEnum consumerIsolationLevel) {
    this.consumerIsolationLevel = consumerIsolationLevel;
  }

  public JsonSchemaKafkaConnect consumerMaxPollRecords(Integer consumerMaxPollRecords) {
    this.consumerMaxPollRecords = consumerMaxPollRecords;
    return this;
  }

   /**
   * The maximum number of records returned in a single call to poll() (defaults to 500).
   * minimum: 1
   * maximum: 10000
   * @return consumerMaxPollRecords
  **/
  @Schema(example = "500", description = "The maximum number of records returned in a single call to poll() (defaults to 500).")
  public Integer getConsumerMaxPollRecords() {
    return consumerMaxPollRecords;
  }

  public void setConsumerMaxPollRecords(Integer consumerMaxPollRecords) {
    this.consumerMaxPollRecords = consumerMaxPollRecords;
  }


  @Override
  public boolean equals(java.lang.Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    JsonSchemaKafkaConnect jsonSchemaKafkaConnect = (JsonSchemaKafkaConnect) o;
    return Objects.equals(this.producerBufferMemory, jsonSchemaKafkaConnect.producerBufferMemory) &&
        Objects.equals(this.consumerMaxPollIntervalMs, jsonSchemaKafkaConnect.consumerMaxPollIntervalMs) &&
        Objects.equals(this.producerCompressionType, jsonSchemaKafkaConnect.producerCompressionType) &&
        Objects.equals(this.connectorClientConfigOverridePolicy, jsonSchemaKafkaConnect.connectorClientConfigOverridePolicy) &&
        Objects.equals(this.offsetFlushIntervalMs, jsonSchemaKafkaConnect.offsetFlushIntervalMs) &&
        Objects.equals(this.scheduledRebalanceMaxDelayMs, jsonSchemaKafkaConnect.scheduledRebalanceMaxDelayMs) &&
        Objects.equals(this.consumerFetchMaxBytes, jsonSchemaKafkaConnect.consumerFetchMaxBytes) &&
        Objects.equals(this.consumerMaxPartitionFetchBytes, jsonSchemaKafkaConnect.consumerMaxPartitionFetchBytes) &&
        Objects.equals(this.offsetFlushTimeoutMs, jsonSchemaKafkaConnect.offsetFlushTimeoutMs) &&
        Objects.equals(this.consumerAutoOffsetReset, jsonSchemaKafkaConnect.consumerAutoOffsetReset) &&
        Objects.equals(this.producerMaxRequestSize, jsonSchemaKafkaConnect.producerMaxRequestSize) &&
        Objects.equals(this.producerBatchSize, jsonSchemaKafkaConnect.producerBatchSize) &&
        Objects.equals(this.sessionTimeoutMs, jsonSchemaKafkaConnect.sessionTimeoutMs) &&
        Objects.equals(this.producerLingerMs, jsonSchemaKafkaConnect.producerLingerMs) &&
        Objects.equals(this.consumerIsolationLevel, jsonSchemaKafkaConnect.consumerIsolationLevel) &&
        Objects.equals(this.consumerMaxPollRecords, jsonSchemaKafkaConnect.consumerMaxPollRecords);
  }

  @Override
  public int hashCode() {
    return Objects.hash(producerBufferMemory, consumerMaxPollIntervalMs, producerCompressionType, connectorClientConfigOverridePolicy, offsetFlushIntervalMs, scheduledRebalanceMaxDelayMs, consumerFetchMaxBytes, consumerMaxPartitionFetchBytes, offsetFlushTimeoutMs, consumerAutoOffsetReset, producerMaxRequestSize, producerBatchSize, sessionTimeoutMs, producerLingerMs, consumerIsolationLevel, consumerMaxPollRecords);
  }


  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class JsonSchemaKafkaConnect {\n");
    
    sb.append("    producerBufferMemory: ").append(toIndentedString(producerBufferMemory)).append("\n");
    sb.append("    consumerMaxPollIntervalMs: ").append(toIndentedString(consumerMaxPollIntervalMs)).append("\n");
    sb.append("    producerCompressionType: ").append(toIndentedString(producerCompressionType)).append("\n");
    sb.append("    connectorClientConfigOverridePolicy: ").append(toIndentedString(connectorClientConfigOverridePolicy)).append("\n");
    sb.append("    offsetFlushIntervalMs: ").append(toIndentedString(offsetFlushIntervalMs)).append("\n");
    sb.append("    scheduledRebalanceMaxDelayMs: ").append(toIndentedString(scheduledRebalanceMaxDelayMs)).append("\n");
    sb.append("    consumerFetchMaxBytes: ").append(toIndentedString(consumerFetchMaxBytes)).append("\n");
    sb.append("    consumerMaxPartitionFetchBytes: ").append(toIndentedString(consumerMaxPartitionFetchBytes)).append("\n");
    sb.append("    offsetFlushTimeoutMs: ").append(toIndentedString(offsetFlushTimeoutMs)).append("\n");
    sb.append("    consumerAutoOffsetReset: ").append(toIndentedString(consumerAutoOffsetReset)).append("\n");
    sb.append("    producerMaxRequestSize: ").append(toIndentedString(producerMaxRequestSize)).append("\n");
    sb.append("    producerBatchSize: ").append(toIndentedString(producerBatchSize)).append("\n");
    sb.append("    sessionTimeoutMs: ").append(toIndentedString(sessionTimeoutMs)).append("\n");
    sb.append("    producerLingerMs: ").append(toIndentedString(producerLingerMs)).append("\n");
    sb.append("    consumerIsolationLevel: ").append(toIndentedString(consumerIsolationLevel)).append("\n");
    sb.append("    consumerMaxPollRecords: ").append(toIndentedString(consumerMaxPollRecords)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(java.lang.Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }

}

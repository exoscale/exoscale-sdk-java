/*
 * Exoscale Public API
 *  Infrastructure automation API, allowing programmatic access to all Exoscale products and services.  The [OpenAPI Specification](http://spec.openapis.org/oas/v3.0.3.html) source of this documentation can be obtained here:  * [JSON format](https://openapi-v2.exoscale.com/source.json) * [YAML format](https://openapi-v2.exoscale.com/source.yaml)
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: api@exoscale.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.Arrays;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.lang.reflect.Type;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * JsonSchemaKafkaConnect
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-03-27T17:14:44.205710495Z[Etc/UTC]", comments = "Generator version: 7.5.0-SNAPSHOT")
public class JsonSchemaKafkaConnect {
  public static final String SERIALIZED_NAME_PRODUCER_BUFFER_MEMORY = "producer_buffer_memory";
  @SerializedName(SERIALIZED_NAME_PRODUCER_BUFFER_MEMORY)
  private Integer producerBufferMemory;

  public static final String SERIALIZED_NAME_CONSUMER_MAX_POLL_INTERVAL_MS = "consumer_max_poll_interval_ms";
  @SerializedName(SERIALIZED_NAME_CONSUMER_MAX_POLL_INTERVAL_MS)
  private Integer consumerMaxPollIntervalMs;

  /**
   * Specify the default compression type for producers. This configuration accepts the standard compression codecs (&#39;gzip&#39;, &#39;snappy&#39;, &#39;lz4&#39;, &#39;zstd&#39;). It additionally accepts &#39;none&#39; which is the default and equivalent to no compression.
   */
  @JsonAdapter(ProducerCompressionTypeEnum.Adapter.class)
  public enum ProducerCompressionTypeEnum {
    GZIP("gzip"),
    
    SNAPPY("snappy"),
    
    LZ4("lz4"),
    
    ZSTD("zstd"),
    
    NONE("none");

    private String value;

    ProducerCompressionTypeEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static ProducerCompressionTypeEnum fromValue(String value) {
      for (ProducerCompressionTypeEnum b : ProducerCompressionTypeEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<ProducerCompressionTypeEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final ProducerCompressionTypeEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public ProducerCompressionTypeEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return ProducerCompressionTypeEnum.fromValue(value);
      }
    }

    public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      String value = jsonElement.getAsString();
      ProducerCompressionTypeEnum.fromValue(value);
    }
  }

  public static final String SERIALIZED_NAME_PRODUCER_COMPRESSION_TYPE = "producer_compression_type";
  @SerializedName(SERIALIZED_NAME_PRODUCER_COMPRESSION_TYPE)
  private ProducerCompressionTypeEnum producerCompressionType;

  /**
   * Defines what client configurations can be overridden by the connector. Default is None
   */
  @JsonAdapter(ConnectorClientConfigOverridePolicyEnum.Adapter.class)
  public enum ConnectorClientConfigOverridePolicyEnum {
    NONE("None"),
    
    ALL("All");

    private String value;

    ConnectorClientConfigOverridePolicyEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static ConnectorClientConfigOverridePolicyEnum fromValue(String value) {
      for (ConnectorClientConfigOverridePolicyEnum b : ConnectorClientConfigOverridePolicyEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<ConnectorClientConfigOverridePolicyEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final ConnectorClientConfigOverridePolicyEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public ConnectorClientConfigOverridePolicyEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return ConnectorClientConfigOverridePolicyEnum.fromValue(value);
      }
    }

    public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      String value = jsonElement.getAsString();
      ConnectorClientConfigOverridePolicyEnum.fromValue(value);
    }
  }

  public static final String SERIALIZED_NAME_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY = "connector_client_config_override_policy";
  @SerializedName(SERIALIZED_NAME_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY)
  private ConnectorClientConfigOverridePolicyEnum connectorClientConfigOverridePolicy;

  public static final String SERIALIZED_NAME_OFFSET_FLUSH_INTERVAL_MS = "offset_flush_interval_ms";
  @SerializedName(SERIALIZED_NAME_OFFSET_FLUSH_INTERVAL_MS)
  private Integer offsetFlushIntervalMs;

  public static final String SERIALIZED_NAME_SCHEDULED_REBALANCE_MAX_DELAY_MS = "scheduled_rebalance_max_delay_ms";
  @SerializedName(SERIALIZED_NAME_SCHEDULED_REBALANCE_MAX_DELAY_MS)
  private Integer scheduledRebalanceMaxDelayMs;

  public static final String SERIALIZED_NAME_CONSUMER_FETCH_MAX_BYTES = "consumer_fetch_max_bytes";
  @SerializedName(SERIALIZED_NAME_CONSUMER_FETCH_MAX_BYTES)
  private Integer consumerFetchMaxBytes;

  public static final String SERIALIZED_NAME_CONSUMER_MAX_PARTITION_FETCH_BYTES = "consumer_max_partition_fetch_bytes";
  @SerializedName(SERIALIZED_NAME_CONSUMER_MAX_PARTITION_FETCH_BYTES)
  private Integer consumerMaxPartitionFetchBytes;

  public static final String SERIALIZED_NAME_OFFSET_FLUSH_TIMEOUT_MS = "offset_flush_timeout_ms";
  @SerializedName(SERIALIZED_NAME_OFFSET_FLUSH_TIMEOUT_MS)
  private Integer offsetFlushTimeoutMs;

  /**
   * What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest
   */
  @JsonAdapter(ConsumerAutoOffsetResetEnum.Adapter.class)
  public enum ConsumerAutoOffsetResetEnum {
    EARLIEST("earliest"),
    
    LATEST("latest");

    private String value;

    ConsumerAutoOffsetResetEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static ConsumerAutoOffsetResetEnum fromValue(String value) {
      for (ConsumerAutoOffsetResetEnum b : ConsumerAutoOffsetResetEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<ConsumerAutoOffsetResetEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final ConsumerAutoOffsetResetEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public ConsumerAutoOffsetResetEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return ConsumerAutoOffsetResetEnum.fromValue(value);
      }
    }

    public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      String value = jsonElement.getAsString();
      ConsumerAutoOffsetResetEnum.fromValue(value);
    }
  }

  public static final String SERIALIZED_NAME_CONSUMER_AUTO_OFFSET_RESET = "consumer_auto_offset_reset";
  @SerializedName(SERIALIZED_NAME_CONSUMER_AUTO_OFFSET_RESET)
  private ConsumerAutoOffsetResetEnum consumerAutoOffsetReset;

  public static final String SERIALIZED_NAME_PRODUCER_MAX_REQUEST_SIZE = "producer_max_request_size";
  @SerializedName(SERIALIZED_NAME_PRODUCER_MAX_REQUEST_SIZE)
  private Integer producerMaxRequestSize;

  public static final String SERIALIZED_NAME_PRODUCER_BATCH_SIZE = "producer_batch_size";
  @SerializedName(SERIALIZED_NAME_PRODUCER_BATCH_SIZE)
  private Integer producerBatchSize;

  public static final String SERIALIZED_NAME_SESSION_TIMEOUT_MS = "session_timeout_ms";
  @SerializedName(SERIALIZED_NAME_SESSION_TIMEOUT_MS)
  private Integer sessionTimeoutMs;

  public static final String SERIALIZED_NAME_PRODUCER_LINGER_MS = "producer_linger_ms";
  @SerializedName(SERIALIZED_NAME_PRODUCER_LINGER_MS)
  private Integer producerLingerMs;

  /**
   * Transaction read isolation level. read_uncommitted is the default, but read_committed can be used if consume-exactly-once behavior is desired.
   */
  @JsonAdapter(ConsumerIsolationLevelEnum.Adapter.class)
  public enum ConsumerIsolationLevelEnum {
    UNCOMMITTED("read_uncommitted"),
    
    COMMITTED("read_committed");

    private String value;

    ConsumerIsolationLevelEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static ConsumerIsolationLevelEnum fromValue(String value) {
      for (ConsumerIsolationLevelEnum b : ConsumerIsolationLevelEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<ConsumerIsolationLevelEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final ConsumerIsolationLevelEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public ConsumerIsolationLevelEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return ConsumerIsolationLevelEnum.fromValue(value);
      }
    }

    public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      String value = jsonElement.getAsString();
      ConsumerIsolationLevelEnum.fromValue(value);
    }
  }

  public static final String SERIALIZED_NAME_CONSUMER_ISOLATION_LEVEL = "consumer_isolation_level";
  @SerializedName(SERIALIZED_NAME_CONSUMER_ISOLATION_LEVEL)
  private ConsumerIsolationLevelEnum consumerIsolationLevel;

  public static final String SERIALIZED_NAME_CONSUMER_MAX_POLL_RECORDS = "consumer_max_poll_records";
  @SerializedName(SERIALIZED_NAME_CONSUMER_MAX_POLL_RECORDS)
  private Integer consumerMaxPollRecords;

  public JsonSchemaKafkaConnect() {
  }

  public JsonSchemaKafkaConnect producerBufferMemory(Integer producerBufferMemory) {
    this.producerBufferMemory = producerBufferMemory;
    return this;
  }

   /**
   * The total bytes of memory the producer can use to buffer records waiting to be sent to the broker (defaults to 33554432).
   * minimum: 5242880
   * maximum: 134217728
   * @return producerBufferMemory
  **/
  @javax.annotation.Nullable
  public Integer getProducerBufferMemory() {
    return producerBufferMemory;
  }

  public void setProducerBufferMemory(Integer producerBufferMemory) {
    this.producerBufferMemory = producerBufferMemory;
  }


  public JsonSchemaKafkaConnect consumerMaxPollIntervalMs(Integer consumerMaxPollIntervalMs) {
    this.consumerMaxPollIntervalMs = consumerMaxPollIntervalMs;
    return this;
  }

   /**
   * The maximum delay in milliseconds between invocations of poll() when using consumer group management (defaults to 300000).
   * minimum: 1
   * maximum: 2147483647
   * @return consumerMaxPollIntervalMs
  **/
  @javax.annotation.Nullable
  public Integer getConsumerMaxPollIntervalMs() {
    return consumerMaxPollIntervalMs;
  }

  public void setConsumerMaxPollIntervalMs(Integer consumerMaxPollIntervalMs) {
    this.consumerMaxPollIntervalMs = consumerMaxPollIntervalMs;
  }


  public JsonSchemaKafkaConnect producerCompressionType(ProducerCompressionTypeEnum producerCompressionType) {
    this.producerCompressionType = producerCompressionType;
    return this;
  }

   /**
   * Specify the default compression type for producers. This configuration accepts the standard compression codecs (&#39;gzip&#39;, &#39;snappy&#39;, &#39;lz4&#39;, &#39;zstd&#39;). It additionally accepts &#39;none&#39; which is the default and equivalent to no compression.
   * @return producerCompressionType
  **/
  @javax.annotation.Nullable
  public ProducerCompressionTypeEnum getProducerCompressionType() {
    return producerCompressionType;
  }

  public void setProducerCompressionType(ProducerCompressionTypeEnum producerCompressionType) {
    this.producerCompressionType = producerCompressionType;
  }


  public JsonSchemaKafkaConnect connectorClientConfigOverridePolicy(ConnectorClientConfigOverridePolicyEnum connectorClientConfigOverridePolicy) {
    this.connectorClientConfigOverridePolicy = connectorClientConfigOverridePolicy;
    return this;
  }

   /**
   * Defines what client configurations can be overridden by the connector. Default is None
   * @return connectorClientConfigOverridePolicy
  **/
  @javax.annotation.Nullable
  public ConnectorClientConfigOverridePolicyEnum getConnectorClientConfigOverridePolicy() {
    return connectorClientConfigOverridePolicy;
  }

  public void setConnectorClientConfigOverridePolicy(ConnectorClientConfigOverridePolicyEnum connectorClientConfigOverridePolicy) {
    this.connectorClientConfigOverridePolicy = connectorClientConfigOverridePolicy;
  }


  public JsonSchemaKafkaConnect offsetFlushIntervalMs(Integer offsetFlushIntervalMs) {
    this.offsetFlushIntervalMs = offsetFlushIntervalMs;
    return this;
  }

   /**
   * The interval at which to try committing offsets for tasks (defaults to 60000).
   * minimum: 1
   * maximum: 100000000
   * @return offsetFlushIntervalMs
  **/
  @javax.annotation.Nullable
  public Integer getOffsetFlushIntervalMs() {
    return offsetFlushIntervalMs;
  }

  public void setOffsetFlushIntervalMs(Integer offsetFlushIntervalMs) {
    this.offsetFlushIntervalMs = offsetFlushIntervalMs;
  }


  public JsonSchemaKafkaConnect scheduledRebalanceMaxDelayMs(Integer scheduledRebalanceMaxDelayMs) {
    this.scheduledRebalanceMaxDelayMs = scheduledRebalanceMaxDelayMs;
    return this;
  }

   /**
   * The maximum delay that is scheduled in order to wait for the return of one or more departed workers before rebalancing and reassigning their connectors and tasks to the group. During this period the connectors and tasks of the departed workers remain unassigned.  Defaults to 5 minutes.
   * minimum: 0
   * maximum: 600000
   * @return scheduledRebalanceMaxDelayMs
  **/
  @javax.annotation.Nullable
  public Integer getScheduledRebalanceMaxDelayMs() {
    return scheduledRebalanceMaxDelayMs;
  }

  public void setScheduledRebalanceMaxDelayMs(Integer scheduledRebalanceMaxDelayMs) {
    this.scheduledRebalanceMaxDelayMs = scheduledRebalanceMaxDelayMs;
  }


  public JsonSchemaKafkaConnect consumerFetchMaxBytes(Integer consumerFetchMaxBytes) {
    this.consumerFetchMaxBytes = consumerFetchMaxBytes;
    return this;
  }

   /**
   * Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum.
   * minimum: 1048576
   * maximum: 104857600
   * @return consumerFetchMaxBytes
  **/
  @javax.annotation.Nullable
  public Integer getConsumerFetchMaxBytes() {
    return consumerFetchMaxBytes;
  }

  public void setConsumerFetchMaxBytes(Integer consumerFetchMaxBytes) {
    this.consumerFetchMaxBytes = consumerFetchMaxBytes;
  }


  public JsonSchemaKafkaConnect consumerMaxPartitionFetchBytes(Integer consumerMaxPartitionFetchBytes) {
    this.consumerMaxPartitionFetchBytes = consumerMaxPartitionFetchBytes;
    return this;
  }

   /**
   * Records are fetched in batches by the consumer.If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. 
   * minimum: 1048576
   * maximum: 104857600
   * @return consumerMaxPartitionFetchBytes
  **/
  @javax.annotation.Nullable
  public Integer getConsumerMaxPartitionFetchBytes() {
    return consumerMaxPartitionFetchBytes;
  }

  public void setConsumerMaxPartitionFetchBytes(Integer consumerMaxPartitionFetchBytes) {
    this.consumerMaxPartitionFetchBytes = consumerMaxPartitionFetchBytes;
  }


  public JsonSchemaKafkaConnect offsetFlushTimeoutMs(Integer offsetFlushTimeoutMs) {
    this.offsetFlushTimeoutMs = offsetFlushTimeoutMs;
    return this;
  }

   /**
   * Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt (defaults to 5000).
   * minimum: 1
   * maximum: 2147483647
   * @return offsetFlushTimeoutMs
  **/
  @javax.annotation.Nullable
  public Integer getOffsetFlushTimeoutMs() {
    return offsetFlushTimeoutMs;
  }

  public void setOffsetFlushTimeoutMs(Integer offsetFlushTimeoutMs) {
    this.offsetFlushTimeoutMs = offsetFlushTimeoutMs;
  }


  public JsonSchemaKafkaConnect consumerAutoOffsetReset(ConsumerAutoOffsetResetEnum consumerAutoOffsetReset) {
    this.consumerAutoOffsetReset = consumerAutoOffsetReset;
    return this;
  }

   /**
   * What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. Default is earliest
   * @return consumerAutoOffsetReset
  **/
  @javax.annotation.Nullable
  public ConsumerAutoOffsetResetEnum getConsumerAutoOffsetReset() {
    return consumerAutoOffsetReset;
  }

  public void setConsumerAutoOffsetReset(ConsumerAutoOffsetResetEnum consumerAutoOffsetReset) {
    this.consumerAutoOffsetReset = consumerAutoOffsetReset;
  }


  public JsonSchemaKafkaConnect producerMaxRequestSize(Integer producerMaxRequestSize) {
    this.producerMaxRequestSize = producerMaxRequestSize;
    return this;
  }

   /**
   * This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.
   * minimum: 131072
   * maximum: 67108864
   * @return producerMaxRequestSize
  **/
  @javax.annotation.Nullable
  public Integer getProducerMaxRequestSize() {
    return producerMaxRequestSize;
  }

  public void setProducerMaxRequestSize(Integer producerMaxRequestSize) {
    this.producerMaxRequestSize = producerMaxRequestSize;
  }


  public JsonSchemaKafkaConnect producerBatchSize(Integer producerBatchSize) {
    this.producerBatchSize = producerBatchSize;
    return this;
  }

   /**
   * This setting gives the upper bound of the batch size to be sent. If there are fewer than this many bytes accumulated for this partition, the producer will &#39;linger&#39; for the linger.ms time waiting for more records to show up. A batch size of zero will disable batching entirely (defaults to 16384).
   * minimum: 0
   * maximum: 5242880
   * @return producerBatchSize
  **/
  @javax.annotation.Nullable
  public Integer getProducerBatchSize() {
    return producerBatchSize;
  }

  public void setProducerBatchSize(Integer producerBatchSize) {
    this.producerBatchSize = producerBatchSize;
  }


  public JsonSchemaKafkaConnect sessionTimeoutMs(Integer sessionTimeoutMs) {
    this.sessionTimeoutMs = sessionTimeoutMs;
    return this;
  }

   /**
   * The timeout in milliseconds used to detect failures when using Kafkaâ€™s group management facilities (defaults to 10000).
   * minimum: 1
   * maximum: 2147483647
   * @return sessionTimeoutMs
  **/
  @javax.annotation.Nullable
  public Integer getSessionTimeoutMs() {
    return sessionTimeoutMs;
  }

  public void setSessionTimeoutMs(Integer sessionTimeoutMs) {
    this.sessionTimeoutMs = sessionTimeoutMs;
  }


  public JsonSchemaKafkaConnect producerLingerMs(Integer producerLingerMs) {
    this.producerLingerMs = producerLingerMs;
    return this;
  }

   /**
   * This setting gives the upper bound on the delay for batching: once there is batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if there are fewer than this many bytes accumulated for this partition the producer will &#39;linger&#39; for the specified time waiting for more records to show up. Defaults to 0.
   * minimum: 0
   * maximum: 5000
   * @return producerLingerMs
  **/
  @javax.annotation.Nullable
  public Integer getProducerLingerMs() {
    return producerLingerMs;
  }

  public void setProducerLingerMs(Integer producerLingerMs) {
    this.producerLingerMs = producerLingerMs;
  }


  public JsonSchemaKafkaConnect consumerIsolationLevel(ConsumerIsolationLevelEnum consumerIsolationLevel) {
    this.consumerIsolationLevel = consumerIsolationLevel;
    return this;
  }

   /**
   * Transaction read isolation level. read_uncommitted is the default, but read_committed can be used if consume-exactly-once behavior is desired.
   * @return consumerIsolationLevel
  **/
  @javax.annotation.Nullable
  public ConsumerIsolationLevelEnum getConsumerIsolationLevel() {
    return consumerIsolationLevel;
  }

  public void setConsumerIsolationLevel(ConsumerIsolationLevelEnum consumerIsolationLevel) {
    this.consumerIsolationLevel = consumerIsolationLevel;
  }


  public JsonSchemaKafkaConnect consumerMaxPollRecords(Integer consumerMaxPollRecords) {
    this.consumerMaxPollRecords = consumerMaxPollRecords;
    return this;
  }

   /**
   * The maximum number of records returned in a single call to poll() (defaults to 500).
   * minimum: 1
   * maximum: 10000
   * @return consumerMaxPollRecords
  **/
  @javax.annotation.Nullable
  public Integer getConsumerMaxPollRecords() {
    return consumerMaxPollRecords;
  }

  public void setConsumerMaxPollRecords(Integer consumerMaxPollRecords) {
    this.consumerMaxPollRecords = consumerMaxPollRecords;
  }



  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    JsonSchemaKafkaConnect jsonSchemaKafkaConnect = (JsonSchemaKafkaConnect) o;
    return Objects.equals(this.producerBufferMemory, jsonSchemaKafkaConnect.producerBufferMemory) &&
        Objects.equals(this.consumerMaxPollIntervalMs, jsonSchemaKafkaConnect.consumerMaxPollIntervalMs) &&
        Objects.equals(this.producerCompressionType, jsonSchemaKafkaConnect.producerCompressionType) &&
        Objects.equals(this.connectorClientConfigOverridePolicy, jsonSchemaKafkaConnect.connectorClientConfigOverridePolicy) &&
        Objects.equals(this.offsetFlushIntervalMs, jsonSchemaKafkaConnect.offsetFlushIntervalMs) &&
        Objects.equals(this.scheduledRebalanceMaxDelayMs, jsonSchemaKafkaConnect.scheduledRebalanceMaxDelayMs) &&
        Objects.equals(this.consumerFetchMaxBytes, jsonSchemaKafkaConnect.consumerFetchMaxBytes) &&
        Objects.equals(this.consumerMaxPartitionFetchBytes, jsonSchemaKafkaConnect.consumerMaxPartitionFetchBytes) &&
        Objects.equals(this.offsetFlushTimeoutMs, jsonSchemaKafkaConnect.offsetFlushTimeoutMs) &&
        Objects.equals(this.consumerAutoOffsetReset, jsonSchemaKafkaConnect.consumerAutoOffsetReset) &&
        Objects.equals(this.producerMaxRequestSize, jsonSchemaKafkaConnect.producerMaxRequestSize) &&
        Objects.equals(this.producerBatchSize, jsonSchemaKafkaConnect.producerBatchSize) &&
        Objects.equals(this.sessionTimeoutMs, jsonSchemaKafkaConnect.sessionTimeoutMs) &&
        Objects.equals(this.producerLingerMs, jsonSchemaKafkaConnect.producerLingerMs) &&
        Objects.equals(this.consumerIsolationLevel, jsonSchemaKafkaConnect.consumerIsolationLevel) &&
        Objects.equals(this.consumerMaxPollRecords, jsonSchemaKafkaConnect.consumerMaxPollRecords);
  }

  @Override
  public int hashCode() {
    return Objects.hash(producerBufferMemory, consumerMaxPollIntervalMs, producerCompressionType, connectorClientConfigOverridePolicy, offsetFlushIntervalMs, scheduledRebalanceMaxDelayMs, consumerFetchMaxBytes, consumerMaxPartitionFetchBytes, offsetFlushTimeoutMs, consumerAutoOffsetReset, producerMaxRequestSize, producerBatchSize, sessionTimeoutMs, producerLingerMs, consumerIsolationLevel, consumerMaxPollRecords);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class JsonSchemaKafkaConnect {\n");
    sb.append("    producerBufferMemory: ").append(toIndentedString(producerBufferMemory)).append("\n");
    sb.append("    consumerMaxPollIntervalMs: ").append(toIndentedString(consumerMaxPollIntervalMs)).append("\n");
    sb.append("    producerCompressionType: ").append(toIndentedString(producerCompressionType)).append("\n");
    sb.append("    connectorClientConfigOverridePolicy: ").append(toIndentedString(connectorClientConfigOverridePolicy)).append("\n");
    sb.append("    offsetFlushIntervalMs: ").append(toIndentedString(offsetFlushIntervalMs)).append("\n");
    sb.append("    scheduledRebalanceMaxDelayMs: ").append(toIndentedString(scheduledRebalanceMaxDelayMs)).append("\n");
    sb.append("    consumerFetchMaxBytes: ").append(toIndentedString(consumerFetchMaxBytes)).append("\n");
    sb.append("    consumerMaxPartitionFetchBytes: ").append(toIndentedString(consumerMaxPartitionFetchBytes)).append("\n");
    sb.append("    offsetFlushTimeoutMs: ").append(toIndentedString(offsetFlushTimeoutMs)).append("\n");
    sb.append("    consumerAutoOffsetReset: ").append(toIndentedString(consumerAutoOffsetReset)).append("\n");
    sb.append("    producerMaxRequestSize: ").append(toIndentedString(producerMaxRequestSize)).append("\n");
    sb.append("    producerBatchSize: ").append(toIndentedString(producerBatchSize)).append("\n");
    sb.append("    sessionTimeoutMs: ").append(toIndentedString(sessionTimeoutMs)).append("\n");
    sb.append("    producerLingerMs: ").append(toIndentedString(producerLingerMs)).append("\n");
    sb.append("    consumerIsolationLevel: ").append(toIndentedString(consumerIsolationLevel)).append("\n");
    sb.append("    consumerMaxPollRecords: ").append(toIndentedString(consumerMaxPollRecords)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("producer_buffer_memory");
    openapiFields.add("consumer_max_poll_interval_ms");
    openapiFields.add("producer_compression_type");
    openapiFields.add("connector_client_config_override_policy");
    openapiFields.add("offset_flush_interval_ms");
    openapiFields.add("scheduled_rebalance_max_delay_ms");
    openapiFields.add("consumer_fetch_max_bytes");
    openapiFields.add("consumer_max_partition_fetch_bytes");
    openapiFields.add("offset_flush_timeout_ms");
    openapiFields.add("consumer_auto_offset_reset");
    openapiFields.add("producer_max_request_size");
    openapiFields.add("producer_batch_size");
    openapiFields.add("session_timeout_ms");
    openapiFields.add("producer_linger_ms");
    openapiFields.add("consumer_isolation_level");
    openapiFields.add("consumer_max_poll_records");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

 /**
  * Validates the JSON Element and throws an exception if issues found
  *
  * @param jsonElement JSON Element
  * @throws IOException if the JSON Element is invalid with respect to JsonSchemaKafkaConnect
  */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!JsonSchemaKafkaConnect.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in JsonSchemaKafkaConnect is not found in the empty JSON string", JsonSchemaKafkaConnect.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!JsonSchemaKafkaConnect.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `JsonSchemaKafkaConnect` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      if ((jsonObj.get("producer_compression_type") != null && !jsonObj.get("producer_compression_type").isJsonNull()) && !jsonObj.get("producer_compression_type").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `producer_compression_type` to be a primitive type in the JSON string but got `%s`", jsonObj.get("producer_compression_type").toString()));
      }
      // validate the optional field `producer_compression_type`
      if (jsonObj.get("producer_compression_type") != null && !jsonObj.get("producer_compression_type").isJsonNull()) {
        ProducerCompressionTypeEnum.validateJsonElement(jsonObj.get("producer_compression_type"));
      }
      if ((jsonObj.get("connector_client_config_override_policy") != null && !jsonObj.get("connector_client_config_override_policy").isJsonNull()) && !jsonObj.get("connector_client_config_override_policy").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `connector_client_config_override_policy` to be a primitive type in the JSON string but got `%s`", jsonObj.get("connector_client_config_override_policy").toString()));
      }
      // validate the optional field `connector_client_config_override_policy`
      if (jsonObj.get("connector_client_config_override_policy") != null && !jsonObj.get("connector_client_config_override_policy").isJsonNull()) {
        ConnectorClientConfigOverridePolicyEnum.validateJsonElement(jsonObj.get("connector_client_config_override_policy"));
      }
      if ((jsonObj.get("consumer_auto_offset_reset") != null && !jsonObj.get("consumer_auto_offset_reset").isJsonNull()) && !jsonObj.get("consumer_auto_offset_reset").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `consumer_auto_offset_reset` to be a primitive type in the JSON string but got `%s`", jsonObj.get("consumer_auto_offset_reset").toString()));
      }
      // validate the optional field `consumer_auto_offset_reset`
      if (jsonObj.get("consumer_auto_offset_reset") != null && !jsonObj.get("consumer_auto_offset_reset").isJsonNull()) {
        ConsumerAutoOffsetResetEnum.validateJsonElement(jsonObj.get("consumer_auto_offset_reset"));
      }
      if ((jsonObj.get("consumer_isolation_level") != null && !jsonObj.get("consumer_isolation_level").isJsonNull()) && !jsonObj.get("consumer_isolation_level").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `consumer_isolation_level` to be a primitive type in the JSON string but got `%s`", jsonObj.get("consumer_isolation_level").toString()));
      }
      // validate the optional field `consumer_isolation_level`
      if (jsonObj.get("consumer_isolation_level") != null && !jsonObj.get("consumer_isolation_level").isJsonNull()) {
        ConsumerIsolationLevelEnum.validateJsonElement(jsonObj.get("consumer_isolation_level"));
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!JsonSchemaKafkaConnect.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'JsonSchemaKafkaConnect' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<JsonSchemaKafkaConnect> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(JsonSchemaKafkaConnect.class));

       return (TypeAdapter<T>) new TypeAdapter<JsonSchemaKafkaConnect>() {
           @Override
           public void write(JsonWriter out, JsonSchemaKafkaConnect value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public JsonSchemaKafkaConnect read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

 /**
  * Create an instance of JsonSchemaKafkaConnect given an JSON string
  *
  * @param jsonString JSON string
  * @return An instance of JsonSchemaKafkaConnect
  * @throws IOException if the JSON string is invalid with respect to JsonSchemaKafkaConnect
  */
  public static JsonSchemaKafkaConnect fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, JsonSchemaKafkaConnect.class);
  }

 /**
  * Convert an instance of JsonSchemaKafkaConnect to an JSON string
  *
  * @return JSON string
  */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}


/* 
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

package com.exoscale.api.models.components;

import com.exoscale.api.utils.LazySingletonValue;
import com.exoscale.api.utils.Utils;
import com.fasterxml.jackson.annotation.JsonFormat;
import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.core.type.TypeReference;
import java.io.InputStream;
import java.lang.Deprecated;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.Optional;


public class JsonSchemaKafkaRest {

    /**
     * The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to 'all' or '-1', the leader will wait for the full set of in-sync replicas to acknowledge the record.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("producer_acks")
    private Optional<? extends ProducerAcks> producerAcks;

    /**
     * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("producer_compression_type")
    private Optional<? extends ProducerCompressionType> producerCompressionType;

    /**
     * Wait for up to the given delay to allow batching records together
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("producer_linger_ms")
    private Optional<? extends Long> producerLingerMs;

    /**
     * The maximum size of a request in bytes. Note that Kafka broker can also cap the record batch size.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("producer_max_request_size")
    private Optional<? extends Long> producerMaxRequestSize;

    /**
     * If true the consumer's offset will be periodically committed to Kafka in the background
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("consumer_enable_auto_commit")
    private Optional<? extends Boolean> consumerEnableAutoCommit;

    /**
     * Maximum number of bytes in unencoded message keys and values by a single request
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("consumer_request_max_bytes")
    private Optional<? extends Long> consumerRequestMaxBytes;

    /**
     * The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("consumer_request_timeout_ms")
    private Optional<? extends ConsumerRequestTimeoutMs> consumerRequestTimeoutMs;

    /**
     * Maximum number of SimpleConsumers that can be instantiated per broker
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("simpleconsumer_pool_size_max")
    private Optional<? extends Long> simpleconsumerPoolSizeMax;

    public JsonSchemaKafkaRest(
            @JsonProperty("producer_acks") Optional<? extends ProducerAcks> producerAcks,
            @JsonProperty("producer_compression_type") Optional<? extends ProducerCompressionType> producerCompressionType,
            @JsonProperty("producer_linger_ms") Optional<? extends Long> producerLingerMs,
            @JsonProperty("producer_max_request_size") Optional<? extends Long> producerMaxRequestSize,
            @JsonProperty("consumer_enable_auto_commit") Optional<? extends Boolean> consumerEnableAutoCommit,
            @JsonProperty("consumer_request_max_bytes") Optional<? extends Long> consumerRequestMaxBytes,
            @JsonProperty("consumer_request_timeout_ms") Optional<? extends ConsumerRequestTimeoutMs> consumerRequestTimeoutMs,
            @JsonProperty("simpleconsumer_pool_size_max") Optional<? extends Long> simpleconsumerPoolSizeMax) {
        Utils.checkNotNull(producerAcks, "producerAcks");
        Utils.checkNotNull(producerCompressionType, "producerCompressionType");
        Utils.checkNotNull(producerLingerMs, "producerLingerMs");
        Utils.checkNotNull(producerMaxRequestSize, "producerMaxRequestSize");
        Utils.checkNotNull(consumerEnableAutoCommit, "consumerEnableAutoCommit");
        Utils.checkNotNull(consumerRequestMaxBytes, "consumerRequestMaxBytes");
        Utils.checkNotNull(consumerRequestTimeoutMs, "consumerRequestTimeoutMs");
        Utils.checkNotNull(simpleconsumerPoolSizeMax, "simpleconsumerPoolSizeMax");
        this.producerAcks = producerAcks;
        this.producerCompressionType = producerCompressionType;
        this.producerLingerMs = producerLingerMs;
        this.producerMaxRequestSize = producerMaxRequestSize;
        this.consumerEnableAutoCommit = consumerEnableAutoCommit;
        this.consumerRequestMaxBytes = consumerRequestMaxBytes;
        this.consumerRequestTimeoutMs = consumerRequestTimeoutMs;
        this.simpleconsumerPoolSizeMax = simpleconsumerPoolSizeMax;
    }

    /**
     * The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to 'all' or '-1', the leader will wait for the full set of in-sync replicas to acknowledge the record.
     */
    public Optional<? extends ProducerAcks> producerAcks() {
        return producerAcks;
    }

    /**
     * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
     */
    public Optional<? extends ProducerCompressionType> producerCompressionType() {
        return producerCompressionType;
    }

    /**
     * Wait for up to the given delay to allow batching records together
     */
    public Optional<? extends Long> producerLingerMs() {
        return producerLingerMs;
    }

    /**
     * The maximum size of a request in bytes. Note that Kafka broker can also cap the record batch size.
     */
    public Optional<? extends Long> producerMaxRequestSize() {
        return producerMaxRequestSize;
    }

    /**
     * If true the consumer's offset will be periodically committed to Kafka in the background
     */
    public Optional<? extends Boolean> consumerEnableAutoCommit() {
        return consumerEnableAutoCommit;
    }

    /**
     * Maximum number of bytes in unencoded message keys and values by a single request
     */
    public Optional<? extends Long> consumerRequestMaxBytes() {
        return consumerRequestMaxBytes;
    }

    /**
     * The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached
     */
    public Optional<? extends ConsumerRequestTimeoutMs> consumerRequestTimeoutMs() {
        return consumerRequestTimeoutMs;
    }

    /**
     * Maximum number of SimpleConsumers that can be instantiated per broker
     */
    public Optional<? extends Long> simpleconsumerPoolSizeMax() {
        return simpleconsumerPoolSizeMax;
    }

    public final static Builder builder() {
        return new Builder();
    }

    /**
     * The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to 'all' or '-1', the leader will wait for the full set of in-sync replicas to acknowledge the record.
     */
    public JsonSchemaKafkaRest withProducerAcks(ProducerAcks producerAcks) {
        Utils.checkNotNull(producerAcks, "producerAcks");
        this.producerAcks = Optional.ofNullable(producerAcks);
        return this;
    }

    /**
     * The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to 'all' or '-1', the leader will wait for the full set of in-sync replicas to acknowledge the record.
     */
    public JsonSchemaKafkaRest withProducerAcks(Optional<? extends ProducerAcks> producerAcks) {
        Utils.checkNotNull(producerAcks, "producerAcks");
        this.producerAcks = producerAcks;
        return this;
    }

    /**
     * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
     */
    public JsonSchemaKafkaRest withProducerCompressionType(ProducerCompressionType producerCompressionType) {
        Utils.checkNotNull(producerCompressionType, "producerCompressionType");
        this.producerCompressionType = Optional.ofNullable(producerCompressionType);
        return this;
    }

    /**
     * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
     */
    public JsonSchemaKafkaRest withProducerCompressionType(Optional<? extends ProducerCompressionType> producerCompressionType) {
        Utils.checkNotNull(producerCompressionType, "producerCompressionType");
        this.producerCompressionType = producerCompressionType;
        return this;
    }

    /**
     * Wait for up to the given delay to allow batching records together
     */
    public JsonSchemaKafkaRest withProducerLingerMs(long producerLingerMs) {
        Utils.checkNotNull(producerLingerMs, "producerLingerMs");
        this.producerLingerMs = Optional.ofNullable(producerLingerMs);
        return this;
    }

    /**
     * Wait for up to the given delay to allow batching records together
     */
    public JsonSchemaKafkaRest withProducerLingerMs(Optional<? extends Long> producerLingerMs) {
        Utils.checkNotNull(producerLingerMs, "producerLingerMs");
        this.producerLingerMs = producerLingerMs;
        return this;
    }

    /**
     * The maximum size of a request in bytes. Note that Kafka broker can also cap the record batch size.
     */
    public JsonSchemaKafkaRest withProducerMaxRequestSize(long producerMaxRequestSize) {
        Utils.checkNotNull(producerMaxRequestSize, "producerMaxRequestSize");
        this.producerMaxRequestSize = Optional.ofNullable(producerMaxRequestSize);
        return this;
    }

    /**
     * The maximum size of a request in bytes. Note that Kafka broker can also cap the record batch size.
     */
    public JsonSchemaKafkaRest withProducerMaxRequestSize(Optional<? extends Long> producerMaxRequestSize) {
        Utils.checkNotNull(producerMaxRequestSize, "producerMaxRequestSize");
        this.producerMaxRequestSize = producerMaxRequestSize;
        return this;
    }

    /**
     * If true the consumer's offset will be periodically committed to Kafka in the background
     */
    public JsonSchemaKafkaRest withConsumerEnableAutoCommit(boolean consumerEnableAutoCommit) {
        Utils.checkNotNull(consumerEnableAutoCommit, "consumerEnableAutoCommit");
        this.consumerEnableAutoCommit = Optional.ofNullable(consumerEnableAutoCommit);
        return this;
    }

    /**
     * If true the consumer's offset will be periodically committed to Kafka in the background
     */
    public JsonSchemaKafkaRest withConsumerEnableAutoCommit(Optional<? extends Boolean> consumerEnableAutoCommit) {
        Utils.checkNotNull(consumerEnableAutoCommit, "consumerEnableAutoCommit");
        this.consumerEnableAutoCommit = consumerEnableAutoCommit;
        return this;
    }

    /**
     * Maximum number of bytes in unencoded message keys and values by a single request
     */
    public JsonSchemaKafkaRest withConsumerRequestMaxBytes(long consumerRequestMaxBytes) {
        Utils.checkNotNull(consumerRequestMaxBytes, "consumerRequestMaxBytes");
        this.consumerRequestMaxBytes = Optional.ofNullable(consumerRequestMaxBytes);
        return this;
    }

    /**
     * Maximum number of bytes in unencoded message keys and values by a single request
     */
    public JsonSchemaKafkaRest withConsumerRequestMaxBytes(Optional<? extends Long> consumerRequestMaxBytes) {
        Utils.checkNotNull(consumerRequestMaxBytes, "consumerRequestMaxBytes");
        this.consumerRequestMaxBytes = consumerRequestMaxBytes;
        return this;
    }

    /**
     * The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached
     */
    public JsonSchemaKafkaRest withConsumerRequestTimeoutMs(ConsumerRequestTimeoutMs consumerRequestTimeoutMs) {
        Utils.checkNotNull(consumerRequestTimeoutMs, "consumerRequestTimeoutMs");
        this.consumerRequestTimeoutMs = Optional.ofNullable(consumerRequestTimeoutMs);
        return this;
    }

    /**
     * The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached
     */
    public JsonSchemaKafkaRest withConsumerRequestTimeoutMs(Optional<? extends ConsumerRequestTimeoutMs> consumerRequestTimeoutMs) {
        Utils.checkNotNull(consumerRequestTimeoutMs, "consumerRequestTimeoutMs");
        this.consumerRequestTimeoutMs = consumerRequestTimeoutMs;
        return this;
    }

    /**
     * Maximum number of SimpleConsumers that can be instantiated per broker
     */
    public JsonSchemaKafkaRest withSimpleconsumerPoolSizeMax(long simpleconsumerPoolSizeMax) {
        Utils.checkNotNull(simpleconsumerPoolSizeMax, "simpleconsumerPoolSizeMax");
        this.simpleconsumerPoolSizeMax = Optional.ofNullable(simpleconsumerPoolSizeMax);
        return this;
    }

    /**
     * Maximum number of SimpleConsumers that can be instantiated per broker
     */
    public JsonSchemaKafkaRest withSimpleconsumerPoolSizeMax(Optional<? extends Long> simpleconsumerPoolSizeMax) {
        Utils.checkNotNull(simpleconsumerPoolSizeMax, "simpleconsumerPoolSizeMax");
        this.simpleconsumerPoolSizeMax = simpleconsumerPoolSizeMax;
        return this;
    }
    
    @Override
    public boolean equals(java.lang.Object o) {
        if (this == o) {
            return true;
        }
        if (o == null || getClass() != o.getClass()) {
            return false;
        }
        JsonSchemaKafkaRest other = (JsonSchemaKafkaRest) o;
        return 
            java.util.Objects.deepEquals(this.producerAcks, other.producerAcks) &&
            java.util.Objects.deepEquals(this.producerCompressionType, other.producerCompressionType) &&
            java.util.Objects.deepEquals(this.producerLingerMs, other.producerLingerMs) &&
            java.util.Objects.deepEquals(this.producerMaxRequestSize, other.producerMaxRequestSize) &&
            java.util.Objects.deepEquals(this.consumerEnableAutoCommit, other.consumerEnableAutoCommit) &&
            java.util.Objects.deepEquals(this.consumerRequestMaxBytes, other.consumerRequestMaxBytes) &&
            java.util.Objects.deepEquals(this.consumerRequestTimeoutMs, other.consumerRequestTimeoutMs) &&
            java.util.Objects.deepEquals(this.simpleconsumerPoolSizeMax, other.simpleconsumerPoolSizeMax);
    }
    
    @Override
    public int hashCode() {
        return java.util.Objects.hash(
            producerAcks,
            producerCompressionType,
            producerLingerMs,
            producerMaxRequestSize,
            consumerEnableAutoCommit,
            consumerRequestMaxBytes,
            consumerRequestTimeoutMs,
            simpleconsumerPoolSizeMax);
    }
    
    @Override
    public String toString() {
        return Utils.toString(JsonSchemaKafkaRest.class,
                "producerAcks", producerAcks,
                "producerCompressionType", producerCompressionType,
                "producerLingerMs", producerLingerMs,
                "producerMaxRequestSize", producerMaxRequestSize,
                "consumerEnableAutoCommit", consumerEnableAutoCommit,
                "consumerRequestMaxBytes", consumerRequestMaxBytes,
                "consumerRequestTimeoutMs", consumerRequestTimeoutMs,
                "simpleconsumerPoolSizeMax", simpleconsumerPoolSizeMax);
    }
    
    public final static class Builder {
 
        private Optional<? extends ProducerAcks> producerAcks;
 
        private Optional<? extends ProducerCompressionType> producerCompressionType = Optional.empty();
 
        private Optional<? extends Long> producerLingerMs;
 
        private Optional<? extends Long> producerMaxRequestSize;
 
        private Optional<? extends Boolean> consumerEnableAutoCommit;
 
        private Optional<? extends Long> consumerRequestMaxBytes;
 
        private Optional<? extends ConsumerRequestTimeoutMs> consumerRequestTimeoutMs;
 
        private Optional<? extends Long> simpleconsumerPoolSizeMax;  
        
        private Builder() {
          // force use of static builder() method
        }

        /**
         * The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to 'all' or '-1', the leader will wait for the full set of in-sync replicas to acknowledge the record.
         */
        public Builder producerAcks(ProducerAcks producerAcks) {
            Utils.checkNotNull(producerAcks, "producerAcks");
            this.producerAcks = Optional.ofNullable(producerAcks);
            return this;
        }

        /**
         * The number of acknowledgments the producer requires the leader to have received before considering a request complete. If set to 'all' or '-1', the leader will wait for the full set of in-sync replicas to acknowledge the record.
         */
        public Builder producerAcks(Optional<? extends ProducerAcks> producerAcks) {
            Utils.checkNotNull(producerAcks, "producerAcks");
            this.producerAcks = producerAcks;
            return this;
        }

        /**
         * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
         */
        public Builder producerCompressionType(ProducerCompressionType producerCompressionType) {
            Utils.checkNotNull(producerCompressionType, "producerCompressionType");
            this.producerCompressionType = Optional.ofNullable(producerCompressionType);
            return this;
        }

        /**
         * Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
         */
        public Builder producerCompressionType(Optional<? extends ProducerCompressionType> producerCompressionType) {
            Utils.checkNotNull(producerCompressionType, "producerCompressionType");
            this.producerCompressionType = producerCompressionType;
            return this;
        }

        /**
         * Wait for up to the given delay to allow batching records together
         */
        public Builder producerLingerMs(long producerLingerMs) {
            Utils.checkNotNull(producerLingerMs, "producerLingerMs");
            this.producerLingerMs = Optional.ofNullable(producerLingerMs);
            return this;
        }

        /**
         * Wait for up to the given delay to allow batching records together
         */
        public Builder producerLingerMs(Optional<? extends Long> producerLingerMs) {
            Utils.checkNotNull(producerLingerMs, "producerLingerMs");
            this.producerLingerMs = producerLingerMs;
            return this;
        }

        /**
         * The maximum size of a request in bytes. Note that Kafka broker can also cap the record batch size.
         */
        public Builder producerMaxRequestSize(long producerMaxRequestSize) {
            Utils.checkNotNull(producerMaxRequestSize, "producerMaxRequestSize");
            this.producerMaxRequestSize = Optional.ofNullable(producerMaxRequestSize);
            return this;
        }

        /**
         * The maximum size of a request in bytes. Note that Kafka broker can also cap the record batch size.
         */
        public Builder producerMaxRequestSize(Optional<? extends Long> producerMaxRequestSize) {
            Utils.checkNotNull(producerMaxRequestSize, "producerMaxRequestSize");
            this.producerMaxRequestSize = producerMaxRequestSize;
            return this;
        }

        /**
         * If true the consumer's offset will be periodically committed to Kafka in the background
         */
        public Builder consumerEnableAutoCommit(boolean consumerEnableAutoCommit) {
            Utils.checkNotNull(consumerEnableAutoCommit, "consumerEnableAutoCommit");
            this.consumerEnableAutoCommit = Optional.ofNullable(consumerEnableAutoCommit);
            return this;
        }

        /**
         * If true the consumer's offset will be periodically committed to Kafka in the background
         */
        public Builder consumerEnableAutoCommit(Optional<? extends Boolean> consumerEnableAutoCommit) {
            Utils.checkNotNull(consumerEnableAutoCommit, "consumerEnableAutoCommit");
            this.consumerEnableAutoCommit = consumerEnableAutoCommit;
            return this;
        }

        /**
         * Maximum number of bytes in unencoded message keys and values by a single request
         */
        public Builder consumerRequestMaxBytes(long consumerRequestMaxBytes) {
            Utils.checkNotNull(consumerRequestMaxBytes, "consumerRequestMaxBytes");
            this.consumerRequestMaxBytes = Optional.ofNullable(consumerRequestMaxBytes);
            return this;
        }

        /**
         * Maximum number of bytes in unencoded message keys and values by a single request
         */
        public Builder consumerRequestMaxBytes(Optional<? extends Long> consumerRequestMaxBytes) {
            Utils.checkNotNull(consumerRequestMaxBytes, "consumerRequestMaxBytes");
            this.consumerRequestMaxBytes = consumerRequestMaxBytes;
            return this;
        }

        /**
         * The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached
         */
        public Builder consumerRequestTimeoutMs(ConsumerRequestTimeoutMs consumerRequestTimeoutMs) {
            Utils.checkNotNull(consumerRequestTimeoutMs, "consumerRequestTimeoutMs");
            this.consumerRequestTimeoutMs = Optional.ofNullable(consumerRequestTimeoutMs);
            return this;
        }

        /**
         * The maximum total time to wait for messages for a request if the maximum number of messages has not yet been reached
         */
        public Builder consumerRequestTimeoutMs(Optional<? extends ConsumerRequestTimeoutMs> consumerRequestTimeoutMs) {
            Utils.checkNotNull(consumerRequestTimeoutMs, "consumerRequestTimeoutMs");
            this.consumerRequestTimeoutMs = consumerRequestTimeoutMs;
            return this;
        }

        /**
         * Maximum number of SimpleConsumers that can be instantiated per broker
         */
        public Builder simpleconsumerPoolSizeMax(long simpleconsumerPoolSizeMax) {
            Utils.checkNotNull(simpleconsumerPoolSizeMax, "simpleconsumerPoolSizeMax");
            this.simpleconsumerPoolSizeMax = Optional.ofNullable(simpleconsumerPoolSizeMax);
            return this;
        }

        /**
         * Maximum number of SimpleConsumers that can be instantiated per broker
         */
        public Builder simpleconsumerPoolSizeMax(Optional<? extends Long> simpleconsumerPoolSizeMax) {
            Utils.checkNotNull(simpleconsumerPoolSizeMax, "simpleconsumerPoolSizeMax");
            this.simpleconsumerPoolSizeMax = simpleconsumerPoolSizeMax;
            return this;
        }
        
        public JsonSchemaKafkaRest build() {
            if (producerAcks == null) {
                producerAcks = _SINGLETON_VALUE_ProducerAcks.value();
            }
            if (producerLingerMs == null) {
                producerLingerMs = _SINGLETON_VALUE_ProducerLingerMs.value();
            }
            if (producerMaxRequestSize == null) {
                producerMaxRequestSize = _SINGLETON_VALUE_ProducerMaxRequestSize.value();
            }
            if (consumerEnableAutoCommit == null) {
                consumerEnableAutoCommit = _SINGLETON_VALUE_ConsumerEnableAutoCommit.value();
            }
            if (consumerRequestMaxBytes == null) {
                consumerRequestMaxBytes = _SINGLETON_VALUE_ConsumerRequestMaxBytes.value();
            }
            if (consumerRequestTimeoutMs == null) {
                consumerRequestTimeoutMs = _SINGLETON_VALUE_ConsumerRequestTimeoutMs.value();
            }
            if (simpleconsumerPoolSizeMax == null) {
                simpleconsumerPoolSizeMax = _SINGLETON_VALUE_SimpleconsumerPoolSizeMax.value();
            }
            return new JsonSchemaKafkaRest(
                producerAcks,
                producerCompressionType,
                producerLingerMs,
                producerMaxRequestSize,
                consumerEnableAutoCommit,
                consumerRequestMaxBytes,
                consumerRequestTimeoutMs,
                simpleconsumerPoolSizeMax);
        }

        private static final LazySingletonValue<Optional<? extends ProducerAcks>> _SINGLETON_VALUE_ProducerAcks =
                new LazySingletonValue<>(
                        "producer_acks",
                        "\"1\"",
                        new TypeReference<Optional<? extends ProducerAcks>>() {});

        private static final LazySingletonValue<Optional<? extends Long>> _SINGLETON_VALUE_ProducerLingerMs =
                new LazySingletonValue<>(
                        "producer_linger_ms",
                        "0",
                        new TypeReference<Optional<? extends Long>>() {});

        private static final LazySingletonValue<Optional<? extends Long>> _SINGLETON_VALUE_ProducerMaxRequestSize =
                new LazySingletonValue<>(
                        "producer_max_request_size",
                        "1048576",
                        new TypeReference<Optional<? extends Long>>() {});

        private static final LazySingletonValue<Optional<? extends Boolean>> _SINGLETON_VALUE_ConsumerEnableAutoCommit =
                new LazySingletonValue<>(
                        "consumer_enable_auto_commit",
                        "true",
                        new TypeReference<Optional<? extends Boolean>>() {});

        private static final LazySingletonValue<Optional<? extends Long>> _SINGLETON_VALUE_ConsumerRequestMaxBytes =
                new LazySingletonValue<>(
                        "consumer_request_max_bytes",
                        "67108864",
                        new TypeReference<Optional<? extends Long>>() {});

        private static final LazySingletonValue<Optional<? extends ConsumerRequestTimeoutMs>> _SINGLETON_VALUE_ConsumerRequestTimeoutMs =
                new LazySingletonValue<>(
                        "consumer_request_timeout_ms",
                        "1000",
                        new TypeReference<Optional<? extends ConsumerRequestTimeoutMs>>() {});

        private static final LazySingletonValue<Optional<? extends Long>> _SINGLETON_VALUE_SimpleconsumerPoolSizeMax =
                new LazySingletonValue<>(
                        "simpleconsumer_pool_size_max",
                        "25",
                        new TypeReference<Optional<? extends Long>>() {});
    }
}


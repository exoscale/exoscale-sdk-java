/*
 * Exoscale Public API
 *  Infrastructure automation API, allowing programmatic access to all Exoscale products and services.  The [OpenAPI Specification](http://spec.openapis.org/oas/v3.0.3.html) source of this documentation can be obtained here:  * [JSON format](https://openapi-v2.exoscale.com/source.json) * [YAML format](https://openapi-v2.exoscale.com/source.yaml)
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: api@exoscale.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package com.exoscale.model;

import java.util.Objects;
import java.util.Arrays;
import com.exoscale.model.ConfigureLogCleanerForTopicCompaction;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonTypeName;
import com.fasterxml.jackson.annotation.JsonValue;
import com.fasterxml.jackson.annotation.JsonPropertyOrder;
import com.fasterxml.jackson.annotation.JsonTypeName;

/**
 * JsonSchemaKafka
 */
@JsonPropertyOrder({
  JsonSchemaKafka.JSON_PROPERTY_SASL_OAUTHBEARER_EXPECTED_AUDIENCE,
  JsonSchemaKafka.JSON_PROPERTY_GROUP_MAX_SESSION_TIMEOUT_MS,
  JsonSchemaKafka.JSON_PROPERTY_LOG_FLUSH_INTERVAL_MESSAGES,
  JsonSchemaKafka.JSON_PROPERTY_SASL_OAUTHBEARER_JWKS_ENDPOINT_URL,
  JsonSchemaKafka.JSON_PROPERTY_MAX_CONNECTIONS_PER_IP,
  JsonSchemaKafka.JSON_PROPERTY_SASL_OAUTHBEARER_EXPECTED_ISSUER,
  JsonSchemaKafka.JSON_PROPERTY_LOG_INDEX_SIZE_MAX_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_AUTO_CREATE_TOPICS_ENABLE,
  JsonSchemaKafka.JSON_PROPERTY_LOG_INDEX_INTERVAL_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_REPLICA_FETCH_MAX_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_NUM_PARTITIONS,
  JsonSchemaKafka.JSON_PROPERTY_TRANSACTION_STATE_LOG_SEGMENT_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_REPLICA_FETCH_RESPONSE_MAX_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_LOG_MESSAGE_TIMESTAMP_TYPE,
  JsonSchemaKafka.JSON_PROPERTY_CONNECTIONS_MAX_IDLE_MS,
  JsonSchemaKafka.JSON_PROPERTY_LOG_FLUSH_INTERVAL_MS,
  JsonSchemaKafka.JSON_PROPERTY_LOG_PREALLOCATE,
  JsonSchemaKafka.JSON_PROPERTY_LOG_SEGMENT_DELETE_DELAY_MS,
  JsonSchemaKafka.JSON_PROPERTY_MESSAGE_MAX_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_GROUP_INITIAL_REBALANCE_DELAY_MS,
  JsonSchemaKafka.JSON_PROPERTY_LOG_LOCAL_RETENTION_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_LOG_ROLL_JITTER_MS,
  JsonSchemaKafka.JSON_PROPERTY_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS,
  JsonSchemaKafka.JSON_PROPERTY_DEFAULT_REPLICATION_FACTOR,
  JsonSchemaKafka.JSON_PROPERTY_LOG_ROLL_MS,
  JsonSchemaKafka.JSON_PROPERTY_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS,
  JsonSchemaKafka.JSON_PROPERTY_LOG_RETENTION_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_MIN_INSYNC_REPLICAS,
  JsonSchemaKafka.JSON_PROPERTY_COMPRESSION_TYPE,
  JsonSchemaKafka.JSON_PROPERTY_LOG_MESSAGE_TIMESTAMP_DIFFERENCE_MAX_MS,
  JsonSchemaKafka.JSON_PROPERTY_LOG_LOCAL_RETENTION_MS,
  JsonSchemaKafka.JSON_PROPERTY_LOG_MESSAGE_DOWNCONVERSION_ENABLE,
  JsonSchemaKafka.JSON_PROPERTY_SASL_OAUTHBEARER_SUB_CLAIM_NAME,
  JsonSchemaKafka.JSON_PROPERTY_MAX_INCREMENTAL_FETCH_SESSION_CACHE_SLOTS,
  JsonSchemaKafka.JSON_PROPERTY_LOG_RETENTION_HOURS,
  JsonSchemaKafka.JSON_PROPERTY_GROUP_MIN_SESSION_TIMEOUT_MS,
  JsonSchemaKafka.JSON_PROPERTY_SOCKET_REQUEST_MAX_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_LOG_SEGMENT_BYTES,
  JsonSchemaKafka.JSON_PROPERTY_LOG_CLEANUP_AND_COMPACTION,
  JsonSchemaKafka.JSON_PROPERTY_OFFSETS_RETENTION_MINUTES,
  JsonSchemaKafka.JSON_PROPERTY_LOG_RETENTION_MS
})
@JsonTypeName("json-schema-kafka")
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-03-04T16:37:05.548288+01:00[Europe/Vienna]")
public class JsonSchemaKafka {
  public static final String JSON_PROPERTY_SASL_OAUTHBEARER_EXPECTED_AUDIENCE = "sasl_oauthbearer_expected_audience";
  private String saslOauthbearerExpectedAudience;

  public static final String JSON_PROPERTY_GROUP_MAX_SESSION_TIMEOUT_MS = "group_max_session_timeout_ms";
  private Integer groupMaxSessionTimeoutMs;

  public static final String JSON_PROPERTY_LOG_FLUSH_INTERVAL_MESSAGES = "log_flush_interval_messages";
  private Integer logFlushIntervalMessages;

  public static final String JSON_PROPERTY_SASL_OAUTHBEARER_JWKS_ENDPOINT_URL = "sasl_oauthbearer_jwks_endpoint_url";
  private String saslOauthbearerJwksEndpointUrl;

  public static final String JSON_PROPERTY_MAX_CONNECTIONS_PER_IP = "max_connections_per_ip";
  private Integer maxConnectionsPerIp;

  public static final String JSON_PROPERTY_SASL_OAUTHBEARER_EXPECTED_ISSUER = "sasl_oauthbearer_expected_issuer";
  private String saslOauthbearerExpectedIssuer;

  public static final String JSON_PROPERTY_LOG_INDEX_SIZE_MAX_BYTES = "log_index_size_max_bytes";
  private Integer logIndexSizeMaxBytes;

  public static final String JSON_PROPERTY_AUTO_CREATE_TOPICS_ENABLE = "auto_create_topics_enable";
  private Boolean autoCreateTopicsEnable;

  public static final String JSON_PROPERTY_LOG_INDEX_INTERVAL_BYTES = "log_index_interval_bytes";
  private Integer logIndexIntervalBytes;

  public static final String JSON_PROPERTY_REPLICA_FETCH_MAX_BYTES = "replica_fetch_max_bytes";
  private Integer replicaFetchMaxBytes;

  public static final String JSON_PROPERTY_NUM_PARTITIONS = "num_partitions";
  private Integer numPartitions;

  public static final String JSON_PROPERTY_TRANSACTION_STATE_LOG_SEGMENT_BYTES = "transaction_state_log_segment_bytes";
  private Integer transactionStateLogSegmentBytes;

  public static final String JSON_PROPERTY_REPLICA_FETCH_RESPONSE_MAX_BYTES = "replica_fetch_response_max_bytes";
  private Integer replicaFetchResponseMaxBytes;

  /**
   * Define whether the timestamp in the message is message create time or log append time.
   */
  public enum LogMessageTimestampTypeEnum {
    CREATETIME("CreateTime"),
    
    LOGAPPENDTIME("LogAppendTime");

    private String value;

    LogMessageTimestampTypeEnum(String value) {
      this.value = value;
    }

    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    @JsonCreator
    public static LogMessageTimestampTypeEnum fromValue(String value) {
      for (LogMessageTimestampTypeEnum b : LogMessageTimestampTypeEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }
  }

  public static final String JSON_PROPERTY_LOG_MESSAGE_TIMESTAMP_TYPE = "log_message_timestamp_type";
  private LogMessageTimestampTypeEnum logMessageTimestampType;

  public static final String JSON_PROPERTY_CONNECTIONS_MAX_IDLE_MS = "connections_max_idle_ms";
  private Integer connectionsMaxIdleMs;

  public static final String JSON_PROPERTY_LOG_FLUSH_INTERVAL_MS = "log_flush_interval_ms";
  private Integer logFlushIntervalMs;

  public static final String JSON_PROPERTY_LOG_PREALLOCATE = "log_preallocate";
  private Boolean logPreallocate;

  public static final String JSON_PROPERTY_LOG_SEGMENT_DELETE_DELAY_MS = "log_segment_delete_delay_ms";
  private Integer logSegmentDeleteDelayMs;

  public static final String JSON_PROPERTY_MESSAGE_MAX_BYTES = "message_max_bytes";
  private Integer messageMaxBytes;

  public static final String JSON_PROPERTY_GROUP_INITIAL_REBALANCE_DELAY_MS = "group_initial_rebalance_delay_ms";
  private Integer groupInitialRebalanceDelayMs;

  public static final String JSON_PROPERTY_LOG_LOCAL_RETENTION_BYTES = "log_local_retention_bytes";
  private Integer logLocalRetentionBytes;

  public static final String JSON_PROPERTY_LOG_ROLL_JITTER_MS = "log_roll_jitter_ms";
  private Integer logRollJitterMs;

  public static final String JSON_PROPERTY_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS = "transaction_remove_expired_transaction_cleanup_interval_ms";
  private Integer transactionRemoveExpiredTransactionCleanupIntervalMs;

  public static final String JSON_PROPERTY_DEFAULT_REPLICATION_FACTOR = "default_replication_factor";
  private Integer defaultReplicationFactor;

  public static final String JSON_PROPERTY_LOG_ROLL_MS = "log_roll_ms";
  private Integer logRollMs;

  public static final String JSON_PROPERTY_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS = "producer_purgatory_purge_interval_requests";
  private Integer producerPurgatoryPurgeIntervalRequests;

  public static final String JSON_PROPERTY_LOG_RETENTION_BYTES = "log_retention_bytes";
  private Integer logRetentionBytes;

  public static final String JSON_PROPERTY_MIN_INSYNC_REPLICAS = "min_insync_replicas";
  private Integer minInsyncReplicas;

  /**
   * Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (&#39;gzip&#39;, &#39;snappy&#39;, &#39;lz4&#39;, &#39;zstd&#39;). It additionally accepts &#39;uncompressed&#39; which is equivalent to no compression; and &#39;producer&#39; which means retain the original compression codec set by the producer.
   */
  public enum CompressionTypeEnum {
    GZIP("gzip"),
    
    SNAPPY("snappy"),
    
    LZ4("lz4"),
    
    ZSTD("zstd"),
    
    UNCOMPRESSED("uncompressed"),
    
    PRODUCER("producer");

    private String value;

    CompressionTypeEnum(String value) {
      this.value = value;
    }

    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    @JsonCreator
    public static CompressionTypeEnum fromValue(String value) {
      for (CompressionTypeEnum b : CompressionTypeEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }
  }

  public static final String JSON_PROPERTY_COMPRESSION_TYPE = "compression_type";
  private CompressionTypeEnum compressionType;

  public static final String JSON_PROPERTY_LOG_MESSAGE_TIMESTAMP_DIFFERENCE_MAX_MS = "log_message_timestamp_difference_max_ms";
  private Integer logMessageTimestampDifferenceMaxMs;

  public static final String JSON_PROPERTY_LOG_LOCAL_RETENTION_MS = "log_local_retention_ms";
  private Integer logLocalRetentionMs;

  public static final String JSON_PROPERTY_LOG_MESSAGE_DOWNCONVERSION_ENABLE = "log_message_downconversion_enable";
  private Boolean logMessageDownconversionEnable;

  public static final String JSON_PROPERTY_SASL_OAUTHBEARER_SUB_CLAIM_NAME = "sasl_oauthbearer_sub_claim_name";
  private String saslOauthbearerSubClaimName;

  public static final String JSON_PROPERTY_MAX_INCREMENTAL_FETCH_SESSION_CACHE_SLOTS = "max_incremental_fetch_session_cache_slots";
  private Integer maxIncrementalFetchSessionCacheSlots;

  public static final String JSON_PROPERTY_LOG_RETENTION_HOURS = "log_retention_hours";
  private Integer logRetentionHours;

  public static final String JSON_PROPERTY_GROUP_MIN_SESSION_TIMEOUT_MS = "group_min_session_timeout_ms";
  private Integer groupMinSessionTimeoutMs;

  public static final String JSON_PROPERTY_SOCKET_REQUEST_MAX_BYTES = "socket_request_max_bytes";
  private Integer socketRequestMaxBytes;

  public static final String JSON_PROPERTY_LOG_SEGMENT_BYTES = "log_segment_bytes";
  private Integer logSegmentBytes;

  public static final String JSON_PROPERTY_LOG_CLEANUP_AND_COMPACTION = "log-cleanup-and-compaction";
  private ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction;

  public static final String JSON_PROPERTY_OFFSETS_RETENTION_MINUTES = "offsets_retention_minutes";
  private Integer offsetsRetentionMinutes;

  public static final String JSON_PROPERTY_LOG_RETENTION_MS = "log_retention_ms";
  private Integer logRetentionMs;

  public JsonSchemaKafka() {
  }

  public JsonSchemaKafka saslOauthbearerExpectedAudience(String saslOauthbearerExpectedAudience) {
    
    this.saslOauthbearerExpectedAudience = saslOauthbearerExpectedAudience;
    return this;
  }

   /**
   * The (optional) comma-delimited setting for the broker to use to verify that the JWT was issued for one of the expected audiences.
   * @return saslOauthbearerExpectedAudience
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_SASL_OAUTHBEARER_EXPECTED_AUDIENCE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public String getSaslOauthbearerExpectedAudience() {
    return saslOauthbearerExpectedAudience;
  }


  @JsonProperty(JSON_PROPERTY_SASL_OAUTHBEARER_EXPECTED_AUDIENCE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setSaslOauthbearerExpectedAudience(String saslOauthbearerExpectedAudience) {
    this.saslOauthbearerExpectedAudience = saslOauthbearerExpectedAudience;
  }


  public JsonSchemaKafka groupMaxSessionTimeoutMs(Integer groupMaxSessionTimeoutMs) {
    
    this.groupMaxSessionTimeoutMs = groupMaxSessionTimeoutMs;
    return this;
  }

   /**
   * The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
   * minimum: 0
   * maximum: 1800000
   * @return groupMaxSessionTimeoutMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_GROUP_MAX_SESSION_TIMEOUT_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getGroupMaxSessionTimeoutMs() {
    return groupMaxSessionTimeoutMs;
  }


  @JsonProperty(JSON_PROPERTY_GROUP_MAX_SESSION_TIMEOUT_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setGroupMaxSessionTimeoutMs(Integer groupMaxSessionTimeoutMs) {
    this.groupMaxSessionTimeoutMs = groupMaxSessionTimeoutMs;
  }


  public JsonSchemaKafka logFlushIntervalMessages(Integer logFlushIntervalMessages) {
    
    this.logFlushIntervalMessages = logFlushIntervalMessages;
    return this;
  }

   /**
   * The number of messages accumulated on a log partition before messages are flushed to disk
   * minimum: 1
   * maximum: 9223372036854775807
   * @return logFlushIntervalMessages
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_FLUSH_INTERVAL_MESSAGES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogFlushIntervalMessages() {
    return logFlushIntervalMessages;
  }


  @JsonProperty(JSON_PROPERTY_LOG_FLUSH_INTERVAL_MESSAGES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogFlushIntervalMessages(Integer logFlushIntervalMessages) {
    this.logFlushIntervalMessages = logFlushIntervalMessages;
  }


  public JsonSchemaKafka saslOauthbearerJwksEndpointUrl(String saslOauthbearerJwksEndpointUrl) {
    
    this.saslOauthbearerJwksEndpointUrl = saslOauthbearerJwksEndpointUrl;
    return this;
  }

   /**
   * OIDC JWKS endpoint URL. By setting this the SASL SSL OAuth2/OIDC authentication is enabled. See also other options for SASL OAuth2/OIDC. 
   * @return saslOauthbearerJwksEndpointUrl
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_SASL_OAUTHBEARER_JWKS_ENDPOINT_URL)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public String getSaslOauthbearerJwksEndpointUrl() {
    return saslOauthbearerJwksEndpointUrl;
  }


  @JsonProperty(JSON_PROPERTY_SASL_OAUTHBEARER_JWKS_ENDPOINT_URL)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setSaslOauthbearerJwksEndpointUrl(String saslOauthbearerJwksEndpointUrl) {
    this.saslOauthbearerJwksEndpointUrl = saslOauthbearerJwksEndpointUrl;
  }


  public JsonSchemaKafka maxConnectionsPerIp(Integer maxConnectionsPerIp) {
    
    this.maxConnectionsPerIp = maxConnectionsPerIp;
    return this;
  }

   /**
   * The maximum number of connections allowed from each ip address (defaults to 2147483647).
   * minimum: 256
   * maximum: 2147483647
   * @return maxConnectionsPerIp
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_MAX_CONNECTIONS_PER_IP)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getMaxConnectionsPerIp() {
    return maxConnectionsPerIp;
  }


  @JsonProperty(JSON_PROPERTY_MAX_CONNECTIONS_PER_IP)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setMaxConnectionsPerIp(Integer maxConnectionsPerIp) {
    this.maxConnectionsPerIp = maxConnectionsPerIp;
  }


  public JsonSchemaKafka saslOauthbearerExpectedIssuer(String saslOauthbearerExpectedIssuer) {
    
    this.saslOauthbearerExpectedIssuer = saslOauthbearerExpectedIssuer;
    return this;
  }

   /**
   * Optional setting for the broker to use to verify that the JWT was created by the expected issuer.
   * @return saslOauthbearerExpectedIssuer
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_SASL_OAUTHBEARER_EXPECTED_ISSUER)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public String getSaslOauthbearerExpectedIssuer() {
    return saslOauthbearerExpectedIssuer;
  }


  @JsonProperty(JSON_PROPERTY_SASL_OAUTHBEARER_EXPECTED_ISSUER)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setSaslOauthbearerExpectedIssuer(String saslOauthbearerExpectedIssuer) {
    this.saslOauthbearerExpectedIssuer = saslOauthbearerExpectedIssuer;
  }


  public JsonSchemaKafka logIndexSizeMaxBytes(Integer logIndexSizeMaxBytes) {
    
    this.logIndexSizeMaxBytes = logIndexSizeMaxBytes;
    return this;
  }

   /**
   * The maximum size in bytes of the offset index
   * minimum: 1048576
   * maximum: 104857600
   * @return logIndexSizeMaxBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_INDEX_SIZE_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogIndexSizeMaxBytes() {
    return logIndexSizeMaxBytes;
  }


  @JsonProperty(JSON_PROPERTY_LOG_INDEX_SIZE_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogIndexSizeMaxBytes(Integer logIndexSizeMaxBytes) {
    this.logIndexSizeMaxBytes = logIndexSizeMaxBytes;
  }


  public JsonSchemaKafka autoCreateTopicsEnable(Boolean autoCreateTopicsEnable) {
    
    this.autoCreateTopicsEnable = autoCreateTopicsEnable;
    return this;
  }

   /**
   * Enable auto creation of topics
   * @return autoCreateTopicsEnable
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_AUTO_CREATE_TOPICS_ENABLE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Boolean getAutoCreateTopicsEnable() {
    return autoCreateTopicsEnable;
  }


  @JsonProperty(JSON_PROPERTY_AUTO_CREATE_TOPICS_ENABLE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setAutoCreateTopicsEnable(Boolean autoCreateTopicsEnable) {
    this.autoCreateTopicsEnable = autoCreateTopicsEnable;
  }


  public JsonSchemaKafka logIndexIntervalBytes(Integer logIndexIntervalBytes) {
    
    this.logIndexIntervalBytes = logIndexIntervalBytes;
    return this;
  }

   /**
   * The interval with which Kafka adds an entry to the offset index
   * minimum: 0
   * maximum: 104857600
   * @return logIndexIntervalBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_INDEX_INTERVAL_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogIndexIntervalBytes() {
    return logIndexIntervalBytes;
  }


  @JsonProperty(JSON_PROPERTY_LOG_INDEX_INTERVAL_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogIndexIntervalBytes(Integer logIndexIntervalBytes) {
    this.logIndexIntervalBytes = logIndexIntervalBytes;
  }


  public JsonSchemaKafka replicaFetchMaxBytes(Integer replicaFetchMaxBytes) {
    
    this.replicaFetchMaxBytes = replicaFetchMaxBytes;
    return this;
  }

   /**
   * The number of bytes of messages to attempt to fetch for each partition (defaults to 1048576). This is not an absolute maximum, if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made.
   * minimum: 1048576
   * maximum: 104857600
   * @return replicaFetchMaxBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_REPLICA_FETCH_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getReplicaFetchMaxBytes() {
    return replicaFetchMaxBytes;
  }


  @JsonProperty(JSON_PROPERTY_REPLICA_FETCH_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setReplicaFetchMaxBytes(Integer replicaFetchMaxBytes) {
    this.replicaFetchMaxBytes = replicaFetchMaxBytes;
  }


  public JsonSchemaKafka numPartitions(Integer numPartitions) {
    
    this.numPartitions = numPartitions;
    return this;
  }

   /**
   * Number of partitions for autocreated topics
   * minimum: 1
   * maximum: 1000
   * @return numPartitions
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_NUM_PARTITIONS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getNumPartitions() {
    return numPartitions;
  }


  @JsonProperty(JSON_PROPERTY_NUM_PARTITIONS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setNumPartitions(Integer numPartitions) {
    this.numPartitions = numPartitions;
  }


  public JsonSchemaKafka transactionStateLogSegmentBytes(Integer transactionStateLogSegmentBytes) {
    
    this.transactionStateLogSegmentBytes = transactionStateLogSegmentBytes;
    return this;
  }

   /**
   * The transaction topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads (defaults to 104857600 (100 mebibytes)).
   * minimum: 1048576
   * maximum: 2147483647
   * @return transactionStateLogSegmentBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_TRANSACTION_STATE_LOG_SEGMENT_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getTransactionStateLogSegmentBytes() {
    return transactionStateLogSegmentBytes;
  }


  @JsonProperty(JSON_PROPERTY_TRANSACTION_STATE_LOG_SEGMENT_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setTransactionStateLogSegmentBytes(Integer transactionStateLogSegmentBytes) {
    this.transactionStateLogSegmentBytes = transactionStateLogSegmentBytes;
  }


  public JsonSchemaKafka replicaFetchResponseMaxBytes(Integer replicaFetchResponseMaxBytes) {
    
    this.replicaFetchResponseMaxBytes = replicaFetchResponseMaxBytes;
    return this;
  }

   /**
   * Maximum bytes expected for the entire fetch response (defaults to 10485760). Records are fetched in batches, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that progress can be made. As such, this is not an absolute maximum.
   * minimum: 10485760
   * maximum: 1048576000
   * @return replicaFetchResponseMaxBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_REPLICA_FETCH_RESPONSE_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getReplicaFetchResponseMaxBytes() {
    return replicaFetchResponseMaxBytes;
  }


  @JsonProperty(JSON_PROPERTY_REPLICA_FETCH_RESPONSE_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setReplicaFetchResponseMaxBytes(Integer replicaFetchResponseMaxBytes) {
    this.replicaFetchResponseMaxBytes = replicaFetchResponseMaxBytes;
  }


  public JsonSchemaKafka logMessageTimestampType(LogMessageTimestampTypeEnum logMessageTimestampType) {
    
    this.logMessageTimestampType = logMessageTimestampType;
    return this;
  }

   /**
   * Define whether the timestamp in the message is message create time or log append time.
   * @return logMessageTimestampType
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_MESSAGE_TIMESTAMP_TYPE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public LogMessageTimestampTypeEnum getLogMessageTimestampType() {
    return logMessageTimestampType;
  }


  @JsonProperty(JSON_PROPERTY_LOG_MESSAGE_TIMESTAMP_TYPE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogMessageTimestampType(LogMessageTimestampTypeEnum logMessageTimestampType) {
    this.logMessageTimestampType = logMessageTimestampType;
  }


  public JsonSchemaKafka connectionsMaxIdleMs(Integer connectionsMaxIdleMs) {
    
    this.connectionsMaxIdleMs = connectionsMaxIdleMs;
    return this;
  }

   /**
   * Idle connections timeout: the server socket processor threads close the connections that idle for longer than this.
   * minimum: 1000
   * maximum: 3600000
   * @return connectionsMaxIdleMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_CONNECTIONS_MAX_IDLE_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getConnectionsMaxIdleMs() {
    return connectionsMaxIdleMs;
  }


  @JsonProperty(JSON_PROPERTY_CONNECTIONS_MAX_IDLE_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setConnectionsMaxIdleMs(Integer connectionsMaxIdleMs) {
    this.connectionsMaxIdleMs = connectionsMaxIdleMs;
  }


  public JsonSchemaKafka logFlushIntervalMs(Integer logFlushIntervalMs) {
    
    this.logFlushIntervalMs = logFlushIntervalMs;
    return this;
  }

   /**
   * The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logFlushIntervalMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_FLUSH_INTERVAL_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogFlushIntervalMs() {
    return logFlushIntervalMs;
  }


  @JsonProperty(JSON_PROPERTY_LOG_FLUSH_INTERVAL_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogFlushIntervalMs(Integer logFlushIntervalMs) {
    this.logFlushIntervalMs = logFlushIntervalMs;
  }


  public JsonSchemaKafka logPreallocate(Boolean logPreallocate) {
    
    this.logPreallocate = logPreallocate;
    return this;
  }

   /**
   * Should pre allocate file when create new segment?
   * @return logPreallocate
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_PREALLOCATE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Boolean getLogPreallocate() {
    return logPreallocate;
  }


  @JsonProperty(JSON_PROPERTY_LOG_PREALLOCATE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogPreallocate(Boolean logPreallocate) {
    this.logPreallocate = logPreallocate;
  }


  public JsonSchemaKafka logSegmentDeleteDelayMs(Integer logSegmentDeleteDelayMs) {
    
    this.logSegmentDeleteDelayMs = logSegmentDeleteDelayMs;
    return this;
  }

   /**
   * The amount of time to wait before deleting a file from the filesystem
   * minimum: 0
   * maximum: 3600000
   * @return logSegmentDeleteDelayMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_SEGMENT_DELETE_DELAY_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogSegmentDeleteDelayMs() {
    return logSegmentDeleteDelayMs;
  }


  @JsonProperty(JSON_PROPERTY_LOG_SEGMENT_DELETE_DELAY_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogSegmentDeleteDelayMs(Integer logSegmentDeleteDelayMs) {
    this.logSegmentDeleteDelayMs = logSegmentDeleteDelayMs;
  }


  public JsonSchemaKafka messageMaxBytes(Integer messageMaxBytes) {
    
    this.messageMaxBytes = messageMaxBytes;
    return this;
  }

   /**
   * The maximum size of message that the server can receive.
   * minimum: 0
   * maximum: 100001200
   * @return messageMaxBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_MESSAGE_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getMessageMaxBytes() {
    return messageMaxBytes;
  }


  @JsonProperty(JSON_PROPERTY_MESSAGE_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setMessageMaxBytes(Integer messageMaxBytes) {
    this.messageMaxBytes = messageMaxBytes;
  }


  public JsonSchemaKafka groupInitialRebalanceDelayMs(Integer groupInitialRebalanceDelayMs) {
    
    this.groupInitialRebalanceDelayMs = groupInitialRebalanceDelayMs;
    return this;
  }

   /**
   * The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.
   * minimum: 0
   * maximum: 300000
   * @return groupInitialRebalanceDelayMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_GROUP_INITIAL_REBALANCE_DELAY_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getGroupInitialRebalanceDelayMs() {
    return groupInitialRebalanceDelayMs;
  }


  @JsonProperty(JSON_PROPERTY_GROUP_INITIAL_REBALANCE_DELAY_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setGroupInitialRebalanceDelayMs(Integer groupInitialRebalanceDelayMs) {
    this.groupInitialRebalanceDelayMs = groupInitialRebalanceDelayMs;
  }


  public JsonSchemaKafka logLocalRetentionBytes(Integer logLocalRetentionBytes) {
    
    this.logLocalRetentionBytes = logLocalRetentionBytes;
    return this;
  }

   /**
   * The maximum size of local log segments that can grow for a partition before it gets eligible for deletion. If set to -2, the value of log.retention.bytes is used. The effective value should always be less than or equal to log.retention.bytes value.
   * minimum: -2
   * maximum: 9223372036854775807
   * @return logLocalRetentionBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_LOCAL_RETENTION_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogLocalRetentionBytes() {
    return logLocalRetentionBytes;
  }


  @JsonProperty(JSON_PROPERTY_LOG_LOCAL_RETENTION_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogLocalRetentionBytes(Integer logLocalRetentionBytes) {
    this.logLocalRetentionBytes = logLocalRetentionBytes;
  }


  public JsonSchemaKafka logRollJitterMs(Integer logRollJitterMs) {
    
    this.logRollJitterMs = logRollJitterMs;
    return this;
  }

   /**
   * The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logRollJitterMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_ROLL_JITTER_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogRollJitterMs() {
    return logRollJitterMs;
  }


  @JsonProperty(JSON_PROPERTY_LOG_ROLL_JITTER_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogRollJitterMs(Integer logRollJitterMs) {
    this.logRollJitterMs = logRollJitterMs;
  }


  public JsonSchemaKafka transactionRemoveExpiredTransactionCleanupIntervalMs(Integer transactionRemoveExpiredTransactionCleanupIntervalMs) {
    
    this.transactionRemoveExpiredTransactionCleanupIntervalMs = transactionRemoveExpiredTransactionCleanupIntervalMs;
    return this;
  }

   /**
   * The interval at which to remove transactions that have expired due to transactional.id.expiration.ms passing (defaults to 3600000 (1 hour)).
   * minimum: 600000
   * maximum: 3600000
   * @return transactionRemoveExpiredTransactionCleanupIntervalMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getTransactionRemoveExpiredTransactionCleanupIntervalMs() {
    return transactionRemoveExpiredTransactionCleanupIntervalMs;
  }


  @JsonProperty(JSON_PROPERTY_TRANSACTION_REMOVE_EXPIRED_TRANSACTION_CLEANUP_INTERVAL_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setTransactionRemoveExpiredTransactionCleanupIntervalMs(Integer transactionRemoveExpiredTransactionCleanupIntervalMs) {
    this.transactionRemoveExpiredTransactionCleanupIntervalMs = transactionRemoveExpiredTransactionCleanupIntervalMs;
  }


  public JsonSchemaKafka defaultReplicationFactor(Integer defaultReplicationFactor) {
    
    this.defaultReplicationFactor = defaultReplicationFactor;
    return this;
  }

   /**
   * Replication factor for autocreated topics
   * minimum: 1
   * maximum: 10
   * @return defaultReplicationFactor
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_DEFAULT_REPLICATION_FACTOR)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getDefaultReplicationFactor() {
    return defaultReplicationFactor;
  }


  @JsonProperty(JSON_PROPERTY_DEFAULT_REPLICATION_FACTOR)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setDefaultReplicationFactor(Integer defaultReplicationFactor) {
    this.defaultReplicationFactor = defaultReplicationFactor;
  }


  public JsonSchemaKafka logRollMs(Integer logRollMs) {
    
    this.logRollMs = logRollMs;
    return this;
  }

   /**
   * The maximum time before a new log segment is rolled out (in milliseconds).
   * minimum: 1
   * maximum: 9223372036854775807
   * @return logRollMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_ROLL_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogRollMs() {
    return logRollMs;
  }


  @JsonProperty(JSON_PROPERTY_LOG_ROLL_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogRollMs(Integer logRollMs) {
    this.logRollMs = logRollMs;
  }


  public JsonSchemaKafka producerPurgatoryPurgeIntervalRequests(Integer producerPurgatoryPurgeIntervalRequests) {
    
    this.producerPurgatoryPurgeIntervalRequests = producerPurgatoryPurgeIntervalRequests;
    return this;
  }

   /**
   * The purge interval (in number of requests) of the producer request purgatory(defaults to 1000).
   * minimum: 10
   * maximum: 10000
   * @return producerPurgatoryPurgeIntervalRequests
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getProducerPurgatoryPurgeIntervalRequests() {
    return producerPurgatoryPurgeIntervalRequests;
  }


  @JsonProperty(JSON_PROPERTY_PRODUCER_PURGATORY_PURGE_INTERVAL_REQUESTS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setProducerPurgatoryPurgeIntervalRequests(Integer producerPurgatoryPurgeIntervalRequests) {
    this.producerPurgatoryPurgeIntervalRequests = producerPurgatoryPurgeIntervalRequests;
  }


  public JsonSchemaKafka logRetentionBytes(Integer logRetentionBytes) {
    
    this.logRetentionBytes = logRetentionBytes;
    return this;
  }

   /**
   * The maximum size of the log before deleting messages
   * minimum: -1
   * maximum: 9223372036854775807
   * @return logRetentionBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_RETENTION_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogRetentionBytes() {
    return logRetentionBytes;
  }


  @JsonProperty(JSON_PROPERTY_LOG_RETENTION_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogRetentionBytes(Integer logRetentionBytes) {
    this.logRetentionBytes = logRetentionBytes;
  }


  public JsonSchemaKafka minInsyncReplicas(Integer minInsyncReplicas) {
    
    this.minInsyncReplicas = minInsyncReplicas;
    return this;
  }

   /**
   * When a producer sets acks to &#39;all&#39; (or &#39;-1&#39;), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful.
   * minimum: 1
   * maximum: 7
   * @return minInsyncReplicas
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_MIN_INSYNC_REPLICAS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getMinInsyncReplicas() {
    return minInsyncReplicas;
  }


  @JsonProperty(JSON_PROPERTY_MIN_INSYNC_REPLICAS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setMinInsyncReplicas(Integer minInsyncReplicas) {
    this.minInsyncReplicas = minInsyncReplicas;
  }


  public JsonSchemaKafka compressionType(CompressionTypeEnum compressionType) {
    
    this.compressionType = compressionType;
    return this;
  }

   /**
   * Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (&#39;gzip&#39;, &#39;snappy&#39;, &#39;lz4&#39;, &#39;zstd&#39;). It additionally accepts &#39;uncompressed&#39; which is equivalent to no compression; and &#39;producer&#39; which means retain the original compression codec set by the producer.
   * @return compressionType
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_COMPRESSION_TYPE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public CompressionTypeEnum getCompressionType() {
    return compressionType;
  }


  @JsonProperty(JSON_PROPERTY_COMPRESSION_TYPE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setCompressionType(CompressionTypeEnum compressionType) {
    this.compressionType = compressionType;
  }


  public JsonSchemaKafka logMessageTimestampDifferenceMaxMs(Integer logMessageTimestampDifferenceMaxMs) {
    
    this.logMessageTimestampDifferenceMaxMs = logMessageTimestampDifferenceMaxMs;
    return this;
  }

   /**
   * The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message
   * minimum: 0
   * maximum: 9223372036854775807
   * @return logMessageTimestampDifferenceMaxMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_MESSAGE_TIMESTAMP_DIFFERENCE_MAX_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogMessageTimestampDifferenceMaxMs() {
    return logMessageTimestampDifferenceMaxMs;
  }


  @JsonProperty(JSON_PROPERTY_LOG_MESSAGE_TIMESTAMP_DIFFERENCE_MAX_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogMessageTimestampDifferenceMaxMs(Integer logMessageTimestampDifferenceMaxMs) {
    this.logMessageTimestampDifferenceMaxMs = logMessageTimestampDifferenceMaxMs;
  }


  public JsonSchemaKafka logLocalRetentionMs(Integer logLocalRetentionMs) {
    
    this.logLocalRetentionMs = logLocalRetentionMs;
    return this;
  }

   /**
   * The number of milliseconds to keep the local log segments before it gets eligible for deletion. If set to -2, the value of log.retention.ms is used. The effective value should always be less than or equal to log.retention.ms value.
   * minimum: -2
   * maximum: 9223372036854775807
   * @return logLocalRetentionMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_LOCAL_RETENTION_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogLocalRetentionMs() {
    return logLocalRetentionMs;
  }


  @JsonProperty(JSON_PROPERTY_LOG_LOCAL_RETENTION_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogLocalRetentionMs(Integer logLocalRetentionMs) {
    this.logLocalRetentionMs = logLocalRetentionMs;
  }


  public JsonSchemaKafka logMessageDownconversionEnable(Boolean logMessageDownconversionEnable) {
    
    this.logMessageDownconversionEnable = logMessageDownconversionEnable;
    return this;
  }

   /**
   * This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests. 
   * @return logMessageDownconversionEnable
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_MESSAGE_DOWNCONVERSION_ENABLE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Boolean getLogMessageDownconversionEnable() {
    return logMessageDownconversionEnable;
  }


  @JsonProperty(JSON_PROPERTY_LOG_MESSAGE_DOWNCONVERSION_ENABLE)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogMessageDownconversionEnable(Boolean logMessageDownconversionEnable) {
    this.logMessageDownconversionEnable = logMessageDownconversionEnable;
  }


  public JsonSchemaKafka saslOauthbearerSubClaimName(String saslOauthbearerSubClaimName) {
    
    this.saslOauthbearerSubClaimName = saslOauthbearerSubClaimName;
    return this;
  }

   /**
   * Name of the scope from which to extract the subject claim from the JWT. Defaults to sub.
   * @return saslOauthbearerSubClaimName
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_SASL_OAUTHBEARER_SUB_CLAIM_NAME)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public String getSaslOauthbearerSubClaimName() {
    return saslOauthbearerSubClaimName;
  }


  @JsonProperty(JSON_PROPERTY_SASL_OAUTHBEARER_SUB_CLAIM_NAME)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setSaslOauthbearerSubClaimName(String saslOauthbearerSubClaimName) {
    this.saslOauthbearerSubClaimName = saslOauthbearerSubClaimName;
  }


  public JsonSchemaKafka maxIncrementalFetchSessionCacheSlots(Integer maxIncrementalFetchSessionCacheSlots) {
    
    this.maxIncrementalFetchSessionCacheSlots = maxIncrementalFetchSessionCacheSlots;
    return this;
  }

   /**
   * The maximum number of incremental fetch sessions that the broker will maintain.
   * minimum: 1000
   * maximum: 10000
   * @return maxIncrementalFetchSessionCacheSlots
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_MAX_INCREMENTAL_FETCH_SESSION_CACHE_SLOTS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getMaxIncrementalFetchSessionCacheSlots() {
    return maxIncrementalFetchSessionCacheSlots;
  }


  @JsonProperty(JSON_PROPERTY_MAX_INCREMENTAL_FETCH_SESSION_CACHE_SLOTS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setMaxIncrementalFetchSessionCacheSlots(Integer maxIncrementalFetchSessionCacheSlots) {
    this.maxIncrementalFetchSessionCacheSlots = maxIncrementalFetchSessionCacheSlots;
  }


  public JsonSchemaKafka logRetentionHours(Integer logRetentionHours) {
    
    this.logRetentionHours = logRetentionHours;
    return this;
  }

   /**
   * The number of hours to keep a log file before deleting it
   * minimum: -1
   * maximum: 2147483647
   * @return logRetentionHours
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_RETENTION_HOURS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogRetentionHours() {
    return logRetentionHours;
  }


  @JsonProperty(JSON_PROPERTY_LOG_RETENTION_HOURS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogRetentionHours(Integer logRetentionHours) {
    this.logRetentionHours = logRetentionHours;
  }


  public JsonSchemaKafka groupMinSessionTimeoutMs(Integer groupMinSessionTimeoutMs) {
    
    this.groupMinSessionTimeoutMs = groupMinSessionTimeoutMs;
    return this;
  }

   /**
   * The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
   * minimum: 0
   * maximum: 60000
   * @return groupMinSessionTimeoutMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_GROUP_MIN_SESSION_TIMEOUT_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getGroupMinSessionTimeoutMs() {
    return groupMinSessionTimeoutMs;
  }


  @JsonProperty(JSON_PROPERTY_GROUP_MIN_SESSION_TIMEOUT_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setGroupMinSessionTimeoutMs(Integer groupMinSessionTimeoutMs) {
    this.groupMinSessionTimeoutMs = groupMinSessionTimeoutMs;
  }


  public JsonSchemaKafka socketRequestMaxBytes(Integer socketRequestMaxBytes) {
    
    this.socketRequestMaxBytes = socketRequestMaxBytes;
    return this;
  }

   /**
   * The maximum number of bytes in a socket request (defaults to 104857600).
   * minimum: 10485760
   * maximum: 209715200
   * @return socketRequestMaxBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_SOCKET_REQUEST_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getSocketRequestMaxBytes() {
    return socketRequestMaxBytes;
  }


  @JsonProperty(JSON_PROPERTY_SOCKET_REQUEST_MAX_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setSocketRequestMaxBytes(Integer socketRequestMaxBytes) {
    this.socketRequestMaxBytes = socketRequestMaxBytes;
  }


  public JsonSchemaKafka logSegmentBytes(Integer logSegmentBytes) {
    
    this.logSegmentBytes = logSegmentBytes;
    return this;
  }

   /**
   * The maximum size of a single log file
   * minimum: 10485760
   * maximum: 1073741824
   * @return logSegmentBytes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_SEGMENT_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogSegmentBytes() {
    return logSegmentBytes;
  }


  @JsonProperty(JSON_PROPERTY_LOG_SEGMENT_BYTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogSegmentBytes(Integer logSegmentBytes) {
    this.logSegmentBytes = logSegmentBytes;
  }


  public JsonSchemaKafka logCleanupAndCompaction(ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction) {
    
    this.logCleanupAndCompaction = logCleanupAndCompaction;
    return this;
  }

   /**
   * Get logCleanupAndCompaction
   * @return logCleanupAndCompaction
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_CLEANUP_AND_COMPACTION)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public ConfigureLogCleanerForTopicCompaction getLogCleanupAndCompaction() {
    return logCleanupAndCompaction;
  }


  @JsonProperty(JSON_PROPERTY_LOG_CLEANUP_AND_COMPACTION)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogCleanupAndCompaction(ConfigureLogCleanerForTopicCompaction logCleanupAndCompaction) {
    this.logCleanupAndCompaction = logCleanupAndCompaction;
  }


  public JsonSchemaKafka offsetsRetentionMinutes(Integer offsetsRetentionMinutes) {
    
    this.offsetsRetentionMinutes = offsetsRetentionMinutes;
    return this;
  }

   /**
   * Log retention window in minutes for offsets topic
   * minimum: 1
   * maximum: 2147483647
   * @return offsetsRetentionMinutes
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_OFFSETS_RETENTION_MINUTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getOffsetsRetentionMinutes() {
    return offsetsRetentionMinutes;
  }


  @JsonProperty(JSON_PROPERTY_OFFSETS_RETENTION_MINUTES)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setOffsetsRetentionMinutes(Integer offsetsRetentionMinutes) {
    this.offsetsRetentionMinutes = offsetsRetentionMinutes;
  }


  public JsonSchemaKafka logRetentionMs(Integer logRetentionMs) {
    
    this.logRetentionMs = logRetentionMs;
    return this;
  }

   /**
   * The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.
   * minimum: -1
   * maximum: 9223372036854775807
   * @return logRetentionMs
  **/
  @javax.annotation.Nullable
  @JsonProperty(JSON_PROPERTY_LOG_RETENTION_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)

  public Integer getLogRetentionMs() {
    return logRetentionMs;
  }


  @JsonProperty(JSON_PROPERTY_LOG_RETENTION_MS)
  @JsonInclude(value = JsonInclude.Include.USE_DEFAULTS)
  public void setLogRetentionMs(Integer logRetentionMs) {
    this.logRetentionMs = logRetentionMs;
  }

  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    JsonSchemaKafka jsonSchemaKafka = (JsonSchemaKafka) o;
    return Objects.equals(this.saslOauthbearerExpectedAudience, jsonSchemaKafka.saslOauthbearerExpectedAudience) &&
        Objects.equals(this.groupMaxSessionTimeoutMs, jsonSchemaKafka.groupMaxSessionTimeoutMs) &&
        Objects.equals(this.logFlushIntervalMessages, jsonSchemaKafka.logFlushIntervalMessages) &&
        Objects.equals(this.saslOauthbearerJwksEndpointUrl, jsonSchemaKafka.saslOauthbearerJwksEndpointUrl) &&
        Objects.equals(this.maxConnectionsPerIp, jsonSchemaKafka.maxConnectionsPerIp) &&
        Objects.equals(this.saslOauthbearerExpectedIssuer, jsonSchemaKafka.saslOauthbearerExpectedIssuer) &&
        Objects.equals(this.logIndexSizeMaxBytes, jsonSchemaKafka.logIndexSizeMaxBytes) &&
        Objects.equals(this.autoCreateTopicsEnable, jsonSchemaKafka.autoCreateTopicsEnable) &&
        Objects.equals(this.logIndexIntervalBytes, jsonSchemaKafka.logIndexIntervalBytes) &&
        Objects.equals(this.replicaFetchMaxBytes, jsonSchemaKafka.replicaFetchMaxBytes) &&
        Objects.equals(this.numPartitions, jsonSchemaKafka.numPartitions) &&
        Objects.equals(this.transactionStateLogSegmentBytes, jsonSchemaKafka.transactionStateLogSegmentBytes) &&
        Objects.equals(this.replicaFetchResponseMaxBytes, jsonSchemaKafka.replicaFetchResponseMaxBytes) &&
        Objects.equals(this.logMessageTimestampType, jsonSchemaKafka.logMessageTimestampType) &&
        Objects.equals(this.connectionsMaxIdleMs, jsonSchemaKafka.connectionsMaxIdleMs) &&
        Objects.equals(this.logFlushIntervalMs, jsonSchemaKafka.logFlushIntervalMs) &&
        Objects.equals(this.logPreallocate, jsonSchemaKafka.logPreallocate) &&
        Objects.equals(this.logSegmentDeleteDelayMs, jsonSchemaKafka.logSegmentDeleteDelayMs) &&
        Objects.equals(this.messageMaxBytes, jsonSchemaKafka.messageMaxBytes) &&
        Objects.equals(this.groupInitialRebalanceDelayMs, jsonSchemaKafka.groupInitialRebalanceDelayMs) &&
        Objects.equals(this.logLocalRetentionBytes, jsonSchemaKafka.logLocalRetentionBytes) &&
        Objects.equals(this.logRollJitterMs, jsonSchemaKafka.logRollJitterMs) &&
        Objects.equals(this.transactionRemoveExpiredTransactionCleanupIntervalMs, jsonSchemaKafka.transactionRemoveExpiredTransactionCleanupIntervalMs) &&
        Objects.equals(this.defaultReplicationFactor, jsonSchemaKafka.defaultReplicationFactor) &&
        Objects.equals(this.logRollMs, jsonSchemaKafka.logRollMs) &&
        Objects.equals(this.producerPurgatoryPurgeIntervalRequests, jsonSchemaKafka.producerPurgatoryPurgeIntervalRequests) &&
        Objects.equals(this.logRetentionBytes, jsonSchemaKafka.logRetentionBytes) &&
        Objects.equals(this.minInsyncReplicas, jsonSchemaKafka.minInsyncReplicas) &&
        Objects.equals(this.compressionType, jsonSchemaKafka.compressionType) &&
        Objects.equals(this.logMessageTimestampDifferenceMaxMs, jsonSchemaKafka.logMessageTimestampDifferenceMaxMs) &&
        Objects.equals(this.logLocalRetentionMs, jsonSchemaKafka.logLocalRetentionMs) &&
        Objects.equals(this.logMessageDownconversionEnable, jsonSchemaKafka.logMessageDownconversionEnable) &&
        Objects.equals(this.saslOauthbearerSubClaimName, jsonSchemaKafka.saslOauthbearerSubClaimName) &&
        Objects.equals(this.maxIncrementalFetchSessionCacheSlots, jsonSchemaKafka.maxIncrementalFetchSessionCacheSlots) &&
        Objects.equals(this.logRetentionHours, jsonSchemaKafka.logRetentionHours) &&
        Objects.equals(this.groupMinSessionTimeoutMs, jsonSchemaKafka.groupMinSessionTimeoutMs) &&
        Objects.equals(this.socketRequestMaxBytes, jsonSchemaKafka.socketRequestMaxBytes) &&
        Objects.equals(this.logSegmentBytes, jsonSchemaKafka.logSegmentBytes) &&
        Objects.equals(this.logCleanupAndCompaction, jsonSchemaKafka.logCleanupAndCompaction) &&
        Objects.equals(this.offsetsRetentionMinutes, jsonSchemaKafka.offsetsRetentionMinutes) &&
        Objects.equals(this.logRetentionMs, jsonSchemaKafka.logRetentionMs);
  }

  @Override
  public int hashCode() {
    return Objects.hash(saslOauthbearerExpectedAudience, groupMaxSessionTimeoutMs, logFlushIntervalMessages, saslOauthbearerJwksEndpointUrl, maxConnectionsPerIp, saslOauthbearerExpectedIssuer, logIndexSizeMaxBytes, autoCreateTopicsEnable, logIndexIntervalBytes, replicaFetchMaxBytes, numPartitions, transactionStateLogSegmentBytes, replicaFetchResponseMaxBytes, logMessageTimestampType, connectionsMaxIdleMs, logFlushIntervalMs, logPreallocate, logSegmentDeleteDelayMs, messageMaxBytes, groupInitialRebalanceDelayMs, logLocalRetentionBytes, logRollJitterMs, transactionRemoveExpiredTransactionCleanupIntervalMs, defaultReplicationFactor, logRollMs, producerPurgatoryPurgeIntervalRequests, logRetentionBytes, minInsyncReplicas, compressionType, logMessageTimestampDifferenceMaxMs, logLocalRetentionMs, logMessageDownconversionEnable, saslOauthbearerSubClaimName, maxIncrementalFetchSessionCacheSlots, logRetentionHours, groupMinSessionTimeoutMs, socketRequestMaxBytes, logSegmentBytes, logCleanupAndCompaction, offsetsRetentionMinutes, logRetentionMs);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class JsonSchemaKafka {\n");
    sb.append("    saslOauthbearerExpectedAudience: ").append(toIndentedString(saslOauthbearerExpectedAudience)).append("\n");
    sb.append("    groupMaxSessionTimeoutMs: ").append(toIndentedString(groupMaxSessionTimeoutMs)).append("\n");
    sb.append("    logFlushIntervalMessages: ").append(toIndentedString(logFlushIntervalMessages)).append("\n");
    sb.append("    saslOauthbearerJwksEndpointUrl: ").append(toIndentedString(saslOauthbearerJwksEndpointUrl)).append("\n");
    sb.append("    maxConnectionsPerIp: ").append(toIndentedString(maxConnectionsPerIp)).append("\n");
    sb.append("    saslOauthbearerExpectedIssuer: ").append(toIndentedString(saslOauthbearerExpectedIssuer)).append("\n");
    sb.append("    logIndexSizeMaxBytes: ").append(toIndentedString(logIndexSizeMaxBytes)).append("\n");
    sb.append("    autoCreateTopicsEnable: ").append(toIndentedString(autoCreateTopicsEnable)).append("\n");
    sb.append("    logIndexIntervalBytes: ").append(toIndentedString(logIndexIntervalBytes)).append("\n");
    sb.append("    replicaFetchMaxBytes: ").append(toIndentedString(replicaFetchMaxBytes)).append("\n");
    sb.append("    numPartitions: ").append(toIndentedString(numPartitions)).append("\n");
    sb.append("    transactionStateLogSegmentBytes: ").append(toIndentedString(transactionStateLogSegmentBytes)).append("\n");
    sb.append("    replicaFetchResponseMaxBytes: ").append(toIndentedString(replicaFetchResponseMaxBytes)).append("\n");
    sb.append("    logMessageTimestampType: ").append(toIndentedString(logMessageTimestampType)).append("\n");
    sb.append("    connectionsMaxIdleMs: ").append(toIndentedString(connectionsMaxIdleMs)).append("\n");
    sb.append("    logFlushIntervalMs: ").append(toIndentedString(logFlushIntervalMs)).append("\n");
    sb.append("    logPreallocate: ").append(toIndentedString(logPreallocate)).append("\n");
    sb.append("    logSegmentDeleteDelayMs: ").append(toIndentedString(logSegmentDeleteDelayMs)).append("\n");
    sb.append("    messageMaxBytes: ").append(toIndentedString(messageMaxBytes)).append("\n");
    sb.append("    groupInitialRebalanceDelayMs: ").append(toIndentedString(groupInitialRebalanceDelayMs)).append("\n");
    sb.append("    logLocalRetentionBytes: ").append(toIndentedString(logLocalRetentionBytes)).append("\n");
    sb.append("    logRollJitterMs: ").append(toIndentedString(logRollJitterMs)).append("\n");
    sb.append("    transactionRemoveExpiredTransactionCleanupIntervalMs: ").append(toIndentedString(transactionRemoveExpiredTransactionCleanupIntervalMs)).append("\n");
    sb.append("    defaultReplicationFactor: ").append(toIndentedString(defaultReplicationFactor)).append("\n");
    sb.append("    logRollMs: ").append(toIndentedString(logRollMs)).append("\n");
    sb.append("    producerPurgatoryPurgeIntervalRequests: ").append(toIndentedString(producerPurgatoryPurgeIntervalRequests)).append("\n");
    sb.append("    logRetentionBytes: ").append(toIndentedString(logRetentionBytes)).append("\n");
    sb.append("    minInsyncReplicas: ").append(toIndentedString(minInsyncReplicas)).append("\n");
    sb.append("    compressionType: ").append(toIndentedString(compressionType)).append("\n");
    sb.append("    logMessageTimestampDifferenceMaxMs: ").append(toIndentedString(logMessageTimestampDifferenceMaxMs)).append("\n");
    sb.append("    logLocalRetentionMs: ").append(toIndentedString(logLocalRetentionMs)).append("\n");
    sb.append("    logMessageDownconversionEnable: ").append(toIndentedString(logMessageDownconversionEnable)).append("\n");
    sb.append("    saslOauthbearerSubClaimName: ").append(toIndentedString(saslOauthbearerSubClaimName)).append("\n");
    sb.append("    maxIncrementalFetchSessionCacheSlots: ").append(toIndentedString(maxIncrementalFetchSessionCacheSlots)).append("\n");
    sb.append("    logRetentionHours: ").append(toIndentedString(logRetentionHours)).append("\n");
    sb.append("    groupMinSessionTimeoutMs: ").append(toIndentedString(groupMinSessionTimeoutMs)).append("\n");
    sb.append("    socketRequestMaxBytes: ").append(toIndentedString(socketRequestMaxBytes)).append("\n");
    sb.append("    logSegmentBytes: ").append(toIndentedString(logSegmentBytes)).append("\n");
    sb.append("    logCleanupAndCompaction: ").append(toIndentedString(logCleanupAndCompaction)).append("\n");
    sb.append("    offsetsRetentionMinutes: ").append(toIndentedString(offsetsRetentionMinutes)).append("\n");
    sb.append("    logRetentionMs: ").append(toIndentedString(logRetentionMs)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }

}

